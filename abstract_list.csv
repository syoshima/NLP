author,shortname,id,url,abstract
Elfar Adalsteinsson,Adalsteinsson_Elfar,arXiv:1903.02959,https://arxiv.org/abs/1903.02959,"b'Abstract:  We present a robust method to correct for motion in volumetric in-utero MRI\ntime series. Time-course analysis for in-utero volumetric MRI time series often\nsuffers from substantial and unpredictable fetal motion. Registration provides\nvoxel correspondences between images and is commonly employed for motion\ncorrection. Current registration methods often fail when aligning images that\nare substantially different from a template (reference image). To achieve\naccurate and robust alignment, we make a Markov assumption on the nature of\nmotion and take advantage of the temporal smoothness in the image data. Forward\nmessage passing in the corresponding hidden Markov model (HMM) yields an\nestimation algorithm that only has to account for relatively small motion\nbetween consecutive frames. We evaluate the utility of the temporal model in\nthe context of in-utero MRI time series alignment by examining the accuracy of\npropagated segmentation label maps. Our results suggest that the proposed model\ncaptures accurately the temporal dynamics of transformations in in-utero MRI\ntime series.'"
Elfar Adalsteinsson,Adalsteinsson_Elfar,arXiv:1608.03907,https://arxiv.org/abs/1608.03907,b'Abstract:  We present a robust method to correct for motion and deformations for\nin-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI\nrequires robust alignment across time in the presence of substantial and\nunpredictable motion. We make a Markov assumption on the nature of deformations\nto take advantage of the temporal structure in the image data. Forward message\npassing in the corresponding hidden Markov model (HMM) yields an estimation\nalgorithm that only has to account for relatively small motion between\nconsecutive frames. We demonstrate the utility of the temporal model by showing\nthat its use improves the accuracy of the segmentation propagation through\ntemporal registration. Our results suggest that the proposed model captures\naccurately the temporal dynamics of deformations in in-utero MRI time series.'
Elfar Adalsteinsson,Adalsteinsson_Elfar,arXiv:0907.2083,https://arxiv.org/abs/0907.2083,"b'Abstract:  A linear inverse problem is proposed that requires the determination of\nmultiple unknown signal vectors. Each unknown vector passes through a different\nsystem matrix and the results are added to yield a single observation vector.\nGiven the matrices and lone observation, the objective is to find a\nsimultaneously sparse set of unknown vectors that solves the system. We will\nrefer to this as the multiple-system single-output (MSSO) simultaneous sparsity\nproblem. This manuscript contrasts the MSSO problem with other simultaneous\nsparsity problems and conducts a thorough initial exploration of algorithms\nwith which to solve it. Seven algorithms are formulated that approximately\nsolve this NP-Hard problem. Three greedy techniques are developed (matching\npursuit, orthogonal matching pursuit, and least squares matching pursuit) along\nwith four methods based on a convex relaxation (iteratively reweighted least\nsquares, two forms of iterative shrinkage, and formulation as a second-order\ncone program). The algorithms are evaluated across three experiments: the first\nand second involve sparsity profile recovery in noiseless and noisy scenarios,\nrespectively, while the third deals with magnetic resonance imaging\nradio-frequency excitation pulse design.'"
Akintunde Akinwande,Akinwande_Akintunde,arXiv:1812.07380,https://arxiv.org/abs/1812.07380,"b'Abstract:  We present a Machine Learning-based method for tomographic reconstruction of\ndense layered objects, with range of projection angles limited to $\\pm\n$10$^\\circ$. Whereas previous approaches to phase tomography generally require\ntwo steps, first to retrieve phase projections from intensity projections and\nthen perform tomographic reconstruction on the retrieved phase projections, in\nour work a physics-informed pre-processor followed by a Deep Neural Network\n(DNN) conduct the three-dimensional reconstruction directly from the intensity\nprojections. We demonstrate this single-step method experimentally in the\nvisible optical domain on a scaled up integrated circuit phantom. We show that\neven under conditions of highly attenuated photon fluxes a DNN trained only on\nsynthetic data can be used to successfully reconstruct physical samples\ndisjoint from the synthetic training set. Thus, the need of producing a large\nnumber of physical examples for training is ameliorated. The method is\ngenerally applicable to tomography with electromagnetic or other types of\nradiation at all bands.'"
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1810.01963,https://arxiv.org/abs/1810.01963,"b""Abstract:  Efficiently scheduling data processing jobs on distributed compute clusters\nrequires complex algorithms. Current systems, however, use simple generalized\nheuristics and ignore workload characteristics, since developing and tuning a\nscheduling policy for each workload is infeasible. In this paper, we show that\nmodern machine learning techniques can generate highly-efficient policies\nautomatically. Decima uses reinforcement learning (RL) and neural networks to\nlearn workload-specific scheduling algorithms without any human instruction\nbeyond a high-level objective such as minimizing average job completion time.\nOff-the-shelf RL techniques, however, cannot handle the complexity and scale of\nthe scheduling problem. To build Decima, we had to develop new representations\nfor jobs' dependency graphs, design scalable RL models, and invent RL training\nmethods for dealing with continuous stochastic job arrivals. Our prototype\nintegration with Spark on a 25-node cluster shows that Decima improves the\naverage job completion time over hand-tuned scheduling heuristics by at least\n21%, achieving up to 2x improvement during periods of high cluster load."""
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1809.05088,https://arxiv.org/abs/1809.05088,"b""Abstract:  With the growing usage of Bitcoin and other cryptocurrencies, many\nscalability challenges have emerged. A promising scaling solution, exemplified\nby the Lightning Network, uses a network of bidirectional payment channels that\nallows fast transactions between two parties. However, routing payments on\nthese networks efficiently is non-trivial, since payments require finding paths\nwith sufficient funds, and channels can become unidirectional over time\nblocking further transactions through them. Today's payment channel networks\nexacerbate these problems by attempting to deliver all payments atomically. In\nthis paper, we present the Spider network, a new packet-switched architecture\nfor payment channel networks. Spider splits payments into transaction units and\ntransmits them over time across different paths. Spider uses congestion\ncontrol, payment scheduling, and imbalance-aware routing to optimize delivery\nof payments. Our results show that Spider improves the volume and number of\nsuccessful payments on the network by 10-45% and 5-40% respectively compared to\nstate-of-the-art approaches."""
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1808.00826,https://arxiv.org/abs/1808.00826,"b'Abstract:  Prior research has proposed technical solutions to use peer-to-peer (P2P)\ncontent delivery to serve Internet video, showing that it can reduce costs to\ncontent providers. Yet, such methods have not become widespread except for a\nfew niche instances. An important challenge is incentivization: what tangible\nbenefits does P2P content delivery offer users who bring resources to the\ntable? In this paper, we ask whether monetary incentives can help attract peers\nin P2P content delivery systems. We commissioned a professional survey of\npeople around theUnited States to answer several relevant questions. We found\nthat 51% of the 876 respondents--substantially larger than our\nexpectations--answered ""yes"" to whether they would participate for suitable\nfinancial incentives. Encouraged by the results of the survey, we propose\nGringotts, a system to structure incentives and securely incorporate P2P\ndelivery into content delivery systems. Gringotts provides a novel Proof of\nDelivery mechanism that allows content providers to verify correct delivery of\ntheir files, and shows how to use cryptocurrency to pay peers while guarding\nagainst liars and Sybil attacks.'"
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1807.02264,https://arxiv.org/abs/1807.02264,"b'Abstract:  We consider reinforcement learning in input-driven environments, where an\nexogenous, stochastic input process affects the dynamics of the system. Input\nprocesses arise in many applications, including queuing systems, robotics\ncontrol with disturbances, and object tracking. Since the state dynamics and\nrewards depend on the input process, the state alone provides limited\ninformation for the expected future returns. Therefore, policy gradient methods\nwith standard state-dependent baselines suffer high variance during training.\nWe derive a bias-free, input-dependent baseline to reduce this variance, and\nanalytically show its benefits over state-dependent baselines. We then propose\na meta-learning approach to overcome the complexity of learning a baseline that\ndepends on a long sequence of inputs. Our experimental results show that across\nenvironments from queuing systems, computer networks, and MuJoCo robotic\nlocomotion, input-dependent baselines consistently improve training stability\nand result in better eventual policies.'"
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1803.09615,https://arxiv.org/abs/1803.09615,"b""Abstract:  Homa is a new transport protocol for datacenter networks. It provides\nexceptionally low latency, especially for workloads with a high volume of very\nshort messages, and it also supports large messages and high network\nutilization. Homa uses in-network priority queues to ensure low latency for\nshort messages; priority allocation is managed dynamically by each receiver and\nintegrated with a receiver-driven flow control mechanism. Homa also uses\ncontrolled overcommitment of receiver downlinks to ensure efficient bandwidth\nutilization at high load. Our implementation of Homa delivers 99th percentile\nround-trip times less than 15{\\mu}s for short messages on a 10 Gbps network\nrunning at 80% load. These latencies are almost 100x lower than the best\npublished measurements of an implementation. In simulations, Homa's latency is\nroughly equal to pFabric and significantly better than pHost, PIAS, and NDP for\nalmost all message sizes and workloads. Homa can also sustain higher network\nloads than pFabric, pHost, or PIAS."""
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1802.08730,https://arxiv.org/abs/1802.08730,"b'Abstract:  This paper develops a technique to detect whether the cross traffic competing\nwith a flow is elastic or not, and shows how to use the elasticity detector to\nimprove congestion control. If the cross traffic is elastic, i.e., made up of\nflows like Cubic or NewReno that increase their rate when they perceive\navailable bandwidth, then one should use a scheme that competes well with such\ntraffic. Such a scheme will not be able to control delays because the cross\ntraffic will not cooperate to maintain low delays. If, however, cross traffic\nis inelastic, then one can use a suitable delay-controlled algorithm. Our\nelasticity detector uses an asymmetric sinusoidal pulse pattern and estimates\nelasticity by computing the frequency response (FFT) of the cross traffic\nestimate; we have measured its accuracy to be over 90%. We present the design\nand evaluation of Nimbus, a congestion control protocol that uses the\nelasticity detector to switch between delay-control and TCP-competitive modes.\nOur results on emulated and real-world paths show that Nimbus achieves\nthroughput comparable to or better than Cubic always, but with delays that are\nmuch lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to\nCubic, and has significantly lower delay by 40-50 ms. Compared to Copa, which\nalso switches between a delay-controlling and a TCP-competitive mode, Nimbus is\nmore robust at correctly detecting the nature of cross traffic, and unlike\nCopa, it is usable by a variety of delay-based and TCP-competitive methods.'"
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1802.04948,https://arxiv.org/abs/1802.04948,"b'Abstract:  Neural networks have been shown to be an effective tool for learning\nalgorithms over graph-structured data. However, graph representation\ntechniques---that convert graphs to real-valued vectors for use with neural\nnetworks---are still in their infancy. Recent works have proposed several\napproaches (e.g., graph convolutional networks), but these methods have\ndifficulty scaling and generalizing to graphs with different sizes and shapes.\nWe present Graph2Seq, a new technique that represents vertices of graphs as\ninfinite time-series. By not limiting the representation to a fixed dimension,\nGraph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq\nis also reversible, allowing full recovery of the graph structure from the\nsequences. By analyzing a formal computational model for graph representation,\nwe show that an unbounded sequence is necessary for scalability. Our\nexperimental results with Graph2Seq show strong generalization and new\nstate-of-the-art performance on a variety of graph combinatorial optimization\nproblems.'"
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1802.03680,https://arxiv.org/abs/1802.03680,"b'Abstract:  Mapping road networks is currently both expensive and labor-intensive.\nHigh-resolution aerial imagery provides a promising avenue to automatically\ninfer a road network. Prior work uses convolutional neural networks (CNNs) to\ndetect which pixels belong to a road (segmentation), and then uses complex\npost-processing heuristics to infer graph connectivity. We show that these\nsegmentation methods have high error rates because noisy CNN outputs are\ndifficult to correct. We propose RoadTracer, a new method to automatically\nconstruct accurate road network maps from aerial images. RoadTracer uses an\niterative search process guided by a CNN-based decision function to derive the\nroad network graph directly from the output of the CNN. We compare our approach\nwith a segmentation method on fifteen cities, and find that at a 5% error rate,\nRoadTracer correctly captures 45% more junctions across these cities.'"
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1801.04519,https://arxiv.org/abs/1801.04519,b'Abstract:  Let $\\mathcal{A}$ and $\\mathcal{B}$\\ are $C^{\\huge \\ast}%\n$-algebras\\textbf{.} A\\textbf{ }linear map $\\phi:\\mathcal{A\\rightarrow B}$ is\n$C^{\\ast}$-Jordan homomorphism if it is a Jordan homomorphism which preserves\nthe adjoint operation. In this note we show that $C^{\\ast}$-Jordan\nhomomorphisms -- under mild assumptions -- preserving covariance set and\ncovariance coset in $C^{\\ast}$-algebras.'
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1702.02588,https://arxiv.org/abs/1702.02588,"b'Abstract:  As its price per bit drops, SSD is increasingly becoming the default storage\nmedium for cloud application databases. However, it has not become the\npreferred storage medium for key-value caches, even though SSD offers more than\n10x lower price per bit and sufficient performance compared to DRAM. This is\nbecause key-value caches need to frequently insert, update and evict small\nobjects. This causes excessive writes and erasures on flash storage, since\nflash only supports writes and erasures of large chunks of data. These\nexcessive writes and erasures significantly shorten the lifetime of flash,\nrendering it impractical to use for key-value caches. We present Flashield, a\nhybrid key-value cache that uses DRAM as a ""filter"" to minimize writes to SSD.\nFlashield performs light-weight machine learning profiling to predict which\nobjects are likely to be read frequently before getting updated; these objects,\nwhich are prime candidates to be stored on SSD, are written to SSD in large\nchunks sequentially. In order to efficiently utilize the cache\'s available\nmemory, we design a novel in-memory index for the variable-sized objects stored\non flash that requires only 4 bytes per object in DRAM. We describe Flashield\'s\ndesign and implementation and, we evaluate it on a real-world cache trace.\nCompared to state-of-the-art systems that suffer a write amplification of 2.5x\nor more, Flashield maintains a median write amplification of 0.5x without any\nloss of hit rate or throughput.'"
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1602.06045,https://arxiv.org/abs/1602.06045,"b'Abstract:  Switches today provide a small set of scheduling algorithms. While we can\ntweak scheduling parameters, we cannot modify algorithmic logic, or add a\ncompletely new algorithm, after the switch has been designed. This paper\npresents a design for a programmable packet scheduler, which allows scheduling\nalgorithms---potentially algorithms that are unknown today---to be programmed\ninto a switch without requiring hardware redesign.\nOur design builds on the observation that scheduling algorithms make two\ndecisions: in what order to schedule packets and when to schedule them.\nFurther, in many scheduling algorithms these decisions can be made when packets\nare enqueued. We leverage this observation to build a programmable scheduler\nusing a single abstraction: the push-in first-out queue (PIFO), a priority\nqueue that maintains the scheduling order and time for such algorithms.\nWe show that a programmable scheduler using PIFOs lets us program a wide\nvariety of scheduling algorithms. We present a detailed hardware design for\nthis scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area\noverhead on a 16-nm standard-cell library. Our design lets us program many\nsophisticated algorithms, such as a 5-level hierarchical scheduler with\nprogrammable scheduling algorithms at each level.'"
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1512.05023,https://arxiv.org/abs/1512.05023,"b""Abstract:  Many algorithms for congestion control, scheduling, network measurement,\nactive queue management, security, and load balancing require custom processing\nof packets as they traverse the data plane of a network switch. To run at line\nrate, these data-plane algorithms must be in hardware. With today's switch\nhardware, algorithms cannot be changed, nor new algorithms installed, after a\nswitch has been built.\nThis paper shows how to program data-plane algorithms in a high-level\nlanguage and compile those programs into low-level microcode that can run on\nemerging programmable line-rate switching chipsets. The key challenge is that\nthese algorithms create and modify algorithmic state. The key idea to achieve\nline-rate programmability for stateful algorithms is the notion of a packet\ntransaction : a sequential code block that is atomic and isolated from other\nsuch code blocks. We have developed this idea in Domino, a C-like imperative\nlanguage to express data-plane algorithms. We show with many examples that\nDomino provides a convenient and natural way to express sophisticated\ndata-plane algorithms, and show that these algorithms can be run at line rate\nwith modest estimated die-area overhead."""
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1512.01271,https://arxiv.org/abs/1512.01271,"b""Abstract:  Hybrid switching - in which a high bandwidth circuit switch (optical or\nwireless) is used in conjunction with a low bandwidth packet switch - is a\npromising alternative to interconnect servers in today's large scale\ndata-centers. Circuit switches offer a very high link rate, but incur a\nnon-trivial reconfiguration delay which makes their scheduling challenging. In\nthis paper, we demonstrate a lightweight, simple and nearly-optimal scheduling\nalgorithm that trades-off configuration costs with the benefits of\nreconfiguration that match the traffic demands. The algorithm has strong\nconnections to submodular optimization, has performance at least half that of\nthe optimal schedule and strictly outperforms state of the art in a variety of\ntraffic demand settings. These ideas naturally generalize: we see that indirect\nrouting leads to exponential connectivity; this is another phenomenon of the\npower of multi hop routing, distinct from the well-known load balancing\neffects."""
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1405.7143,https://arxiv.org/abs/1405.7143,"b'Abstract:  This paper presents a practical approach to rapidly introduce new dataplane\nfunctionality into networks: End-hosts embed tiny programs into packets to\nactively query and manipulate a network\'s internal state. We show how this\n""tiny packet program"" (TPP) interface gives end-hosts unprecedented visibility\ninto network behavior, enabling them to work with the network to achieve a\ncommon goal. Our design leverages what each component does best: (a) switches\nforward and execute tiny packet programs (at most 5 instructions) at line rate,\nand (b) end-hosts perform arbitrary computation on network state, which are\neasy to evolve. Using a hardware prototype on a NetFPGA, we show our design is\nfeasible, at a reasonable cost. By implementing three different research\nproposals, we show that TPPs are also useful. And finally, we present an\narchitecture in which they can be made secure.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1902.02816,https://arxiv.org/abs/1902.02816,"b""Abstract:  Modern microprocessors are equipped with Single Instruction Multiple Data\n(SIMD) or vector instructions which expose data level parallelism at a fine\ngranularity. Programmers exploit this parallelism by using low-level vector\nintrinsics in their code. However, once programs are written using vector\nintrinsics of a specific instruction set, the code becomes non-portable. Modern\ncompilers are unable to analyze and retarget the code to newer vector\ninstruction sets. Hence, programmers have to manually rewrite the same code\nusing vector intrinsics of a newer generation to exploit higher data widths and\ncapabilities of new instruction sets. This process is tedious, error-prone and\nrequires maintaining multiple code bases. We propose Revec, a compiler\noptimization pass which revectorizes already vectorized code, by retargeting it\nto use vector instructions of newer generations. The transformation is\ntransparent, happening at the compiler intermediate representation level, and\nenables performance portability of hand-vectorized code.\nRevec can achieve performance improvements in real-world performance critical\nkernels. In particular, Revec achieves geometric mean speedups of 1.160$\\times$\nand 1.430$\\times$ on fast integer unpacking kernels, and speedups of\n1.145$\\times$ and 1.195$\\times$ on hand-vectorized x265 media codec kernels\nwhen retargeting their SSE-series implementations to use AVX2 and AVX-512\nvector instructions respectively. We also extensively test Revec's impact on\n216 intrinsic-rich implementations of image processing and stencil kernels\nrelative to hand-retargeting."""
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1808.07412,https://arxiv.org/abs/1808.07412,"b""Abstract:  Statically estimating the number of processor clock cycles it takes to\nexecute a basic block of assembly instructions in steady state (throughput) is\nimportant for compiler backend optimizations such as register allocation,\ninstruction selection and instruction scheduling. This is complicated specially\nin modern x86-64 Complex Instruction Set Computer (CISC) machines with\nsophisticated processor microarchitectures. Traditionally, compiler writers\ninvest time experimenting and referring to processor manuals to analytically\nmodel modern processors with incomplete specifications. This is tedious, error\nprone and should be done for each processor generation. We present Ithemal, the\nfirst automatically learnt estimator to statically predict throughput of a set\nof basic block instructions using machine learning. Ithemal uses a novel\nDirected Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven\napproach for throughput estimation. We show that Ithemal is accurate than\nstate-of-the-art hand written tools used in compiler backends and static\nmachine code analyzers. In particular, our model has a worst case average error\nof 10.53% on actual throughput values when compared to best case average errors\nof 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's\nmachine code analyzer when compared on three different microarchitectures,\nwhile predicting throughput values at a faster rate than aforementioned tools.\nWe also show that Ithemal is portable, learning throughput estimation for Intel\nNehalem, Haswell and Skylake microarchitectures without requiring changes to\nits structure."""
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1807.01624,https://arxiv.org/abs/1807.01624,"b'Abstract:  Modern out-of-order processors have increased capacity to exploit instruction\nlevel parallelism (ILP) and memory level parallelism (MLP), e.g., by using wide\nsuperscalar pipelines and vector execution units, as well as deep buffers for\nin-flight memory requests. These resources, however, often exhibit poor\nutilization rates on workloads with large working sets, e.g., in-memory\ndatabases, key-value stores, and graph analytics, as compilers and hardware\nstruggle to expose ILP and MLP from the instruction stream automatically.\nIn this paper, we introduce the IMLP (Instruction and Memory Level\nParallelism) task programming model. IMLP tasks execute as coroutines that\nyield execution at annotated long-latency operations, e.g., memory accesses,\ndivisions, or unpredictable branches. IMLP tasks are interleaved on a single\nthread, and integrate well with thread parallelism and vectorization. Our DSL\nembedded in C++, Cimple, allows exploration of task scheduling and\ntransformations, such as buffering, vectorization, pipelining, and prefetching.\nWe demonstrate state-of-the-art performance on core algorithms used in\nin-memory databases that operate on arrays, hash tables, trees, and skip lists.\nCimple applications reach 2.5x throughput gains over hardware multithreading on\na multi-core, and 6.4x single thread speedup.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1805.00923,https://arxiv.org/abs/1805.00923,"b'Abstract:  The performance bottlenecks of graph applications depend not only on the\nalgorithm and the underlying hardware, but also on the size and structure of\nthe input graph. Programmers must try different combinations of a large set of\ntechniques to develop the best implementation for a specific algorithm and type\nof graph. Existing graph frameworks lack flexibility, supporting only a limited\nset of optimizations.\nThis paper introduces GraphIt, a new DSL for graph computations that\ngenerates fast implementations for algorithms with different performance\ncharacteristics running on graphs with different sizes and structures. GraphIt\nseparates what is computed (algorithm) from how it is computed (schedule).\nProgrammers specify the algorithm using an algorithm language, and performance\noptimizations are specified using a scheduling language. The algorithm language\nsimplifies expressing the algorithms. We formulate graph optimizations,\nincluding edge traversal direction, data layout, parallelization, cache, NUMA,\nand kernel fusion optimizations, as tradeoffs among locality, parallelism, and\nwork-efficiency. The scheduling language enables programmers to easily search\nthrough this complicated tradeoff space by composing together optimizations. We\nalso built an autotuner to automatically find high-performance schedules. The\ncompiler uses a new scheduling representation, the graph iteration space, to\nmodel, compose, and ensure the validity of the large number of optimizations.\nGraphIt outperforms the next fastest of six state-of-the-art shared-memory\nframeworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24\nout of 32 experiments by up to 4.8$\\times$, and is never more than 43% slower\nthan the fastest framework on the other experiments. GraphIt also reduces the\nlines of code by up to an order of magnitude compared to the next fastest\nframework.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1804.10694,https://arxiv.org/abs/1804.10694,"b'Abstract:  This paper introduces Tiramisu, a polyhedral framework designed to generate\nhigh performance code for multiple platforms including multicores, GPUs, and\ndistributed machines. Tiramisu introduces a scheduling language with novel\nextensions to explicitly manage the complexities that arise when targeting\nthese systems. The framework is designed for the areas of image processing,\nstencils, linear algebra and deep learning. Tiramisu has two main features: it\nrelies on a flexible representation based on the polyhedral model and it has a\nrich scheduling language allowing fine-grained control of optimizations.\nTiramisu uses a four-level intermediate representation that allows full\nseparation between the algorithms, loop transformations, data layouts, and\ncommunication. This separation simplifies targeting multiple hardware\narchitectures with the same algorithm. We evaluate Tiramisu by writing a set of\nimage processing, deep learning, and linear algebra benchmarks and compare them\nwith state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu\nmatches or outperforms existing compilers and libraries on different hardware\narchitectures, including multicore CPUs, GPUs, and distributed machines.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1804.10112,https://arxiv.org/abs/1804.10112,"b'Abstract:  This paper shows how to build a sparse tensor algebra compiler that is\nagnostic to tensor formats (data layouts). We develop an interface that\ndescribes formats in terms of their capabilities and properties, and show how\nto build a modular code generator where new formats can be added as plugins. We\nthen describe six implementations of the interface that compose to form the\ndense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants\nthereof. With these implementations at hand, our code generator can generate\ncode to compute any tensor algebra expression on any combination of the\naforementioned formats.\nTo demonstrate our technique, we have implemented it in the taco tensor\nalgebra compiler. Our modular code generator design makes it simple to add\nsupport for new tensor formats, and the performance of the generated code is\ncompetitive with hand-optimized implementations. Furthermore, by extending taco\nto support a wider range of formats specialized for different application and\ndata characteristics, we can improve end-user application performance. For\nexample, if input data is provided in the COO format, our technique allows\ncomputing a single matrix-vector multiplication directly with the data in COO,\nwhich is up to 3.6$\\times$ faster than by first converting the data to CSR.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1804.08733,https://arxiv.org/abs/1804.08733,"b""Abstract:  Modern microprocessors are equipped with single instruction multiple data\n(SIMD) or vector instruction sets which allow compilers to exploit superword\nlevel parallelism (SLP), a type of fine-grained parallelism. Current SLP\nauto-vectorization techniques use heuristics to discover vectorization\nopportunities in high-level language code. These heuristics are fragile, local\nand typically only present one vectorization strategy that is either accepted\nor rejected by a cost model. We present goSLP, a novel SLP auto-vectorization\nframework which solves the statement packing problem in a pairwise optimal\nmanner. Using an integer linear programming (ILP) solver, goSLP searches the\nentire space of statement packing opportunities for a whole function at a time,\nwhile limiting total compilation time to a few minutes. Furthermore, goSLP\noptimally solves the vector permutation selection problem using dynamic\nprogramming. We implemented goSLP in the LLVM compiler infrastructure,\nachieving a geometric mean speedup of 7.58% on SPEC2017fp, 2.42% on SPEC2006fp\nand 4.07% on NAS benchmarks compared to LLVM's existing SLP auto-vectorizer."""
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"b'Abstract:  In this position paper, we describe our vision of the future of machine\nprogramming through a categorical examination of three pillars of research.\nThose pillars are: (i) intention, (ii) invention, and(iii) adaptation.\nIntention emphasizes advancements in the human-to-computer and\ncomputer-to-machine-learning interfaces. Invention emphasizes the creation or\nrefinement of algorithms or core hardware and software building blocks through\nmachine learning (ML). Adaptation emphasizes advances in the use of ML-based\nconstructs to autonomously evolve software.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1803.00419,https://arxiv.org/abs/1803.00419,"b'Abstract:  High-performance DSL developers work hard to take advantage of modern\nhardware. The DSL compilers have to build their own complex middle-ends before\nthey can target a common back-end such as LLVM, which only handles single\ninstruction streams with SIMD instructions. We introduce Tiramisu, a common\nmiddle-end that can generate efficient code for modern processors and\naccelerators such as multicores, GPUs, FPGAs and distributed clusters. Tiramisu\nintroduces a novel three-level IR that separates the algorithm, how that\nalgorithm is executed, and where intermediate data are stored. This separation\nsimplifies optimization and makes targeting multiple hardware architectures\nfrom the same algorithm easier. As a result, DSL compilers can be made\nconsiderably less complex with no loss of performance while immediately\ntargeting multiple hardware or hardware combinations such as distributed nodes\nwith both CPUs and GPUs. We evaluated Tiramisu by creating a new middle-end for\nthe Halide and Julia compilers. We show that Tiramisu extends Halide and Julia\nwith many new capabilities including the ability to: express new algorithms\n(such as recurrent filters and non-rectangular iteration spaces), perform new\ncomplex loop nest transformations (such as wavefront parallelization, loop\nshifting and loop fusion) and generate efficient code for more architectures\n(such as combinations of distributed clusters, multicores, GPUs and FPGAs).\nFinally, we demonstrate that Tiramisu can generate very efficient code that\nmatches the highly optimized Intel MKL gemm (generalized matrix multiplication)\nimplementation, we also show speedups reaching 4X in Halide and 16X in Julia\ndue to optimizations enabled by Tiramisu.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1802.10574,https://arxiv.org/abs/1802.10574,"b'Abstract:  This paper shows how to optimize sparse tensor algebraic expressions by\nintroducing temporary tensors, called workspaces, into the resulting loop\nnests. We develop a new intermediate language for tensor operations called\nconcrete index notation that extends tensor index notation. Concrete index\nnotation expresses when and where sub-computations occur and what tensor they\nare stored into. We then describe the workspace optimization in this language,\nand how to compile it to sparse code by building on prior work in the\nliterature.\nWe demonstrate the importance of the optimization on several important sparse\ntensor kernels, including sparse matrix-matrix multiplication (SpMM), sparse\ntensor addition (SpAdd), and the matricized tensor times Khatri-Rao product\n(MTTKRP) used to factorize tensors. Our results show improvements over prior\nwork on tensor algebra compilation and brings the performance of these kernels\non par with state-of-the-art hand-optimized implementations. For example, SpMM\nwas not supported by prior tensor algebra compilers, the performance of MTTKRP\non the nell-2 data set improves by 35%, and MTTKRP can for the first time have\nsparse results.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1709.06416,https://arxiv.org/abs/1709.06416,"b'Abstract:  Data analytics applications combine multiple functions from different\nlibraries and frameworks. Even when each function is optimized in isolation,\nthe performance of the combined application can be an order of magnitude below\nhardware limits due to extensive data movement across these functions. To\naddress this problem, we propose Weld, a new interface between data-intensive\nlibraries that can optimize across disjoint libraries and functions. Weld\nexposes a lazily-evaluated API where diverse functions can submit their\ncomputations in a simple but general intermediate representation that captures\ntheir data-parallel structure. It then optimizes data movement across these\nfunctions and emits efficient code for diverse hardware. Weld can be integrated\ninto existing frameworks such as Spark, TensorFlow, Pandas and NumPy without\nchanging their user-facing APIs. We demonstrate that Weld can speed up\napplications using these frameworks by up to 29x.'"
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1608.01362,https://arxiv.org/abs/1608.01362,"b'Abstract:  Modern hardware systems are heavily underutilized when running large-scale\ngraph applications. While many in-memory graph frameworks have made substantial\nprogress in optimizing these applications, we show that it is still possible to\nachieve up to 4 $\\times$ speedups over the fastest frameworks by greatly\nimproving cache utilization. Previous systems have applied out-of-core\nprocessing techniques from the memory/disk boundary to the cache/DRAM boundary.\nHowever, we find that blindly applying such techniques is ineffective because\nof the much smaller performance gap between DRAM and cache. We present two\ntechniques that take advantage of the cache with minimal or no instruction\noverhead. The first, frequency based clustering, groups together frequently\naccessed vertices to improve the utilization of each cache line with no runtime\noverhead. The second, CSR segmenting, partitions the graph to restrict all\nrandom accesses to the cache, makes all DRAM access sequential, and merges\npartition results using a very low overhead cache-aware merge. Both techniques\ncan be easily implemented on top of optimized graph frameworks. Our techniques\ncombined give speedups of up to 4 $\\times$ for PageRank, Label Propagation and\nCollaborative Filtering, and 2 $\\times$ for Betweenness Centrality over the\nbest published results'"
Dimitri Antoniadis,Antoniadis_Dimitri,arXiv:1807.00272,https://arxiv.org/abs/1807.00272,"b'Abstract:  We present a compact model for Tunnel Field Effect Transistors (TFET), that\ncaptures sev- eral non-idealities such as the Trap Assisted Tunneling (TAT)\noriginating from interface traps (Dit), along with Verilog-A implementation. We\nshow that the TAT, together with band edge non-abruptness known as the Urbach\ntail, sets the lower limit of the sub-threshold swing and the minimum\nachievable current at a given temperature. Presence of charged trap states also\ncontributes to reduced gate efficiency. We show that we can decouple the\ncontribution of each of these processes and extract the intrinsic sub-threshold\nswing from a given experimental data. We derive closed form expressions of\nchannel potential, electric field and effective tunnel energy window to\naccurately capture the essential device physics of TFETs. We test the model\nagainst recently published exper- imental data, and simulate simple TFET\ncircuits using the Verilog-A model. The compact model provides a framework for\nTFET technology projections with improved device metrics such as better\nelectrostatic design, reduced TAT, material with better transport properties\netc.'"
Dimitri Antoniadis,Antoniadis_Dimitri,arXiv:1603.06654,https://arxiv.org/abs/1603.06654,"b'Abstract:  We provide a detailed study of the interface Trap Assisted Tunneling (TAT)\nmechanism in tunnel field effect transistors to show how it contributes a major\nleakage current path before the Band To Band Tunneling (BTBT) is initiated.\nWith a modified Shockley-Read-Hall formalism, we show that at room temperature,\nthe phonon assisted TAT current always dominates and obscures the steep turn ON\nof the BTBT current for common densities of traps. Our results are applicable\nto top gate, double gate and gate all around structures where the traps are\npositioned between the source-channel tunneling region. Since the TAT has\nstrong dependence on electric field, any effort to increase the BTBT current by\nenhancing local electric field also increases the leakage current. Unless the\nBTBT current can be increased separately, calculations show that the trap\ndensity Dit has to be decreased by 40-100 times compared with the state of the\nart in order for the steep turn ON (for III-V materials) to be clearly\nobservable at room temperature. We find that the combination of the intrinsic\nsharpness of the band edges (Urbach tail) and the surface trap density\ndetermines the subthreshold swing.'"
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1808.00826,https://arxiv.org/abs/1808.00826,"b'Abstract:  Prior research has proposed technical solutions to use peer-to-peer (P2P)\ncontent delivery to serve Internet video, showing that it can reduce costs to\ncontent providers. Yet, such methods have not become widespread except for a\nfew niche instances. An important challenge is incentivization: what tangible\nbenefits does P2P content delivery offer users who bring resources to the\ntable? In this paper, we ask whether monetary incentives can help attract peers\nin P2P content delivery systems. We commissioned a professional survey of\npeople around theUnited States to answer several relevant questions. We found\nthat 51% of the 876 respondents--substantially larger than our\nexpectations--answered ""yes"" to whether they would participate for suitable\nfinancial incentives. Encouraged by the results of the survey, we propose\nGringotts, a system to structure incentives and securely incorporate P2P\ndelivery into content delivery systems. Gringotts provides a novel Proof of\nDelivery mechanism that allows content providers to verify correct delivery of\ntheir files, and shows how to use cryptocurrency to pay peers while guarding\nagainst liars and Sybil attacks.'"
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1802.08730,https://arxiv.org/abs/1802.08730,"b'Abstract:  This paper develops a technique to detect whether the cross traffic competing\nwith a flow is elastic or not, and shows how to use the elasticity detector to\nimprove congestion control. If the cross traffic is elastic, i.e., made up of\nflows like Cubic or NewReno that increase their rate when they perceive\navailable bandwidth, then one should use a scheme that competes well with such\ntraffic. Such a scheme will not be able to control delays because the cross\ntraffic will not cooperate to maintain low delays. If, however, cross traffic\nis inelastic, then one can use a suitable delay-controlled algorithm. Our\nelasticity detector uses an asymmetric sinusoidal pulse pattern and estimates\nelasticity by computing the frequency response (FFT) of the cross traffic\nestimate; we have measured its accuracy to be over 90%. We present the design\nand evaluation of Nimbus, a congestion control protocol that uses the\nelasticity detector to switch between delay-control and TCP-competitive modes.\nOur results on emulated and real-world paths show that Nimbus achieves\nthroughput comparable to or better than Cubic always, but with delays that are\nmuch lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to\nCubic, and has significantly lower delay by 40-50 ms. Compared to Copa, which\nalso switches between a delay-controlling and a TCP-competitive mode, Nimbus is\nmore robust at correctly detecting the nature of cross traffic, and unlike\nCopa, it is usable by a variety of delay-based and TCP-competitive methods.'"
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1802.03680,https://arxiv.org/abs/1802.03680,"b'Abstract:  Mapping road networks is currently both expensive and labor-intensive.\nHigh-resolution aerial imagery provides a promising avenue to automatically\ninfer a road network. Prior work uses convolutional neural networks (CNNs) to\ndetect which pixels belong to a road (segmentation), and then uses complex\npost-processing heuristics to infer graph connectivity. We show that these\nsegmentation methods have high error rates because noisy CNN outputs are\ndifficult to correct. We propose RoadTracer, a new method to automatically\nconstruct accurate road network maps from aerial images. RoadTracer uses an\niterative search process guided by a CNN-based decision function to derive the\nroad network graph directly from the output of the CNN. We compare our approach\nwith a segmentation method on fifteen cities, and find that at a 5% error rate,\nRoadTracer correctly captures 45% more junctions across these cities.'"
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1602.06045,https://arxiv.org/abs/1602.06045,"b'Abstract:  Switches today provide a small set of scheduling algorithms. While we can\ntweak scheduling parameters, we cannot modify algorithmic logic, or add a\ncompletely new algorithm, after the switch has been designed. This paper\npresents a design for a programmable packet scheduler, which allows scheduling\nalgorithms---potentially algorithms that are unknown today---to be programmed\ninto a switch without requiring hardware redesign.\nOur design builds on the observation that scheduling algorithms make two\ndecisions: in what order to schedule packets and when to schedule them.\nFurther, in many scheduling algorithms these decisions can be made when packets\nare enqueued. We leverage this observation to build a programmable scheduler\nusing a single abstraction: the push-in first-out queue (PIFO), a priority\nqueue that maintains the scheduling order and time for such algorithms.\nWe show that a programmable scheduler using PIFOs lets us program a wide\nvariety of scheduling algorithms. We present a detailed hardware design for\nthis scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area\noverhead on a 16-nm standard-cell library. Our design lets us program many\nsophisticated algorithms, such as a 5-level hierarchical scheduler with\nprogrammable scheduling algorithms at each level.'"
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1512.05023,https://arxiv.org/abs/1512.05023,"b""Abstract:  Many algorithms for congestion control, scheduling, network measurement,\nactive queue management, security, and load balancing require custom processing\nof packets as they traverse the data plane of a network switch. To run at line\nrate, these data-plane algorithms must be in hardware. With today's switch\nhardware, algorithms cannot be changed, nor new algorithms installed, after a\nswitch has been built.\nThis paper shows how to program data-plane algorithms in a high-level\nlanguage and compile those programs into low-level microcode that can run on\nemerging programmable line-rate switching chipsets. The key challenge is that\nthese algorithms create and modify algorithmic state. The key idea to achieve\nline-rate programmability for stateful algorithms is the notion of a packet\ntransaction : a sequential code block that is atomic and isolated from other\nsuch code blocks. We have developed this idea in Domino, a C-like imperative\nlanguage to express data-plane algorithms. We show with many examples that\nDomino provides a convenient and natural way to express sophisticated\ndata-plane algorithms, and show that these algorithms can be run at line rate\nwith modest estimated die-area overhead."""
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1206.0418,https://arxiv.org/abs/1206.0418,"b""Abstract:  This paper presents an analysis of spinal codes, a class of rateless codes\nproposed recently. We prove that spinal codes achieve Shannon capacity for the\nbinary symmetric channel (BSC) and the additive white Gaussian noise (AWGN)\nchannel with an efficient polynomial-time encoder and decoder. They are the\nfirst rateless codes with proofs of these properties for BSC and AWGN. The key\nidea in the spinal code is the sequential application of a hash function over\nthe message bits. The sequential structure of the code turns out to be crucial\nfor efficient decoding. Moreover, counter to the wisdom of having an expander\nstructure in good codes, we show that the spinal code, despite its sequential\nstructure, achieves capacity. The pseudo-randomness provided by a hash function\nsuffices for this purpose. Our proof introduces a variant of Gallager's result\ncharacterizing the error exponent of random codes for any memoryless channel.\nWe present a novel application of these error-exponent results within the\nframework of an efficient sequential code. The application of a hash function\nover the message bits provides a methodical and effective way to de-randomize\nShannon's random codebook construction."""
Hari Balakrishnan,Balakrishnan_Hari,arXiv:cs/0104012,https://arxiv.org/abs/cs/0104012,"b'Abstract:  This paper describes the implementation and evaluation of an operating system\nmodule, the Congestion Manager (CM), which provides integrated network flow\nmanagement and exports a convenient programming interface that allows\napplications to be notified of, and adapt to, changing network conditions. We\ndescribe the API by which applications interface with the CM, and the\narchitectural considerations that factored into the design. To evaluate the\narchitecture and API, we describe our implementations of TCP; a streaming\nlayered audio/video application; and an interactive audio application using the\nCM, and show that they achieve adaptive behavior without incurring much\nend-system overhead. All flows including TCP benefit from the sharing of\ncongestion information, and applications are able to incorporate new\nfunctionality such as congestion control and adaptive behavior.'"
Regina Barzilay,Barzilay_Regina,arXiv:1902.09492,https://arxiv.org/abs/1902.09492,"b'Abstract:  We introduce a novel method for multilingual transfer that utilizes deep\ncontextual embeddings, pretrained in an unsupervised fashion. While contextual\nembeddings have been shown to yield richer representations of meaning compared\nto their static counterparts, aligning them poses a challenge due to their\ndynamic nature. To this end, we construct context-independent variants of the\noriginal monolingual spaces and utilize their mapping to derive an alignment\nfor the context-dependent spaces. This mapping readily supports processing of a\ntarget language, improving transfer by context-aware embeddings. Our\nexperimental results demonstrate the effectiveness of this approach for\nzero-shot and few-shot learning of dependency parsing. Specifically, our method\nconsistently outperforms the previous state-of-the-art on 6 target languages,\nyielding an improvement of 6.8 LAS points on average.'"
Regina Barzilay,Barzilay_Regina,arXiv:1812.01070,https://arxiv.org/abs/1812.01070,"b'Abstract:  We view molecular optimization as a graph-to-graph translation problem. The\ngoal is to learn to map from one molecular graph to another with better\nproperties based on an available corpus of paired molecules. Since molecules\ncan be optimized in different ways, there are multiple viable translations for\neach input graph. A key challenge is therefore to model diverse translation\noutputs. Our primary contributions include a junction tree encoder-decoder for\nlearning diverse graph translations along with a novel adversarial training\nmethod for aligning distributions of molecules. Diverse output distributions in\nour model are explicitly realized by low-dimensional latent vectors that\nmodulate the translation process. We evaluate our model on multiple molecular\noptimization tasks and show that our model outperforms previous\nstate-of-the-art baselines.'"
Regina Barzilay,Barzilay_Regina,arXiv:1810.13083,https://arxiv.org/abs/1810.13083,"b'Abstract:  Most modern Information Extraction (IE) systems are implemented as sequential\ntaggers and only model local dependencies. Non-local and non-sequential context\nis, however, a valuable source of information to improve predictions. In this\npaper, we introduce GraphIE, a framework that operates over a graph\nrepresenting a broad set of dependencies between textual units (i.e. words or\nsentences). The algorithm propagates information between connected nodes\nthrough graph convolutions, generating a richer representation that can be\nexploited to improve word-level predictions. Evaluation on three different\ntasks - namely social media, textual and visual information extraction - shows\nthat GraphIE consistently outperforms the state-of-the-art sequence tagging\nmodel by a significant margin.'"
Regina Barzilay,Barzilay_Regina,arXiv:1809.02256,https://arxiv.org/abs/1809.02256,"b'Abstract:  We propose a mixture-of-experts approach for unsupervised domain adaptation\nfrom multiple sources. The key idea is to explicitly capture the relationship\nbetween a target example and different source domains. This relationship,\nexpressed by a point-to-set metric, determines how to combine predictors\ntrained on various domains. The metric is learned in an unsupervised fashion\nusing meta-training. Experimental results on sentiment analysis and\npart-of-speech tagging demonstrate that our approach consistently outperforms\nmultiple baselines and can robustly handle negative transfer.'"
Regina Barzilay,Barzilay_Regina,arXiv:1808.09367,https://arxiv.org/abs/1808.09367,"b'Abstract:  Attention-based models are successful when trained on large amounts of data.\nIn this paper, we demonstrate that even in the low-resource scenario, attention\ncan be learned effectively. To this end, we start with discrete human-annotated\nrationales and map them into continuous attention. Our central hypothesis is\nthat this mapping is general across domains, and thus can be transferred from\nresource-rich domains to low-resource ones. Our model jointly learns a\ndomain-invariant representation and induces the desired mapping between\nrationales and attention. Our empirical results validate this hypothesis and\nshow that our approach delivers significant gains over state-of-the-art\nbaselines, yielding over 15% average error reduction on benchmark datasets.'"
Regina Barzilay,Barzilay_Regina,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"b'Abstract:  In this position paper, we describe our vision of the future of machine\nprogramming through a categorical examination of three pillars of research.\nThose pillars are: (i) intention, (ii) invention, and(iii) adaptation.\nIntention emphasizes advancements in the human-to-computer and\ncomputer-to-machine-learning interfaces. Invention emphasizes the creation or\nrefinement of algorithms or core hardware and software building blocks through\nmachine learning (ML). Adaptation emphasizes advances in the use of ML-based\nconstructs to autonomously evolve software.'"
Regina Barzilay,Barzilay_Regina,arXiv:1802.04364,https://arxiv.org/abs/1802.04364,"b'Abstract:  We seek to automate the design of molecules based on specific chemical\nproperties. In computational terms, this task involves continuous embedding and\ngeneration of molecular graphs. Our primary contribution is the direct\nrealization of molecular graphs, a task previously approached by generating\nlinear SMILES strings instead of graphs. Our junction tree variational\nautoencoder generates molecular graphs in two phases, by first generating a\ntree-structured scaffold over chemical substructures, and then combining them\ninto a molecule with a graph message passing network. This approach allows us\nto incrementally expand molecules while maintaining chemical validity at every\nstep. We evaluate our model on multiple tasks ranging from molecular generation\nto optimization. Across these tasks, our model outperforms previous\nstate-of-the-art baselines by a significant margin.'"
Regina Barzilay,Barzilay_Regina,arXiv:1709.04555,https://arxiv.org/abs/1709.04555,"b'Abstract:  The prediction of organic reaction outcomes is a fundamental problem in\ncomputational chemistry. Since a reaction may involve hundreds of atoms, fully\nexploring the space of possible transformations is intractable. The current\nsolution utilizes reaction templates to limit the space, but it suffers from\ncoverage and efficiency issues. In this paper, we propose a template-free\napproach to efficiently explore the space of product molecules by first\npinpointing the reaction center -- the set of nodes and edges where graph edits\noccur. Since only a small number of atoms contribute to reaction center, we can\ndirectly enumerate candidate products. The generated candidates are scored by a\nWeisfeiler-Lehman Difference Network that models high-order interactions\nbetween changes occurring at nodes across the molecule. Our framework\noutperforms the top-performing template-based approach with a 10\\% margin,\nwhile running orders of magnitude faster. Finally, we demonstrate that the\nmodel accuracy rivals the performance of domain experts.'"
Regina Barzilay,Barzilay_Regina,arXiv:1708.00133,https://arxiv.org/abs/1708.00133,"b'Abstract:  In this paper, we explore the utilization of natural language to drive\ntransfer for reinforcement learning (RL). Despite the wide-spread application\nof deep RL techniques, learning generalized policy representations that work\nacross domains remains a challenging problem. We demonstrate that textual\ndescriptions of environments provide a compact intermediate channel to\nfacilitate effective policy transfer. Specifically, by learning to ground the\nmeaning of text to the dynamics of the environment such as transitions and\nrewards, an autonomous agent can effectively bootstrap policy learning on a new\ndomain given its description. We employ a model-based RL approach consisting of\na differentiable planning module, a model-free component and a factorized state\nrepresentation to effectively use entity descriptions. Our model outperforms\nprior work on both transfer and multi-task scenarios in a variety of different\nenvironments. For instance, we achieve up to 14% and 11.5% absolute improvement\nover previously existing models in terms of average and initial rewards,\nrespectively.'"
Regina Barzilay,Barzilay_Regina,arXiv:1707.03938,https://arxiv.org/abs/1707.03938,"b'Abstract:  The interpretation of spatial references is highly contextual, requiring\njoint inference over both language and the environment. We consider the task of\nspatial reasoning in a simulated environment, where an agent can act and\nreceive rewards. The proposed model learns a representation of the world\nsteered by instruction text. This design allows for precise alignment of local\nneighborhoods with corresponding verbalizations, while also handling global\nreferences in the instructions. We train our model with reinforcement learning\nusing a variant of generalized value iteration. The model outperforms\nstate-of-the-art approaches on several metrics, yielding a 45% reduction in\ngoal localization error.'"
Regina Barzilay,Barzilay_Regina,arXiv:1705.09655,https://arxiv.org/abs/1705.09655,"b'Abstract:  This paper focuses on style transfer on the basis of non-parallel text. This\nis an instance of a broad family of problems including machine translation,\ndecipherment, and sentiment modification. The key challenge is to separate the\ncontent from other aspects such as style. We assume a shared latent content\ndistribution across different text corpora, and propose a method that leverages\nrefined alignment of latent representations to perform style transfer. The\ntransferred sentences from one style should match example sentences from the\nother style as a population. We demonstrate the effectiveness of this\ncross-alignment method on three tasks: sentiment modification, decipherment of\nword substitution ciphers, and recovery of word order.'"
Regina Barzilay,Barzilay_Regina,arXiv:1705.09037,https://arxiv.org/abs/1705.09037,"b'Abstract:  The design of neural architectures for structured objects is typically guided\nby experimental insights rather than a formal process. In this work, we appeal\nto kernels over combinatorial structures, such as sequences and graphs, to\nderive appropriate neural operations. We introduce a class of deep recurrent\nneural operations and formally characterize their associated kernel spaces. Our\nrecurrent modules compare the input to virtual reference objects (cf. filters\nin CNN) via the kernels. Similar to traditional neural operations, these\nreference objects are parameterized and directly optimized in end-to-end\ntraining. We empirically evaluate the proposed class of neural architectures on\nstandard applications such as language modeling and molecular graph regression,\nachieving state-of-the-art results across these applications.'"
Regina Barzilay,Barzilay_Regina,arXiv:1702.07015,https://arxiv.org/abs/1702.07015,"b'Abstract:  This paper focuses on unsupervised modeling of morphological families,\ncollectively comprising a forest over the language vocabulary. This formulation\nenables us to capture edgewise properties reflecting single-step morphological\nderivations, along with global distributional properties of the entire forest.\nThese global properties constrain the size of the affix set and encourage\nformation of tight morphological families. The resulting objective is solved\nusing Integer Linear Programming (ILP) paired with contrastive estimation. We\ntrain the model by alternating between optimizing the local log-linear model\nand the global ILP objective. We evaluate our system on three tasks: root\ndetection, clustering of morphological families and segmentation. Our\nexperiments demonstrate that our model yields consistent gains in all three\ntasks compared with the best published results.'"
Regina Barzilay,Barzilay_Regina,arXiv:1701.00188,https://arxiv.org/abs/1701.00188,"b'Abstract:  We introduce a neural method for transfer learning between two (source and\ntarget) classification tasks or aspects over the same domain. Rather than\ntraining on target labels, we use a few keywords pertaining to source and\ntarget aspects indicating sentence relevance instead of document class labels.\nDocuments are encoded by learning to embed and softly select relevant sentences\nin an aspect-dependent manner. A shared classifier is trained on the source\nencoded documents and labels, and applied to target encoded documents. We\nensure transfer through aspect-adversarial training so that encoded documents\nare, as sets, aspect-invariant. Experimental results demonstrate that our\napproach outperforms different baselines and model variants on two datasets,\nyielding an improvement of 27% on a pathology dataset and 5% on a review\ndataset.'"
Regina Barzilay,Barzilay_Regina,arXiv:1608.03000,https://arxiv.org/abs/1608.03000,"b'Abstract:  This paper explores the task of translating natural language queries into\nregular expressions which embody their meaning. In contrast to prior work, the\nproposed neural model does not utilize domain-specific crafting, learning to\ntranslate directly from a parallel corpus. To fully explore the potential of\nneural models, we propose a methodology for collecting a large corpus of\nregular expression, natural language pairs. Our resulting model achieves a\nperformance gain of 19.6% over previous state-of-the-art models.'"
Regina Barzilay,Barzilay_Regina,arXiv:1607.02902,https://arxiv.org/abs/1607.02902,"b'Abstract:  We present a novel technique for automatic program correction in MOOCs,\ncapable of fixing both syntactic and semantic errors without manual, problem\nspecific correction strategies. Given an incorrect student program, it\ngenerates candidate programs from a distribution of likely corrections, and\nchecks each candidate for correctness against a test suite.\nThe key observation is that in MOOCs many programs share similar code\nfragments, and the seq2seq neural network model, used in the natural-language\nprocessing task of machine translation, can be modified and trained to recover\nthese fragments.\nExperiment shows our scheme can correct 29% of all incorrect submissions and\nout-performs state of the art approach which requires manual, problem specific\ncorrection strategies.'"
Regina Barzilay,Barzilay_Regina,arXiv:1606.04155,https://arxiv.org/abs/1606.04155,"b'Abstract:  Prediction without justification has limited applicability. As a remedy, we\nlearn to extract pieces of input text as justifications -- rationales -- that\nare tailored to be short and coherent, yet sufficient for making the same\nprediction. Our approach combines two modular components, generator and\nencoder, which are trained to operate well together. The generator specifies a\ndistribution over text fragments as candidate rationales and these are passed\nthrough the encoder for prediction. Rationales are never given during training.\nInstead, the model is regularized by desiderata for rationales. We evaluate the\napproach on multi-aspect sentiment analysis against manually annotated test\ncases. Our approach outperforms attention-based baseline by a significant\nmargin. We also successfully illustrate the method on the question retrieval\ntask.'"
Regina Barzilay,Barzilay_Regina,arXiv:1603.07954,https://arxiv.org/abs/1603.07954,"b'Abstract:  Most successful information extraction systems operate with access to a large\ncollection of documents. In this work, we explore the task of acquiring and\nincorporating external evidence to improve extraction accuracy in domains where\nthe amount of training data is scarce. This process entails issuing search\nqueries, extraction from new sources and reconciliation of extracted values,\nwhich are repeated until sufficient evidence is collected. We approach the\nproblem using a reinforcement learning framework where our model learns to\nselect optimal actions based on contextual information. We employ a deep\nQ-network, trained to optimize a reward function that reflects extraction\naccuracy while penalizing extra effort. Our experiments on two databases -- of\nshooting incidents, and food adulteration cases -- demonstrate that our system\nsignificantly outperforms traditional extractors and a competitive\nmeta-classifier baseline.'"
Regina Barzilay,Barzilay_Regina,arXiv:1512.05726,https://arxiv.org/abs/1512.05726,"b'Abstract:  Question answering forums are rapidly growing in size with no effective\nautomated ability to refer to and reuse answers already available for previous\nposted questions. In this paper, we develop a methodology for finding\nsemantically related questions. The task is difficult since 1) key pieces of\ninformation are often buried in extraneous details in the question body and 2)\navailable annotations on similar questions are scarce and fragmented. We design\na recurrent and convolutional model (gated convolution) to effectively map\nquestions to their semantic representations. The models are pre-trained within\nan encoder-decoder framework (from body to title) on the basis of the entire\nraw corpus, and fine-tuned discriminatively from limited annotations. Our\nevaluation demonstrates that our model yields substantial gains over a standard\nIR baseline and various neural network architectures (including CNNs, LSTMs and\nGRUs).'"
Regina Barzilay,Barzilay_Regina,arXiv:1508.04112,https://arxiv.org/abs/1508.04112,"b'Abstract:  The success of deep learning often derives from well-chosen operational\nbuilding blocks. In this work, we revise the temporal convolution operation in\nCNNs to better adapt it to text processing. Instead of concatenating word\nrepresentations, we appeal to tensor algebra and use low-rank n-gram tensors to\ndirectly exploit interactions between words already at the convolution stage.\nMoreover, we extend the n-gram convolution to non-consecutive words to\nrecognize patterns with intervening words. Through a combination of low-rank\ntensors, and pattern weighting, we can efficiently evaluate the resulting\nconvolution operation via dynamic programming. We test the resulting\narchitecture on standard sentiment classification and news categorization\ntasks. Our model achieves state-of-the-art performance both in terms of\naccuracy and training speed. For instance, we obtain 51.2% accuracy on the\nfine-grained sentiment classification task.'"
Regina Barzilay,Barzilay_Regina,arXiv:1506.08941,https://arxiv.org/abs/1506.08941,"b'Abstract:  In this paper, we consider the task of learning control policies for\ntext-based games. In these games, all interactions in the virtual world are\nthrough text and the underlying state is not observed. The resulting language\nbarrier makes such environments challenging for automatic game players. We\nemploy a deep reinforcement learning framework to jointly learn state\nrepresentations and action policies using game rewards as feedback. This\nframework enables us to map text descriptions into vector representations that\ncapture the semantics of the game states. We evaluate our approach on two game\nworlds, comparing against baselines using bag-of-words and bag-of-bigrams for\nstate representations. Our algorithm outperforms the baselines on both worlds\ndemonstrating the importance of learning expressive representations.'"
Regina Barzilay,Barzilay_Regina,arXiv:1503.02335,https://arxiv.org/abs/1503.02335,"b'Abstract:  Most state-of-the-art systems today produce morphological analysis based only\non orthographic patterns. In contrast, we propose a model for unsupervised\nmorphological analysis that integrates orthographic and semantic views of\nwords. We model word formation in terms of morphological chains, from base\nwords to the observed words, breaking the chains into parent-child relations.\nWe use log-linear models with morpheme and word-level features to predict\npossible parents, including their modifications, for each word. The limited set\nof candidate parents for each word render contrastive estimation feasible. Our\nmodel consistently matches or outperforms five state-of-the-art systems on\nArabic, English and Turkish.'"
Regina Barzilay,Barzilay_Regina,arXiv:1401.6422,https://arxiv.org/abs/1401.6422,"b'Abstract:  We present a model for aggregation of product review snippets by joint aspect\nidentification and sentiment analysis. Our model simultaneously identifies an\nunderlying set of ratable aspects presented in the reviews of a product (e.g.,\nsushi and miso for a Japanese restaurant) and determines the corresponding\nsentiment of each aspect. This approach directly enables discovery of\nhighly-rated or inconsistent aspects of a product. Our generative model admits\nan efficient variational mean-field inference algorithm. It is also easily\nextensible, and we describe several modifications and their effects on model\nstructure and inference. We test our model on two tasks, joint aspect\nidentification and sentiment analysis on a set of Yelp reviews and aspect\nidentification alone on a set of medical summaries. We evaluate the performance\nof the model on aspect identification, sentiment analysis, and per-word\nlabeling accuracy. We demonstrate that our model outperforms applicable\nbaselines by a considerable margin, yielding up to 32% relative error reduction\non aspect identification and up to 20% relative error reduction on sentiment\nanalysis.'"
Regina Barzilay,Barzilay_Regina,arXiv:1401.5695,https://arxiv.org/abs/1401.5695,"b'Abstract:  We demonstrate the effectiveness of multilingual learning for unsupervised\npart-of-speech tagging. The central assumption of our work is that by combining\ncues from multiple languages, the structure of each becomes more apparent. We\nconsider two ways of applying this intuition to the problem of unsupervised\npart-of-speech tagging: a model that directly merges tag structures for a pair\nof languages into a single sequence and a second model which instead\nincorporates multilingual context using latent variables. Both approaches are\nformulated as hierarchical Bayesian models, using Markov Chain Monte Carlo\nsampling techniques for inference. Our results demonstrate that by\nincorporating multilingual evidence we can achieve impressive performance gains\nacross a range of scenarios. We also found that performance improves steadily\nas the number of available languages increases.'"
Regina Barzilay,Barzilay_Regina,arXiv:1401.5390,https://arxiv.org/abs/1401.5390,"b'Abstract:  Domain knowledge is crucial for effective performance in autonomous control\nsystems. Typically, human effort is required to encode this knowledge into a\ncontrol algorithm. In this paper, we present an approach to language grounding\nwhich automatically interprets text in the context of a complex control\napplication, such as a game, and uses domain knowledge extracted from the text\nto improve control performance. Both text analysis and control strategies are\nlearned jointly using only a feedback signal inherent to the application. To\neffectively leverage textual information, our method automatically extracts the\ntext segment most relevant to the current game state, and labels it with a\ntask-centric predicate structure. This labeled text is then used to bias an\naction selection policy for the game, guiding it towards promising regions of\nthe action space. We encode our model for text analysis and game playing in a\nmulti-layer neural network, representing linguistic decisions via latent\nvariables in the hidden layers, and game action quality via the output layer.\nOperating within the Monte-Carlo Search framework, we estimate model parameters\nusing feedback from simulated games. We apply our approach to the complex\nstrategy game Civilization II using the official game manual as the text guide.\nOur results show that a linguistically-informed game-playing agent\nsignificantly outperforms its language-unaware counterpart, yielding a 34%\nabsolute improvement and winning over 65% of games when playing against the\nbuilt-in AI of Civilization.'"
Regina Barzilay,Barzilay_Regina,arXiv:1401.3488,https://arxiv.org/abs/1401.3488,"b'Abstract:  We present a novel Bayesian topic model for learning discourse-level document\nstructure. Our model leverages insights from discourse theory to constrain\nlatent topic assignments in a way that reflects the underlying organization of\ndocument topics. We propose a global model in which both topic selection and\nordering are biased to be similar across a collection of related documents. We\nshow that this space of orderings can be effectively represented using a\ndistribution over permutations called the Generalized Mallows Model. We apply\nour method to three complementary discourse-level tasks: cross-document\nalignment, document segmentation, and information ordering. Our experiments\nshow that incorporating our permutation-based model in these applications\nyields substantial improvements in performance over previously proposed\nmethods.'"
Regina Barzilay,Barzilay_Regina,arXiv:1401.3457,https://arxiv.org/abs/1401.3457,"b'Abstract:  This paper presents a new method for inferring the semantic properties of\ndocuments by leveraging free-text keyphrase annotations. Such annotations are\nbecoming increasingly abundant due to the recent dramatic growth in\nsemi-structured, user-generated online content. One especially relevant domain\nis product reviews, which are often annotated by their authors with pros/cons\nkeyphrases such as a real bargain or good value. These annotations are\nrepresentative of the underlying semantic properties; however, unlike expert\nannotations, they are noisy: lay authors may use different labels to denote the\nsame property, and some labels may be missing. To learn using such noisy\nannotations, we find a hidden paraphrase structure which clusters the\nkeyphrases. The paraphrase structure is linked with a latent topic model of the\nreview texts, enabling the system to predict the properties of unannotated\ndocuments and to effectively aggregate the semantic properties of multiple\nreviews. Our approach is implemented as a hierarchical Bayesian model with\njoint inference. We find that joint inference increases the robustness of the\nkeyphrase clustering and encourages the latent topics to correlate with\nsemantically meaningful properties. Multiple evaluations demonstrate that our\nmodel substantially outperforms alternative approaches for summarizing single\nand multiple documents into a set of semantically salient keyphrases.'"
Regina Barzilay,Barzilay_Regina,arXiv:cs/0405039,https://arxiv.org/abs/cs/0405039,"b'Abstract:  We consider the problem of modeling the content structure of texts within a\nspecific domain, in terms of the topics the texts address and the order in\nwhich these topics appear. We first present an effective knowledge-lean method\nfor learning content models from un-annotated documents, utilizing a novel\nadaptation of algorithms for Hidden Markov Models. We then apply our method to\ntwo complementary tasks: information ordering and extractive summarization. Our\nexperiments show that incorporating content models in these applications yields\nsubstantial improvement over previously-proposed methods.'"
Regina Barzilay,Barzilay_Regina,arXiv:cs/0304006,https://arxiv.org/abs/cs/0304006,"b'Abstract:  We address the text-to-text generation problem of sentence-level paraphrasing\n-- a phenomenon distinct from and more difficult than word- or phrase-level\nparaphrasing. Our approach applies multiple-sequence alignment to sentences\ngathered from unannotated comparable corpora: it learns a set of paraphrasing\npatterns represented by word lattice pairs and automatically determines how to\napply these patterns to rewrite new sentences. The results of our evaluation\nexperiments show that the system derives accurate paraphrases, outperforming\nbaseline systems.'"
Regina Barzilay,Barzilay_Regina,arXiv:cs/0205065,https://arxiv.org/abs/cs/0205065,"b'Abstract:  An important component of any generation system is the mapping dictionary, a\nlexicon of elementary semantic expressions and corresponding natural language\nrealizations. Typically, labor-intensive knowledge-based methods are used to\nconstruct the dictionary. We instead propose to acquire it automatically via a\nnovel multiple-pass algorithm employing multiple-sequence alignment, a\ntechnique commonly used in bioinformatics. Crucially, our method leverages\nlatent information contained in multi-parallel corpora -- datasets that supply\nseveral verbalizations of the corresponding semantics rather than just one.\nWe used our techniques to generate natural language versions of\ncomputer-generated mathematical proofs, with good results on both a\nper-component and overall-output basis. For example, in evaluations involving a\ndozen human judges, our system produced output whose readability and\nfaithfulness to the semantic input rivaled that of a traditional generation\nsystem.'"
Karl Berggren,Berggren_Karl,arXiv:1903.05101,https://arxiv.org/abs/1903.05101,"b'Abstract:  We propose the use of superconducting nanowires as both target and sensor for\ndirect detection of sub-GeV dark matter. With excellent sensitivity to small\nenergy deposits on electrons, and demonstrated low dark counts, such devices\ncould be used to probe electron recoils from dark matter scattering and\nabsorption processes. We demonstrate the feasibility of this idea using\nmeasurements of an existing fabricated tungsten-silicide nanowire prototype\nwith 0.8 eV energy threshold and 4.3 nanograms with 10 thousand seconds of\nexposure, which showed no dark counts. The results from this device already\nplace meaningful bounds on dark matter-electron interactions, including the\nstrongest terrestrial bounds on sub-eV dark photon absorption to date. Future\nexpected fabrication on larger scales and with lower thresholds should enable\nprobing new territory in the direct detection landscape, establishing the\ncomplementarity of this approach to other existing proposals.'"
Karl Berggren,Berggren_Karl,arXiv:1901.09702,https://arxiv.org/abs/1901.09702,"b'Abstract:  Interaction-free measurement (IFM) has been proposed as a means of\nhigh-resolution, low-damage imaging of radiation-sensitive samples, such as\nbiomolecules and proteins. The basic setup for IFM is a Mach-Zehnder\ninterferometer, and recent progress in nanofabricated electron diffraction\ngratings has made it possible to incorporate a Mach-Zehnder interferometer in a\ntransmission-electron microscope (TEM). Therefore, the limits of performance of\nIFM with such an interferometer and a shot-noise limited electron source (such\nas that in a TEM) are of interest. In this work, we compared the error\nprobability and sample damage for ideal IFM and classical imaging schemes,\nthrough theoretical analysis and numerical simulation. We considered a sample\nthat is either completely transparent or completely opaque at each pixel. In\nour analysis, we also evaluated the impact of an additional detector for\nscattered electrons. The additional detector resulted in reduction of error by\nup to an order of magnitude, for both IFM and classical schemes. We also\ninvestigated a sample re-illumination scheme based on updating priors after\neach round of illumination and found that this scheme further reduced error by\na factor of two. Implementation of these methods is likely achievable with\nexisting instrumentation and would result in improved resolution in low-dose\nelectron microscopy.'"
Karl Berggren,Berggren_Karl,arXiv:1901.03988,https://arxiv.org/abs/1901.03988,"b'Abstract:  Local, bulk response functions, e.g permittivity, and the macroscopic Maxwell\nequations completely specify the classical electromagnetic problem, which\nfeatures only wavelength $\\lambda$ and geometric scales. The above neglect of\nintrinsic electronic length scales $L_{\\text{e}}$ leads to an eventual\nbreakdown in the nanoscopic limit. Here, we present a general theoretical and\nexperimental framework for treating nanoscale electromagnetic phenomena. The\nframework features surface-response functions---known as the Feibelman\n$d$-parameters---which reintroduce the missing electronic length scales. As a\npart of our framework, we establish an experimental procedure to measure these\ncomplex, dispersive surface response functions, enabled by quasi-normal-mode\nperturbation theory and observations of pronounced nonclassical\neffects---spectral shifts in excess of 30% and the breakdown of Kreibig-like\nbroadening---in a quintessential multiscale architecture: film-coupled\nnanoresonators, with feature-sizes comparable to both $L_{\\text{e}}$ and\n$\\lambda$.'"
Karl Berggren,Berggren_Karl,arXiv:1812.05559,https://arxiv.org/abs/1812.05559,"b'Abstract:  We present the use of a commercially available fixed-angle multi-wavelength\nellipsometer for quickly measuring the thickness of NbN thin films for the\nfabrication and performance improvement of superconducting nanowire single\nphoton detectors. The process can determine the optical constants of absorbing\nthin films, removing the need for inaccurate approximations. The tool can be\nused to observe oxidation growth and allows thickness measurements to be\nintegrated into the characterization of various fabrication processes.'"
Karl Berggren,Berggren_Karl,arXiv:1811.05192,https://arxiv.org/abs/1811.05192,"b'Abstract:  The method of negative-tone-PMMA electron-beam lithography is investigated to\nimprove the performance of nanowire-based superconducting detectors. Using this\napproach, the superconducting nanowire single-photon detectors (SNSPDs) have\nbeen fabricated from thick 5-nm NbN film sputtered at the room temperature. To\ninvestigate the impact of this process, SNSPDs were prepared by positive-tone\nand negative-tone-PMMA lithography, and their electrical and photodetection\ncharacteristics at 4.2 K were compared. The SNSPDs made by negative-tone-PMMA\nlithography show higher critical-current density and higher photon count rate\nat various wavelengths. Our results suggest a higher negative-tone-PMMA\ntechnology may be preferable to the standard positive-tone-PMMA lithography for\nthis application.'"
Karl Berggren,Berggren_Karl,arXiv:1811.03991,https://arxiv.org/abs/1811.03991,"b'Abstract:  Conventional readout of a superconducting nanowire single-photon detector\n(SNSPD) sets an upper bound on the output voltage to be the product of the bias\ncurrent and the load impedance, $I_\\mathrm{B}\\times Z_\\mathrm{load}$, where\n$Z_\\mathrm{load}$ is limited to 50 $\\Omega$ in standard r.f. electronics. Here,\nwe break this limit by interfacing the 50 $\\Omega$ load and the SNSPD using an\nintegrated superconducting transmission line taper. The taper is a transformer\nthat effectively loads the SNSPD with high impedance without latching. It\nincreases the amplitude of the detector output while preserving the fast rising\nedge. Using a taper with a starting width of 500 nm, we experimentally observed\na 3.6$\\times$ higher pulse amplitude, 3.7$\\times$ faster slew rate, and 25.1 ps\nsmaller timing jitter. The results match our numerical simulation, which\nincorporates both the hotspot dynamics in the SNSPD and the distributed nature\nin the transmission line taper. The taper studied here may become a useful tool\nto interface high-impedance superconducting nanowire devices to conventional\nlow-impedance circuits.'"
Karl Berggren,Berggren_Karl,arXiv:1810.09542,https://arxiv.org/abs/1810.09542,"b'Abstract:  The basis for superconducting electronics can broadly be divided between two\ntechnologies: the Josephson junction and the superconducting nanowire. While\nthe Josephson junction (JJ) remains the dominant technology due to its high\nspeed and low power dissipation, recently proposed nanowire devices offer\nimprovements such as gain, high fanout, and compatibility with CMOS circuits.\nDespite these benefits, nanowire-based electronics have largely been limited to\nbinary operations, with devices switching between the superconducting state and\na high-impedance resistive state dominated by uncontrolled hotspot dynamics.\nUnlike the JJ, they cannot increment an output through successive switching,\nand their operation speeds are limited by their slow thermal reset times. Thus,\nthere is a need for an intermediate device with the interfacing capabilities of\na nanowire but a faster, moderated response allowing for modulation of the\noutput. Here, we present a nanowire device based on controlled fluxon\ntransport. We show that the device is capable of responding proportionally to\nthe strength of its input, unlike other nanowire technologies. The device can\nbe operated to produce a multilevel output with distinguishable states, which\ncan be tuned by circuit parameters. Agreement between experimental results and\nelectrothermal circuit simulations demonstrates that the device is classical\nand may be readily engineered for applications including use as a multilevel\nmemory.'"
Karl Berggren,Berggren_Karl,arXiv:1808.03363,https://arxiv.org/abs/1808.03363,"b'Abstract:  Semi-transparent mirrors are standard elements in light optics for splitting\nlight beams or creating two versions of the same image. Such mirrors do not\nexist in electron optics, although they could be beneficial in existing\ntechniques such as electron interferometry and holography and enable novel\nelectron imaging and spectroscopy techniques. We propose a design for an\nelectron beam splitter using the concept of quantum interaction-free\nmeasurement (IFM). The design combines an electron resonator with a weak phase\ngrating. Fast switching gates allow electrons to enter and exit the resonator.\nWhile in the resonator, the phase grating transfers intensity from the direct\nbeam into one of the weakly diffracted beams at each pass. To make the beam\nsplitter an efficient two-port splitter, the intensity in all other diffracted\nbeams is blocked by an aperture. The IFM principle minimizes the loss of total\nintensity by this aperture. We use a scattering matrix method to analyze the\nperformance of the beam splitter, including the effects of inelastic scattering\nin the phase grating. This design can be generalized to beam splitters for not\nonly electrons, but also photons, neutrons, atoms, and other quantum mechanical\nsystems.'"
Karl Berggren,Berggren_Karl,arXiv:1805.05601,https://arxiv.org/abs/1805.05601,"b'Abstract:  To analyze the switching dynamics and output performance of a superconducting\nnanowire single photon detector (SNSPD), the nanowire is usually modelled as an\ninductor in series with a time-varying resistor induced by absorption of a\nphoton. Our recent experimental results show that, due to the effect of kinetic\ninductance, for a SNSPD made of a nanowire of sufficient length, its geometry\nlength can be comparable to or even longer than the effective wavelength of\nfrequencies contained in the output pulse. In other words, a superconducting\nnanowire can behave as a distributed transmission line so that the readout\npulse depends on the photon detection location and the transmission line\nproperties of the nanowire. Here, we develop a distributed model for a\nsuperconducting nanowire and apply it to simulate the output performance of a\nlong nanowire designed into a coplanar waveguide. We compare this coplanar\nwaveguide geometry to a conventional meander nanowire geometry. The simulation\nresults agree well with our experimental observations. With this distributed\nmodel, we discussed the importance of microwave design of a nanowire and how\nimpedance matching can affect the output pulse shape. We also discuss how the\ndistributed model affects the growth and decay of the photon-triggered\nresistive hotspot.'"
Karl Berggren,Berggren_Karl,arXiv:1805.00130,https://arxiv.org/abs/1805.00130,"b'Abstract:  We analyze the origin of the intrinsic timing jitter in superconducting\nnanowire single photon detectors (SNSPDs) in terms of fluctuations in the\nlatency of the detector response, which is determined by the microscopic\nphysics of the photon detection process. We demonstrate that fluctuations in\nthe physical parameters which determine the latency give rise to the intrinsic\ntiming jitter. We develop a general description of latency by introducing the\nexplicit time dependence of the internal detection efficiency. By considering\nthe dynamic Fano fluctuations together with static spatial inhomogeneities, we\nstudy the details of the connection between latency and timing jitter. We\ndevelop both a simple phenomenological model and a more general microscopic\nmodel of detector latency and timing jitter based on the solution of the\ngeneralized time-dependent Ginzburg-Landau equations for the 1D hotbelt\ngeometry. While the analytical model is sufficient for qualitative\ninterpretation of recent data, the general approach establishes the framework\nfor a quantitative analysis of detector latency and the fundamental limits of\nintrinsic timing jitter. These theoretical advances can be used to interpret\nthe results of recent experiments measuring the dependence of detection latency\nand timing jitter on photon energy to the few-picosecond level.'"
Karl Berggren,Berggren_Karl,arXiv:1803.11306,https://arxiv.org/abs/1803.11306,b'Abstract:  Report of the first workshop to identify approaches and techniques in the\ndomain of quantum sensing that can be utilized by future High Energy Physics\napplications to further the scientific goals of High Energy Physics.'
Karl Berggren,Berggren_Karl,arXiv:1711.10546,https://arxiv.org/abs/1711.10546,"b'Abstract:  Coincidence detection of single photons is crucial in numerous quantum\ntechnologies and usually requires multiple time-resolved single-photon\ndetectors. However, the electronic readout becomes a major challenge when the\nmeasurement basis scales to large numbers of spatial modes. Here, we address\nthis problem by introducing a two-terminal coincidence detector that enables\nscalable readout of an array of detector segments based on superconducting\nnanowire microstrip transmission line. Exploiting timing logic, we demonstrate\na 16-element detector that resolves all 136 possible single-photon and\ntwo-photon coincidence events. We further explore the pulse shapes of the\ndetector output and resolve up to four-photon coincidence events in a 4-element\ndevice, giving the detector photon-number-resolving capability. This new\ndetector architecture and operating scheme will be particularly useful for\nmulti-photon coincidence detection in large-scale photonic integrated circuits.'"
Karl Berggren,Berggren_Karl,arXiv:1711.08290,https://arxiv.org/abs/1711.08290,"b'Abstract:  A superconducting loop stores persistent current without any ohmic loss,\nmaking it an ideal platform for energy efficient memories. Conventional\nsuperconducting memories use an architecture based on Josephson junctions (JJs)\nand have demonstrated access times less than 10 ps and power dissipation as low\nas $10^{-19}$ J. However, their scalability has been slow to develop due to the\nchallenges in reducing the dimensions of JJs and minimizing the area of the\nsuperconducting loops. In addition to the memory itself, complex readout\ncircuits require additional JJs and inductors for coupling signals, increasing\nthe overall area. Here, we have demonstrated a superconducting memory based\nsolely on lithographic nanowires. The small dimensions of the nanowire ensure\nthat the device can be fabricated in a dense area in multiple layers, while the\nhigh kinetic inductance makes the loop essentially independent of geometric\ninductance, allowing it to be scaled down without sacrificing performance. The\nmemory is operated by a group of nanowire cryotrons patterned alongside the\nstorage loop, enabling us to reduce the entire memory cell to 3 {\\mu}m $\\times\n$ 7 {\\mu}m in our proof-of-concept device. In this work we present the\noperation principles of a superconducting nanowire memory (nMem) and\ncharacterize its bit error rate, speed, and power dissipation.'"
Karl Berggren,Berggren_Karl,arXiv:1711.01305,https://arxiv.org/abs/1711.01305,"b'Abstract:  We present the performance of a superconducting nanowire that can be operated\nin two detection modes: i) as a kinetic inductance detector (KID) or ii) as a\nsingle-photon detector (SPD). Two superconducting nanowires developed for use\nas single-photon detectors (SNSPDs) are embedded as the inductive (L) component\nin resonant inductor/capacitor (LC) circuits coupled to a microwave\ntransmission line. The capacitors are low loss commercial chip capacitors and\nlimit the internal quality factor of the resonators to approximately $Q_i =\n170$. The resonator quality factor, $Q_r \\simeq 23$, is dominated by the\ncoupling to the feedline and limits the detection bandwidth to on the order of\n1MHz. When operated in KID mode, the detectors are AC biased with tones at\ntheir resonant frequencies of 45.85 and 91.81MHz. In the low-bias, standard KID\nmode, a single photon produces a hot spot that does not turn an entire section\nof the line normal but only increases the kinetic inductance. In the high-bias,\ncritical KID mode, a photon event turns a section of the line normal and the\nresonance is destroyed until the normal region is dissipated. When operated as\nan SPD in Geiger mode, the resonators are DC biased through cryogenic bias tees\nand each photon produces a sharp voltage step followed by a ringdown signal at\nthe resonant frequency of the detector which is converted to a standard pulse\nwith an envelop detector. We show that AC biasing in the critical KID mode is\ninferior to the sensitivity achieved in DC-biased SPD mode due to the small\nfraction of time spent near the critical current with an AC bias.'"
Karl Berggren,Berggren_Karl,arXiv:1710.05358,https://arxiv.org/abs/1710.05358,"b'Abstract:  Recent advances in the fabrication of nanostructures and nanoscale features\nin metasurfaces offer a new prospect for generating visible, light emission\nfrom low energy electrons. In this paper, we present the experimental\nobservation of visible light emission from low-energy free electrons\ninteracting with nanoscale periodic surfaces through the Smith-Purcell (SP)\neffect. SP radiation is emitted when electrons pass in close proximity over a\nperiodic structure, inducing collective charge motion or dipole excitations\nnear the surface, thereby giving rise to electromagnetic radiation. We\ndemonstrate a controlled emission of SP light from nanoscale gold gratings with\nperiodicity as small as 50 nm, enabling the observation of visible SP radiation\nby low energy electrons (1.5 to 6 keV), an order of magnitude lower than\npreviously reported. We study the emission wavelength and intensity dependence\non the grating pitch and electron energy, showing agreement between experiment\nand theory. Further reduction of structure periodicity should enable the\nproduction of SP-based devices that operate with even slower electrons that\nallow an even smaller footprint and facilitate the investigation of quantum\neffects for light generation in nanoscale devices. A tunable light source\nintegrated in an electron microscope would enable the development of novel\nelectron-optical correlated spectroscopic techniques, with additional\napplications ranging from biological imaging to solid-state lighting.'"
Karl Berggren,Berggren_Karl,arXiv:1709.06598,https://arxiv.org/abs/1709.06598,"b'Abstract:  Many superconducting technologies such as rapid single flux quantum computing\n(RSFQ) and superconducting quantum interference devices (SQUIDs) rely on the\nmodulation of nonlinear dynamics in Josephson junctions for functionality. More\nrecently, however, superconducting devices have been developed based on the\nswitching and thermal heating of nanowires for use in fields such as single\nphoton detection and digital logic. In this paper, we use resistive shunting to\ncontrol the nonlinear heating of a superconducting nanowire and compare the\nresulting dynamics to those observed in Josephson junctions. We show that\ninteraction of the hotspot growth with the external shunt produces high\nfrequency relaxation oscillations with similar behavior as observed in\nJosephson junctions due to their rapid time constants and ability to be\nmodulated by a weak periodic signal. In particular, we use a microwave drive to\npull and mix the oscillation frequency, resulting in phase locked features that\nresemble the AC Josephson effect. New nanowire devices based on these\nconclusions have promising applications in fields such as parametric\namplification and frequency multiplexing.'"
Karl Berggren,Berggren_Karl,arXiv:1703.08034,https://arxiv.org/abs/1703.08034,"b'Abstract:  The lack of energy dissipation and abrupt electrical phase transition of\nsuperconductors favorite them for nanoscale technologies, including radiation\ndetectors, and quantum technologies. Moreover, understanding the nanoscale\nbehavior of superconductivity is significant for revealing the onset of\ncollective-electron behavior in nature. Nevertheless, the limited number of\naccessible superconductors restricts availability of the superconducting\nproperties, encumbering the realization of their potential. Superconducting\nnanowire single photon detectors (SNSPDs) sense single-IR photons faster and\nmore efficient with respect to competing technologies. However, these\nadvantageous properties are material-dependent causing an undesirable\nspeed-efficiency payoff. Usually, SNSPDs based on granular materials are\nfaster, while those based on amorphous materials are more efficient. Here we\noptimized ultrathin films of granular NbN on SiO2 and of amorphous W5Si3. We\nshowed that hybrid superconducting nanowire single photon detectors (SNSPDs)\nmade of 2-nm-thick W5Si3 films over 2-nm-thick NbN films exhibit advantageous\ncoexistence of timing (< 5-ns reset time and 52-ps timing jitter) and\nefficiency (> 96% quantum efficiency) performance. We propose that the\ngoverning mechanism of this hybridization is the presence of a dual\nsuperconducting behavior: native superconductivity of each of the films and\nsuperconductivity that is induced from the neighboring film via the proximity\neffect. In addition to improvement in SNSPDs performance, our results suggest\nthat such hybridization can expand the range of available superconducting\nproperties, impacting nano-superconducting technologies. Lastly, this\nhybridization may be used to tune the amorphous character of superconducting\nfilms and to illuminate the elusive onset of collective-electron behavior near\nthe superconducting-to-insulating transition.'"
Karl Berggren,Berggren_Karl,arXiv:1610.09349,https://arxiv.org/abs/1610.09349,"b'Abstract:  Integration with conventional electronics offers a straightforward and\neconomical approach to upgrading existing superconducting technologies, such as\nscaling up superconducting detectors into large arrays and combining single\nflux quantum (SFQ) digital circuits with semiconductor logic and memories.\nHowever, direct output signals from superconducting devices (e.g., Josephson\njunctions) are usually not compatible with the input requirements of\nconventional devices (e.g., transistors). Here, we demonstrate the use of a\nsingle three-terminal superconducting-nanowire device, called the nanocryotron\n(nTron), as a digital comparator to combine SFQ circuits with mature\nsemiconductor circuits such as complementary metal oxide semiconductor (CMOS)\ncircuits. Since SFQ circuits can digitize output signals from general\nsuperconducting devices and CMOS circuits can interface existing\nCMOS-compatible electronics, our results demonstrate the feasibility of a\ngeneral architecture that uses an nTron as an interface to realize a\nsuper-hybrid system consisting of superconducting detectors, superconducting\nquantum electronics, CMOS logic and memories, and other conventional\nelectronics.'"
Karl Berggren,Berggren_Karl,arXiv:1608.08616,https://arxiv.org/abs/1608.08616,"b'Abstract:  We report a self-aligned, monolithic electron interferometer, consisting of\ntwo 45 nm thick silicon layers separated by 20 $\\mu$m. This interferometer was\nfabricated from a single crystal silicon cantilever on a transmission electron\nmicroscope grid by gallium focused ion-beam milling. Using this interferometer,\nwe demonstrate beam path-separation, and obtain interference fringes in a\nMach-Zehnder geometry, in an unmodified 200 kV transmission electron\nmicroscope. The fringes have a period of 0.32 nm, which corresponds to the\n$\\left[\\bar{1}\\bar{1}1\\right]$ lattice planes of silicon, and a maximum\ncontrast of 15 %. This design can potentially be scaled to millimeter-scale,\nand used in electron holography. It can also be applied to perform fundamental\nphysics experiments, such as interaction-free measurement with electrons.'"
Karl Berggren,Berggren_Karl,arXiv:1607.06713,https://arxiv.org/abs/1607.06713,"b'Abstract:  Detection jitter quantifies variance introduced by the detector in the\ndetermination of photon arrival time. It is a crucial performance parameter for\nsystems using superconducting nanowire single photon detectors (SNSPDs). In\nthis work, we have demonstrated that the detection timing jitter is limited in\npart by the spatial variation of photon detection events along the length of\nthe wire. This distribution causes the generated electrical pulses to arrive at\nthe readout at varied times. We define this jitter source as geometric jitter\nsince it is related to the length and area of the SNSPD. To characterize the\ngeometric jitter, we have constructed a novel differential cryogenic readout\nwith less than 7 ps of electronic jitter that can amplify the pulses generated\nfrom the two ends of an SNSPD. By differencing the measured arrival times of\nthe two electrical pulses, we were able to partially cancel out the difference\nof the propagation times and thus reduce the uncertainty of the photon arrival\ntime. Our experimental data indicates that the variation of the differential\npropagation time was a few ps for a 3 {\\mu}m x 3 {\\mu}m device while it\nincreased up to 50 ps for a 20 {\\mu}m x 20 {\\mu}m device. In a 20 {\\mu}m x 20\n{\\mu}m large SNSPD, we achieved a 20% reduction in the overall detection timing\njitter for detecting telecom-wavelength photons by using the differential\ncryogenic readout. The geometric jitter hypothesis was further confirmed by\nstudying jitter in devices that consisted of long wires with 1-{\\mu}m-long\nnarrowed regions used for sensing photons.'"
Karl Berggren,Berggren_Karl,arXiv:1606.01395,https://arxiv.org/abs/1606.01395,"b'Abstract:  We describe a superconducting three-terminal device that uses a simple\ngeometric effect known as current crowding to sense the flow of current and\nactuate a readout signal. The device consists of a ""Y""-shaped current combiner,\nwith two currents (sense and bias) entering through the top arms of the ""Y"",\nintersecting, and then exiting through the bottom leg of the ""Y""\'. This\ngeometry--mixing two inputs at a sharp intersection point--takes its\ninspiration from Y-shaped combiners in fluid flow systems, where variations in\nthe input pressures can produce at turbulence and mixing at the intersection.\nWhen current is added to or removed from one of the arms (the sense arm), the\nsuperconducting critical current in the other arm (the bias arm) is modulated.\nThe current in the sense arm can thus be determined by measuring the critical\ncurrent of the bias arm. The dependence of the bias critical current on the\nsense current is possible because current crowding causes the sense current to\ninteract locally with the bias arm. Measurement of the critical current in the\nbias arm does not break the superconducting state of the sense arm or of the\nbottom leg, and thus the signal to be sensed is fully restored after the\nmeasurement process. This device thus has potential for broad applicability\nacross superconducting technologies and materials.'"
Karl Berggren,Berggren_Karl,arXiv:1605.08693,https://arxiv.org/abs/1605.08693,"b""Abstract:  Detecting spatial and temporal information of individual photons by using\nsingle-photon-detector (SPD) arrays is critical to applications in\nspectroscopy, communication, biological imaging, astronomical observation, and\nquantum-information processing. Among the current SPDs1,detectors based on\nsuperconducting nanowires have outstanding performance2, but are limited in\ntheir ability to be integrated into large scale arrays due to the engineering\ndifficulty of high-bandwidth cryogenic electronic readout3-8. Here, we address\nthis problem by demonstrating a scalable single-photon imager using a single\ncontinuous photon-sensitive superconducting nanowire microwave-plasmon\ntransmission line. By appropriately designing the nanowire's local\nelectromagnetic environment so that the nanowire guides microwave plasmons, the\npropagating voltages signals generated by a photon-detection event were slowed\ndown to ~ 2% of the speed of light. As a result, the time difference between\narrivals of the signals at the two ends of the nanowire naturally encoded the\nposition and time of absorption of the photon. Thus, with only two readout\nlines, we demonstrated that a 19.7-mm-long nanowire meandered across an area of\n286 {\\mu}m * 193 {\\mu}m was capable of resolving ~590 effective pixels while\nsimultaneously recording the arrival times of photons with a temporal\nresolution of 50 ps. The nanowire imager presents a scalable approach to\nrealizing high-resolution photon imaging in time and space."""
Karl Berggren,Berggren_Karl,arXiv:1602.06895,https://arxiv.org/abs/1602.06895,"b""Abstract:  We study the microwave impedance of extremely high aspect ratio (length/width\n~ 5,000) superconducting niobium nitride nanowires. The nanowires are\nfabricated in a compact meander geometry that is in series with the center\nconductor of a 50 ohm coplanar waveguide transmission line. The transmission\ncoefficient of the sample is measured up to 20 GHz. At high frequency, a peak\nin the transmission coefficient is seen. Numerical simulations show that this\nis a half-wave resonance along the length of the nanowire, where the nanowire\nacts as a high impedance, slow wave transmission line. This resonance sets the\nupper frequency limit for these nanowires as inductive elements. Fitting\nsimulations to the measured resonance enables a precise determination of the\nnanowire's complex sheet impedance at the resonance frequency. The real part is\na measure of dissipation, while the imaginary part is dominated by kinetic\ninductance. We characterize the dependence of the sheet resistance and sheet\ninductance on both temperature and current and compare the results to recent\ntheoretical predictions for disordered superconductors. These results can aid\nin the understanding of high frequency devices based on superconducting\nnanowires. They may also lead to the development of novel superconducting\ndevices such as ultra-compact resonators and slow-wave structures."""
Karl Berggren,Berggren_Karl,arXiv:1511.05786,https://arxiv.org/abs/1511.05786,"b'Abstract:  This paper describes the construction of a cryostat and an optical system\nwith a free-space coupling efficiency of 56.5% +/- 3.4% to a superconducting\nnanowire single-photon detector (SNSPD) for infrared quantum communication and\nspectrum analysis. A 1K pot decreases the base temperature to T = 1.7 K from\nthe 2.9 K reached by the cold head cooled by a pulse-tube cryocooler. The\nminimum spot size coupled to the detector chip was 6.6 +/- 0.11 {\\mu}m starting\nfrom a fiber source at wavelength, {\\lambda} = 1.55 {\\mu}m. We demonstrated\nefficient photon counting on a detector with an 8 x 7.3 {\\mu}m^2 area. We\nmeasured a dark count rate of 95 +/- 3.35 kcps and a system detection\nefficiency of 1.64% +/- 0.13%. We explain the key steps that are required to\nfurther improve the coupling efficiency.'"
Karl Berggren,Berggren_Karl,arXiv:1510.05946,https://arxiv.org/abs/1510.05946,"b'Abstract:  One of the astounding consequences of quantum mechanics is that it allows the\ndetection of a target using an incident probe, with only a low probability of\ninteraction of the probe and the target. This \'quantum weirdness\' could be\napplied in the field of electron microscopy to generate images of\nbeam-sensitive specimens with substantially reduced damage to the specimen. A\nreduction of beam-induced damage to specimens is especially of great importance\nif it can enable imaging of biological specimens with atomic resolution.\nFollowing a recent suggestion that interaction-free measurements are possible\nwith electrons, we now analyze the difficulties of actually building an atomic\nresolution interaction-free electron microscope, or ""quantum electron\nmicroscope"". A quantum electron microscope would require a number of unique\ncomponents not found in conventional transmission electron microscopes. These\ncomponents include a coherent electron-beam splitter or two-state-coupler, and\na resonator structure to allow each electron to interrogate the specimen\nmultiple times, thus supporting high success probabilities for interaction-free\ndetection of the specimen. Different system designs are presented here, which\nare based on four different choices of two-state-couplers: a thin crystal, a\ngrating mirror, a standing light wave and an electro-dynamical pseudopotential.\nChallenges for the detailed electron optical design are identified as future\ndirections for development. While it is concluded that it should be possible to\nbuild an atomic resolution quantum electron microscope, we have also identified\na number of hurdles to the development of such a microscope and further\ntheoretical investigations that will be required to enable a complete\ninterpretation of the images produced by such a microscope.'"
Karl Berggren,Berggren_Karl,arXiv:1508.01877,https://arxiv.org/abs/1508.01877,"b'Abstract:  Methods for patterning biomolecules on a substrate at the single molecule\nlevel have been studied as a route to sensors with single-molecular sensitivity\nor as a way to probe biological phenomena at the single-molecule level.\nHowever, the arrangement and orientation of single biomolecules on substrates\nhas been less investigated. Here, we examined the arrangement and orientation\nof two rod-like coiled-coil proteins, cortexillin and tropomyosin, around\npatterned gold nanostructures. The high aspect ratio of the coiled coils made\nit possible to study their orientations and to pursue a strategy of protein\norientation via two-point attachment. The proteins were anchored to the\nsurfaces using thiol groups, and the number of cysteine residues in tropomyosin\nwas varied to test how this variation affected the structure and arrangement of\nthe surface-attached proteins. Molecular dynamics studies were used to\ninterpret the observed positional distributions. Based on initial studies of\nprotein attachment to gold post structures, two 31-nm-long tropomyosin\nmolecules were aligned between the two sidewalls of a trench with a width of 68\nnm. Because the approach presented in this study uses one of twenty natural\namino acids, this method provides a convenient way to pattern biomolecules on\nsubstrates using standard chemistry.'"
Karl Berggren,Berggren_Karl,arXiv:1503.07135,https://arxiv.org/abs/1503.07135,"b'Abstract:  We present an optical setup that can be used to characterize the thicknesses\nof thin NbN films to screen samples for fabrication and to better model the\nperformance of the resulting superconducting nanowire single photon detectors.\nThe infrared transmissometer reported here is easy to use, gives results within\nminutes and is non-destructive. Thus, the thickness measurement can be easily\nintegrated into the workflow of deposition and characterization. Comparison to\na similar visible-wavelength transmissometer is provided.'"
Karl Berggren,Berggren_Karl,arXiv:1408.1124,https://arxiv.org/abs/1408.1124,"b'Abstract:  Superconducting nanowire avalanche single-photon detectors (SNAPs) with n\nparallel nanowires are advantageous over single-nanowire detectors because\ntheir output signal amplitude scales linearly with n. However, the SNAP\narchitecture has not been viably demonstrated for n > 4. To increase n for\nlarger signal amplification, we designed a multi-stage, successive-avalanche\narchitecture which used nanowires, connected via choke inductors in a\nbinary-tree layout. We demonstrated an avalanche detector with n = 8 parallel\nnanowires and achieved eight-fold signal amplification, with a timing jitter of\n54 ps.'"
Karl Berggren,Berggren_Karl,arXiv:1407.5945,https://arxiv.org/abs/1407.5945,"b'Abstract:  Thin superconducting films form a unique platform for geometrically-confined,\nstrongly-interacting electrons. They allow an inherent competition between\ndisorder and superconductivity, which in turn enables the intriguing\nsuperconducting-to-insulator transition and believed to facilitate the\ncomprehension of high-Tc superconductivity. Furthermore, understanding thin\nfilm superconductivity is technologically essential e.g. for photo-detectors,\nand quantum-computers. Consequently, the absence of an established universal\nrelationships between critical temperature ($T_c$), film thickness ($d$) and\nsheet resistance ($R_s$) hinders both our understanding of the onset of the\nsuperconductivity and the development of miniaturised superconducting devices.\nWe report that in thin films, superconductivity scales as $d^.$$T_c(R_s)$. We\ndemonstrated this scaling by analysing the data published over the past 46\nyears for different materials (and facilitated this database for further\nanalysis). Moreover, we experimentally confirmed the discovered scaling for NbN\nfilms, quantified it with a power law, explored its possible origin and\ndemonstrated its usefulness for superconducting film-based devices.'"
Karl Berggren,Berggren_Karl,arXiv:1405.4244,https://arxiv.org/abs/1405.4244,"b'Abstract:  Photonic integrated circuits (PICs) have emerged as a scalable platform for\ncomplex quantum technologies using photonic and atomic systems. A central goal\nhas been to integrate photon-resolving detectors to reduce optical losses,\nlatency, and wiring complexity associated with off-chip detectors.\nSuperconducting nanowire single-photon detectors (SNSPDs) are particularly\nattractive because of high detection efficiency, sub-50-ps timing jitter,\nnanosecond-scale reset time, and sensitivity from the visible to the\nmid-infrared spectrum. However, while single SNSPDs have been incorporated into\nindividual waveguides, the system efficiency of multiple SNSPDs in one photonic\ncircuit has been limited below 0.2% due to low device yield. Here we introduce\na micrometer-scale flip-chip process that enables scalable integration of\nSNSPDs on a range of PICs. Ten low-jitter detectors were integrated on one PIC\nwith 100% device yield. With an average system efficiency beyond 10% for\nmultiple SNSPDs on one PIC, we demonstrate high-fidelity on-chip photon\ncorrelation measurements of non-classical light.'"
Karl Berggren,Berggren_Karl,arXiv:1403.6423,https://arxiv.org/abs/1403.6423,"b'Abstract:  In existing superconducting electronic systems, Josephson junctions play a\ncentral role in processing and transmitting small-amplitude electrical signals.\nHowever, Josephson-junction-based devices have a number of limitations\nincluding: (1) sensitivity to magnetic fields, (2) limited gain, (3) inability\nto drive large impedances, and (4) difficulty in controlling the junction\ncritical current (which depends sensitively on sub-Angstrom-scale thickness\nvariation of the tunneling barrier). Here we present a nanowire-based\nsuperconducting electronic device, which we call the nanocryotron (nTron), that\ndoes not rely on Josephson junctions and can be patterned from a single thin\nfilm of superconducting material with conventional electron-beam lithography.\nThe nTron is a 3-terminal, T-shaped planar device with a gain of ~20 that is\ncapable of driving impedances of more than 100 k{\\Omega}, and operates in\ntypical ambient magnetic fields at temperatures of 4.2K. The device uses a\nlocalized, Joule-heated hotspot formed in the gate to modulate current flow in\na perpendicular superconducting channel. We have characterized the nTron,\nmatched it to a theoretical framework, and applied it both as a digital logic\nelement in a half-adder circuit, and as a digital amplifier for superconducting\nnanowire single-photon detectors pulses. The nTron has immediate applications\nin classical and quantum communications, photon sensing and astronomy, and its\nperformance characteristics make it compatible with existing superconducting\ntechnologies. Furthermore, because the hotspot effect occurs in all known\nsuperconductors, we expect the design to be extensible to other materials,\nproviding a path to digital logic, switching, and amplification in\nhigh-temperature superconductors.'"
Karl Berggren,Berggren_Karl,arXiv:1202.2835,https://arxiv.org/abs/1202.2835,"b'Abstract:  The optimal orientations are determined for polarized substrate side\nillumination of three superconducting nanowire single-photon detector (SNSPD)\ndesigns: (1) periodic niobium-nitride (NbN) stripes standing in air with\ndimensions according to conventional SNSPDs, (2) same NbN patterns below\n~quarter-wavelength hydrogensilsesquioxane-filled nano-cavity, (3) analogous\nNbN patterns in HSQ nano-cavity closed by a thin gold reflector. Numerical\ncomputation results have shown that the optical response and near-field\ndistribution vary significantly with polar-angle, fi, and these variations are\nanalogous across all azimuthal-angles, gamma, but are fundamentally different\nin various device designs. Larger absorptance is available due to p-polarized\nillumination of NbN patterns in P-structure configuration, while s-polarized\nillumination results in higher absorptance in S-structure arrangement. As a\nresult of p-polarized illumination a global maximum appears on absorptance of\nbare NbN pattern at polar angle corresponding to NbN-related ATIR; integration\nwith HSQ nano-cavity results in a global absorptance maximum at polar angle\ncorresponding to TIR at sapphire-air interface; while the highest absorptance\nis observable at perpendicular incidence on P-structures aligned below gold\nreflector covered HSQ nano-cavity. S-polarized light illumination results in a\nglobal absorptance maximum at TIR on bare NbN patterns; the highest absorptance\nis available below HSQ nano-cavity at polar angle corresponding to ATIR\nphenomenon; while the benefit of gold reflector is large and polar angle\nindependent absorptance.'"
Karl Berggren,Berggren_Karl,arXiv:1109.4881,https://arxiv.org/abs/1109.4881,"b'Abstract:  In this paper we calculate the critical currents in thin superconducting\nstrips with sharp right-angle turns, 180-degree turnarounds, and more\ncomplicated geometries, where all the line widths are much smaller than the\nPearl length $\\Lambda = 2 \\lambda^2/d$. We define the critical current as the\ncurrent that reduces the Gibbs free-energy barrier to zero. We show that\ncurrent crowding, which occurs whenever the current rounds a sharp turn, tends\nto reduce the critical current, but we also show that when the radius of\ncurvature is less than the coherence length this effect is partially\ncompensated by a radius-of-curvature effect. We propose several patterns with\nrounded corners to avoid critical-current reduction due to current crowding.\nThese results are relevant to superconducting nanowire single-photon detectors,\nwhere they suggest a means of improving the bias conditions and reducing dark\ncounts. These results also have relevance to normal-metal nanocircuits, as\nthese patterns can reduce the electrical resistance, electromigration, and hot\nspots caused by nonuniform heating.'"
Karl Berggren,Berggren_Karl,arXiv:1106.3591,https://arxiv.org/abs/1106.3591,"b'Abstract:  A novel finite-element method for calculating the illumination-dependence of\nabsorption in three-dimensional nanostructures is presented based on the RF\nmodule of the COMSOL software package. This method is capable of numerically\ndetermining the optical response and near-field distribution of sub-wavelength\nperiodic structures as a function of illumination orientations specified by\npolar angle, fi, and azimuthal angle, gamma. The method was applied to\ndetermine the illumination-angle-dependent absorptance in cavity-based\nsuperconducting-nanowire single-photon detector (SNSPD) designs.\nNiobium-nitride stripes based on dimensions of conventional SNSPDs and\nintegrated with ~ quarter-wavelength hydrogensilsesquioxane-filled nano-optical\ncavities and covered by a thin gold film acting as a reflector were illuminated\nfrom below by p-polarized light in this study. The numerical results were\ncompared to results from complementary transfer-matrix-method calculations on\ncomposite layers made of analogous film-stacks. This comparison helped to\nuncover the optical phenomena contributing to the appearance of extrema in the\noptical response. This paper presents an approach to optimizing the absorptance\nof different sensing and detecting devices via simultaneous numerical\noptimization of the polar and azimuthal illumination angles.'"
Karl Berggren,Berggren_Karl,arXiv:1012.3964,https://arxiv.org/abs/1012.3964,"b'Abstract:  We developed an electro thermal model of NbN superconducting nanowire\navalanche photodetectors (SNAPs) on sapphire substrates. SNAPs are single\nphoton detectors consisting of the parallel connection of N superconducting\nnanowires. We extrapolated the physical constants of the model from\nexperimental data and we simulated the time evolution of the device resistance,\ntemperature and current by solving two coupled electrical and thermal\ndifferential equations describing the nanowires. The predictions of the model\nwere in good quantitative agreement with the experimental results.'"
Karl Berggren,Berggren_Karl,arXiv:1010.6108,https://arxiv.org/abs/1010.6108,"b'Abstract:  We fabricate superconducting ion traps with niobium and niobium nitride and\ntrap single 88Sr ions at cryogenic temperatures. The superconducting transition\nis verified and characterized by measuring the resistance and critical current\nusing a 4-wire measurement on the trap structure, and observing change in the\nrf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz\nat 6 K and shows no significant change across the superconducting transition,\nsuggesting that anomalous heating is primarily caused by noise sources on the\nsurface. This demonstration of superconducting ion traps opens up possibilities\nfor integrating trapped ions and molecular ions with superconducting devices.'"
Karl Berggren,Berggren_Karl,arXiv:0812.4670,https://arxiv.org/abs/0812.4670,"b'Abstract:  Transitions in an artificial atom, driven non-adiabatically through an\nenergy-level avoided crossing, can be controlled by carefully engineering the\ndriving protocol. We have driven a superconducting persistent-current qubit\nwith a large-amplitude, radio-frequency field. By applying a bi-harmonic\nwaveform generated by a digital source, we demonstrate a mapping between the\namplitude and phase of the harmonics produced at the source and those received\nby the device. This allows us to image the actual waveform at the device. This\ninformation is used to engineer a desired time dependence, as confirmed by\ndetailed comparison with simulation.'"
Karl Berggren,Berggren_Karl,arXiv:0812.0290,https://arxiv.org/abs/0812.0290,"b'Abstract:  We investigate the role of electrothermal feedback in the operation of\nsuperconducting nanowire single-photon detectors (SNSPDs). It is found that the\ndesired mode of operation for SNSPDs is only achieved if this feedback is\nunstable, which happens naturally through the slow electrical response\nassociated with their relatively large kinetic inductance. If this response is\nsped up in an effort to increase the device count rate, the electrothermal\nfeedback becomes stable and results in an effect known as latching, where the\ndevice is locked in a resistive state and can no longer detect photons. We\npresent a set of experiments which elucidate this effect, and a simple model\nwhich quantitatively explains the results.'"
Karl Berggren,Berggren_Karl,arXiv:0806.3194,https://arxiv.org/abs/0806.3194,"b'Abstract:  We measured the optical absorptance of superconducting nanowire single photon\ndetectors. We found that 200-nm-pitch, 50%-fill-factor devices had an average\nabsorptance of 21% for normally-incident front-illumination of\n1.55-um-wavelength light polarized parallel to the nanowires, and only 10% for\nperpendicularly-polarized light. We also measured devices with lower\nfill-factors and narrower wires that were five times more sensitive to\nparallel-polarized photons than perpendicular-polarized photons. We developed a\nnumerical model that predicts the absorptance of our structures. We also used\nour measurements, coupled with measurements of device detection efficiencies,\nto determine the probability of photon detection after an absorption event. We\nfound that, remarkably, absorbed parallel-polarized photons were more likely to\nresult in detection events than perpendicular-polarized photons, and we present\na hypothesis that qualitatively explains this result. Finally, we also\ndetermined the enhancement of device detection efficiency and absorptance due\nto the inclusion of an integrated optical cavity over a range of wavelengths\n(700-1700 nm) on a number of devices, and found good agreement with our\nnumerical model.'"
Karl Berggren,Berggren_Karl,arXiv:0805.2397,https://arxiv.org/abs/0805.2397,"b'Abstract:  A photon-number-resolving detector based on a four-element superconducting\nnanowire single photon detector is demonstrated to have sub-30-ps resolution in\nmeasuring the arrival time of individual photons. This detector can be used to\ncharacterize the photon statistics of non-pulsed light sources and to mitigate\ndead-time effects in high-speed photon counting applications. Furthermore, a\n25% system detection efficiency at 1550 nm was demonstrated, making the\ndetector useful for both low-flux source characterization and high-speed\nphoton-counting and quantum communication applications. The design, fabrication\nand testing of this detector are described, and a comparison between the\nmeasured and theoretical performance is presented.'"
Karl Berggren,Berggren_Karl,arXiv:0805.1552,https://arxiv.org/abs/0805.1552,"b""Abstract:  The energy-level structure of a quantum system plays a fundamental role in\ndetermining its behavior and manifests itself in a discrete absorption and\nemission spectrum. Conventionally, spectra are probed via frequency\nspectroscopy whereby the frequency \\nu of a harmonic driving field is varied to\nfulfill the conditions \\Delta E = h \\nu, where the driving field is resonant\nwith the level separation \\Delta E (h is Planck's constant). Although this\ntechnique has been successfully employed in a variety of physical systems,\nincluding natural and artificial atoms and molecules, its application is not\nuniversally straightforward, and becomes extremely challenging for frequencies\nin the range of 10's and 100's of gigahertz. Here we demonstrate an alternative\napproach, whereby a harmonic driving field sweeps the atom through its\nenergy-level avoided crossings at a fixed frequency, surmounting many of the\nlimitations of the conventional approach. Spectroscopic information is obtained\nfrom the amplitude dependence of the system response. The resulting\n``spectroscopy diamonds'' contain interference patterns and population\ninversion that serve as a fingerprint of the atom's spectrum. By analyzing\nthese features, we determine the energy spectrum of a manifold of states with\nenergies from 0.01 to 120 GHz \\times h in a superconducting artificial atom,\nusing a driving frequency near 0.1 GHz. This approach provides a means to\nmanipulate and characterize systems over a broad bandwidth, using only a single\ndriving frequency that may be orders of magnitude smaller than the energy\nscales being probed."""
Karl Berggren,Berggren_Karl,arXiv:0711.5021,https://arxiv.org/abs/0711.5021,"b'Abstract:  Novel optical phenomena, including electromagnetically induced transparency,\nslow light, superluminal light propagation, have recently been demonstrated in\ndiverse physical implementations. These phenomena are challenging to realize in\npractical systems because they require quantum coherence as well as careful\npreparation and control of prescribed quantum states. Here we present a unified\napproach to engineering optical materials that exhibit these phenomena by using\nmixtures of active and passive optical materials at frequencies near their\nresonances. Our approach does not depend on quantum coherence and can realize\nlarge and small (much less than 1) indices of refraction and negative\npermittivity ($\\epsilon<0$), normal and anomalous dispersion, all while\nmaintaining transparency.'"
Karl Berggren,Berggren_Karl,arXiv:physics/0611260,https://arxiv.org/abs/physics/0611260,"b'Abstract:  We investigate the source of large variations in the observed detection\neffiiencies of superconducting nanowire single-photon detectors between many\nnominally identical devices. Through both electrical and optical measurements,\nwe infer that these variations arise from ""constrictions:"" highly localized\nregions of the nanowires where the effective cross-sectional area for\nsuperconducting current is reduced. These constrictions limit the DC bias\ncurrent density to well below its critical value over the remainder of the\nwire, and thus prevent the detection efficiency from reaching the high values\nthat occur in these devices only when they are biased near the critical current\ndensity.'"
Karl Berggren,Berggren_Karl,arXiv:cond-mat/0609561,https://arxiv.org/abs/cond-mat/0609561,"b'Abstract:  A nonlinear resonant circuit comprising a SQUID magnetometer and a parallel\ncapacitor is studied as a readout scheme for a persistent-current (PC) qubit.\nThe flux state of the qubit is detected as a change in the Josephson inductance\nof the SQUID magnetometer, which in turn mediates a shift in the resonance\nfrequency of the readout circuit. The nonlinearity and resulting hysteresis in\nthe resonant behavior are characterized as a function of the power of both the\ninput drive and the associated resonance peak response. Numerical simulations\nbased on a phenomenological circuit model are presented which display the\nfeatures of the observed nonlinearity.'"
Karl Berggren,Berggren_Karl,arXiv:cond-mat/0512691,https://arxiv.org/abs/cond-mat/0512691,"b'Abstract:  We demonstrate Mach-Zehnder-type interferometry in a superconducting flux\nqubit. The qubit is a tunable artificial atom, whose ground and excited states\nexhibit an avoided crossing. Strongly driving the qubit with harmonic\nexcitation sweeps it through the avoided crossing two times per period. As the\ninduced Landau-Zener transitions act as coherent beamsplitters, the accumulated\nphase between transitions, which varies with microwave amplitude, results in\nquantum interference fringes for n=1...20 photon transitions. The\ngeneralization of optical Mach-Zehnder interferometry, performed in qubit phase\nspace, provides an alternative means to manipulate and characterize the qubit\nin the strongly-driven regime.'"
Karl Berggren,Berggren_Karl,arXiv:physics/0510238,https://arxiv.org/abs/physics/0510238,"b'Abstract:  We investigate the recovery of superconducting NbN-nanowire photon counters\nafter detection of an optical pulse at a wavelength of 1550 nm, and present a\nmodel that quantitatively accounts for our observations. The reset time is\nfound to be limited by the large kinetic inductance of these nanowires, which\nforces a tradeoff between counting rate and either detection efficiency or\nactive area. Devices of usable size and high detection efficiency are found to\nhave reset times orders of magnitude longer than their intrinsic photoresponse\ntime.'"
Karl Berggren,Berggren_Karl,arXiv:physics/0509228,https://arxiv.org/abs/physics/0509228,"b'Abstract:  Quantum optical techniques may yield immersion fluids with high indices of\nrefraction without absorption. We describe one such technique in which a probe\nfield experiences a large index of refraction with amplification rather than\nabsorption, and examine its practicality for an immersion lithography\napplication. Enhanced index can be observed in a three-level system with a\ntunable, near-resonant, coherent probe and incoherent pump field that inverts\npopulation of the probe transition. This observation contradicts the common\nbelief that large indices of refraction are impossible without absorption,\nhowever it is well in accord with existing electromagnetic theory and practice.\nCalculations show that a refractive index >> 2 is possible with practical\nexperimental parameters. A scheme with an incoherent mixture of pumped and\nunpumped atoms is also examined, and is seen to have a lower refractive index\n(~2) accompanied by neither gain nor loss.'"
Karl Berggren,Berggren_Karl,arXiv:cond-mat/0501283,https://arxiv.org/abs/cond-mat/0501283,"b'Abstract:  We have implemented a resonant circuit that uses a SQUID as a flux-sensitive\nJosephson inductor for qubit readout. In contrast to the conventional switching\ncurrent measurement that generates undesired quasi-particles when the SQUID\nswitches to the voltage state, our approach keeps the readout SQUID biased\nalong the supercurrent branch during the measurement. By incorporating the\nSQUID inductor in a high-Q resonant circuit, we can distinguish the two flux\nstates of a niobium persistent-current (PC) qubit by observing a shift in the\nresonant frequency of both the magnitude and the phase spectra. The readout\ncircuit was also characterized in the nonlinear regime to investigate its\npotential use as a nonlinear amplifier.'"
Karl Berggren,Berggren_Karl,arXiv:cond-mat/0311289,https://arxiv.org/abs/cond-mat/0311289,"b""Abstract:  We measured the intrawell energy relaxation time \\tau_{d} between macroscopic\nquantum levels in the double well potential of a Nb persistent-current qubit.\nInterwell population transitions were generated by irradiating the qubit with\nmicrowaves. Zero population in the initial well was then observed due to a\nmulti-level decay process in which the initial population relaxed to the lower\nenergy levels during transitions. The qubit's decoherence time, determined from\n\\tau_{d}, is longer than 20 microseconds, holding the promise of building a\nquantum computer with Nb-based superconducting qubits."""
Karl Berggren,Berggren_Karl,arXiv:quant-ph/0310157,https://arxiv.org/abs/quant-ph/0310157,"b""Abstract:  A numerical method for solving Schrodinger's equation based upon a\nBaker-Campbell-Hausdorff (BCH) expansion of the time evolution operator is\npresented herein. The technique manifestly preserves wavefunction norm, and it\ncan be applied to problems in any number of spatial dimensions. We also\nidentify a particular dimensionless ratio of potential to kinetic energies as a\nkey coupling constant. This coupling establishes characteristic length and time\nscales for a large class of low energy quantum states, and it guides the choice\nof step sizes in numerical work. Using the BCH method in conjunction with an\nimaginary time rotation, we compute low energy eigenstates for several quantum\nsystems coupled to non-trivial background potentials. The approach is\nsubsequently applied to the study of 1D propagating wave packets and 2D bound\nstate time development. Failures of classical expectations uncovered by\nsimulations of these simple systems help develop quantum intuition.\nFinally, we investigate the response of a Superconducting Quantum\nInterference Device (SQUID) to a time dependent potential. We discuss how to\nengineer the potential's energy and time scales so that the SQUID acts as a\nquantum NOT gate. The notional simulation we present for this gate provides\nuseful insight into the design of one candidate building block for a quantum\ncomputer."""
Karl Berggren,Berggren_Karl,arXiv:nlin/0111010,https://arxiv.org/abs/nlin/0111010,"b'Abstract:  The probability current statistics of two-dimensional open chaotic ballistic\nbilliards is studied both analytically and numerically. Assuming that the real\nand imaginary parts of the scattering wave function are both random Gaussian\nfields, we find a universal distribution function for the probability current.\nIn by-passing we recover previous analytic forms for wave function statistics.\nThe expressions bridge the entire region from GOE to GUE type statistics. Our\nanalytic expressions are verified numerically by explicit quantum-mechanical\ncalculations of transport through a Bunimovich billiard.'"
Karl Berggren,Berggren_Karl,arXiv:nlin/0012019,https://arxiv.org/abs/nlin/0012019,b'Abstract:  According to Berry a wave-chaotic state may be viewed as a superposition of\nmonochromatic plane waves with random phases and amplitudes. Here we consider\nthe distribution of nodal points associated with this state. Using the property\nthat both the real and imaginary parts of the wave function are random Gaussian\nfields we analyze the correlation function and densities of the nodal points.\nUsing two approaches (the Poisson and Bernoulli) we derive the distribution of\nnearest neighbor separations. Furthermore the distribution functions for nodal\npoints with specific chirality are found. Comparison is made with results from\nfrom numerical calculations for the Berry wave function.'
Karl Berggren,Berggren_Karl,arXiv:chao-dyn/9910011,https://arxiv.org/abs/chao-dyn/9910011,"b'Abstract:  Streamlines and distributions of nodal points are used as signatures of chaos\nin coherent electron transport through three types of billiards, Sinai,\nBunimovich and rectangular. Numerical averaged distribution functions of\nnearest distances between nodal points are presented. We find the same form for\nthe Sinai and Bunimovich billiards and suggest that there is a universal form\nthat can be used as a signature of quantum chaos for electron transport in open\nbilliards. The universal distribution function is found to be insensitive to\nthe way avaraging is performed (over positions of leads, over an energy\ninterval with a few conductance fluctuations, or both). The integrable\nrectangular billiard, on the other hand, displays nonuniversal distribution\nwith a central peak related to partial order of nodal points for the case of\nsymmetric attachment of leads. However cases with nonsymmetric leads tend to\nthe universal form.\nAlso it is shown how nodal points in rectangular billiard can lead to\n""channeling of quantum flows"" while disorder in nodal points in the Sinai\nbilliard gives rise to unstable irregular behavior of the flow.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1804.04577,https://arxiv.org/abs/1804.04577,"b'Abstract:  In this paper we discuss policy iteration methods for approximate solution of\na finite-state discounted Markov decision problem, with a focus on\nfeature-based aggregation methods and their connection with deep reinforcement\nlearning schemes. We introduce features of the states of the original problem,\nand we formulate a smaller ""aggregate"" Markov decision problem, whose states\nrelate to the features. We discuss properties and possible implementations of\nthis type of aggregation, including a new approach to approximate policy\niteration. In this approach the policy improvement operation combines\nfeature-based aggregation with feature construction using deep neural networks\nor other calculations. We argue that the cost function of a policy may be\napproximated much more accurately by the nonlinear function of the features\nprovided by aggregation, than by the linear function of the features provided\nby neural network-based reinforcement learning, thereby potentially leading to\nmore effective policy improvement.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1712.06659,https://arxiv.org/abs/1712.06659,"b""Abstract:  We consider discrete-time infinite horizon deterministic optimal control\nproblems with nonnegative cost per stage, and a destination that is cost-free\nand absorbing. The classical linear-quadratic regulator problem is a special\ncase. Our assumptions are very general, and allow the possibility that the\noptimal policy may not be stabilizing the system, e.g., may not reach the\ndestination either asymptotically or in a finite number of steps. We introduce\na new unifying notion of stable feedback policy, based on perturbation of the\ncost per stage, which in addition to implying convergence of the generated\nstates to the destination, quantifies the speed of convergence. We consider the\nproperties of two distinct cost functions: $\\jstar$, the overall optimal, and\n$\\hat J$, the restricted optimal over just the stable policies. Different\nclasses of stable policies (with different speeds of convergence) may yield\ndifferent values of $\\hat J$. We show that for any class of stable policies,\n$\\hat J$ is a solution of Bellman's equation, and we characterize the smallest\nand the largest solutions: they are $\\jstar$, and $J^+$, the restricted optimal\ncost function over the class of (finitely) terminating policies. We also\ncharacterize the regions of convergence of various modified versions of value\nand policy iteration algorithms, as substitutes for the standard algorithms,\nwhich may not work in general."""
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1711.10129,https://arxiv.org/abs/1711.10129,"b""Abstract:  We consider stochastic shortest path problems with infinite state and control\nspaces, a nonnegative cost per stage, and a termination state. We extend the\nnotion of a proper policy, a policy that terminates within a finite expected\nnumber of steps, from the context of finite state space to the context of\ninfinite state space. We consider the optimal cost function $\\jstar$, and the\noptimal cost function $\\hat J$ over just the proper policies. We show that\n$\\jstar$ and $\\hat J$ are the smallest and largest solutions of Bellman's\nequation, respectively, within a suitable class of Lyapounov-like functions. If\nthe cost per stage is bounded, these functions are those that are bounded over\nthe effective domain of $\\hat J$. The standard value iteration algorithm may be\nattracted to either $\\jstar$ or $\\hat J$, depending on the initial condition.\nIn the favorable case where $\\jstar=\\hat J$, strong analytical and algorithmic\nresults are obtained."""
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1610.05427,https://arxiv.org/abs/1610.05427,"b'Abstract:  We consider large linear and nonlinear fixed point problems, and solution\nwith proximal algorithms. We show that there is a close connection between two\nseemingly different types of methods from distinct fields: 1) Proximal\niterations for linear systems of equations, which are prominent in numerical\nanalysis and convex optimization, and 2) Temporal difference (TD) type methods,\nsuch as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in\nsimulation-based approximate dynamic programming/reinforcement learning\n(DP/RL), and its recent prominent successes in large-scale game contexts, among\nothers.\nOne benefit of this connection is a new and simple way to accelerate the\nstandard proximal algorithm by extrapolation towards the TD iteration, which\ngenerically has a faster convergence rate. Another benefit is the potential\nintegration into the proximal algorithmic context of several new ideas that\nhave emerged in the DP/RL context. We discuss some of the possibilities, and in\nparticular, algorithms that project each proximal iterate onto the subspace\nspanned by a small number of basis functions, using low-dimensional\ncalculations and simulation. A third benefit is that insights and analysis from\nproximal algorithms can be brought to bear on the enhancement of TD methods.\nThe linear fixed point methodology can be extended to nonlinear fixed point\nproblems involving a contraction, thus providing guaranteed and potentially\nsubstantial acceleration of the proximal and forward backward splitting\nalgorithms at no extra cost. Moreover, the connection of proximal and TD\nmethods can be extended to nonlinear (nondifferentiable) fixed point problems\nthrough new proximal-like algorithms that involve successive linearization,\nsimilar to policy iteration in DP.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1609.03115,https://arxiv.org/abs/1609.03115,"b'Abstract:  We consider challenging dynamic programming models where the associated\nBellman equation, and the value and policy iteration algorithms commonly\nexhibit complex and even pathological behavior. Our analysis is based on the\nnew notion of regular policies. These are policies that are well-behaved with\nrespect to value and policy iteration, and are patterned after proper policies,\nwhich are central in the theory of stochastic shortest path problems. We show\nthat the optimal cost function over regular policies may have favorable value\nand policy iteration properties, which the optimal cost function over all\npolicies need not have. We accordingly develop a unifying methodology to\naddress long standing analytical and algorithmic issues in broad classes of\nundiscounted models, including stochastic and minimax shortest path problems,\nas well as positive cost, negative cost, risk-sensitive, and multiplicative\ncost problems.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1608.01670,https://arxiv.org/abs/1608.01670,"b'Abstract:  In this paper we consider shortest path problems in a directed graph where\nthe transitions between nodes are subject to uncertainty. We use a minimax\nformulation, where the objective is to guarantee that a special destination\nstate is reached with a minimum cost path under the worst possible instance of\nthe uncertainty. Problems of this type arise, among others, in planning and\npursuit-evasion contexts, and in model predictive control. Our analysis makes\nuse of the recently developed theory of abstract semicontractive dynamic\nprogramming models. We investigate questions of existence and uniqueness of\nsolution of the optimality equation, existence of optimal paths, and the\nvalidity of various algorithms patterned after the classical methods of value\nand policy iteration, as well as a Dijkstra-like algorithm for problems with\nnonnegative arc lengths.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1608.01393,https://arxiv.org/abs/1608.01393,"b""Abstract:  In this paper we consider a broad class of infinite horizon discrete-time\noptimal control models that involve a nonnegative cost function and an affine\nmapping in their dynamic programming equation. They include as special cases\nclassical models such as stochastic undiscounted nonnegative cost problems,\nstochastic multiplicative cost problems, and risk-sensitive problems with\nexponential cost. We focus on the case where the state space is finite and the\ncontrol space has some compactness properties. We assume that the affine\nmapping has a semicontractive character, whereby for some policies it is a\ncontraction, while for others it is not. In one line of analysis, we impose\nassumptions that guarantee that the latter policies cannot be optimal. Under\nthese assumptions, we prove strong results that resemble those for discounted\nMarkovian decision problems, such as the uniqueness of solution of Bellman's\nequation, and the validity of forms of value and policy iteration. In the\nabsence of these assumptions, the results are weaker and unusual in character:\nthe optimal cost function need not be a solution of Bellman's equation, and an\noptimal policy may not be found by value or policy iteration. Instead the\noptimal cost function over just the contractive policies solves Bellman's\nequation, and can be computed by a variety of algorithms."""
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1509.09257,https://arxiv.org/abs/1509.09257,"b'Abstract:  We consider minimization of the sum of a large number of convex functions,\nand we propose an incremental aggregated version of the proximal algorithm,\nwhich bears similarity to the incremental aggregated gradient and subgradient\nmethods that have received a lot of recent attention. Under cost function\ndifferentiability and strong convexity assumptions, we show linear convergence\nfor a sufficiently small constant stepsize. This result also applies to\ndistributed asynchronous variants of the method, involving bounded\ninterprocessor communication delays.\nWe then consider dual versions of incremental proximal algorithms, which are\nincremental augmented Lagrangian methods for separable equality-constrained\noptimization problems. Contrary to the standard augmented Lagrangian method,\nthese methods admit decomposition in the minimization of the augmented\nLagrangian, and update the multipliers far more frequently. Our incremental\naggregated augmented Lagrangian methods bear similarity to several known\ndecomposition algorithms, including the alternating direction method of\nmultipliers (ADMM) and more recent variations. We compare these methods in\nterms of their properties, and highlight their potential advantages and\nlimitations.\nWe also address the solution of separable inequality-constrained optimization\nproblems through the use of nonquadratic augmented Lagrangiias such as the\nexponential, and we dually consider a corresponding incremental aggregated\nversion of the proximal algorithm that uses nonquadratic regularization, such\nas an entropy function. We finally propose a closely related linearly\nconvergent method for minimization of large differentiable sums subject to an\northant constraint, which may be viewed as an incremental aggregated version of\nthe mirror descent method.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1507.01030,https://arxiv.org/abs/1507.01030,"b'Abstract:  We survey incremental methods for minimizing a sum $\\sum_{i=1}^mf_i(x)$\nconsisting of a large number of convex component functions $f_i$. Our methods\nconsist of iterations applied to single components, and have proved very\neffective in practice. We introduce a unified algorithmic framework for a\nvariety of such methods, some involving gradient and subgradient iterations,\nwhich are known, and some involving combinations of subgradient and proximal\nmethods, which are new and offer greater flexibility in exploiting the special\nstructure of $f_i$. We provide an analysis of the convergence and rate of\nconvergence properties of these methods, including the advantages offered by\nrandomization in the selection of components. We also survey applications in\ninference/machine learning, signal processing, and large-scale and distributed\noptimization.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1507.01029,https://arxiv.org/abs/1507.01029,"b'Abstract:  In this paper we discuss $\\l$-policy iteration, a method for exact and\napproximate dynamic programming. It is intermediate between the classical value\niteration (VI) and policy iteration (PI) methods, and it is closely related to\noptimistic (also known as modified) PI, whereby each policy evaluation is done\napproximately, using a finite number of VI. We review the theory of the method\nand associated questions of bias and exploration arising in simulation-based\ncost function approximation. We then discuss various implementations, which\noffer advantages over well-established PI methods that use LSPE($\\l$),\nLSTD($\\l$), or TD($\\l$) for policy evaluation with cost function approximation.\nOne of these implementations is based on a new simulation scheme, called\ngeometric sampling, which uses multiple short trajectories rather than a single\ninfinitely long trajectory.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1507.01026,https://arxiv.org/abs/1507.01026,"b""Abstract:  In this paper, we consider discrete-time infinite horizon problems of optimal\ncontrol to a terminal set of states. These are the problems that are often\ntaken as the starting point for adaptive dynamic programming. Under very\ngeneral assumptions, we establish the uniqueness of solution of Bellman's\nequation, and we provide convergence results for value and policy iteration."""
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1507.00702,https://arxiv.org/abs/1507.00702,"b'Abstract:  We consider Newton methods for common types of single commodity and\nmulti-commodity network flow problems. Despite the potentially very large\ndimension of the problem, they can be implemented using the conjugate gradient\nmethod and low-dimensional network operations, as shown nearly thirty years\nago. We revisit these methods, compare them to more recent proposals, and\ndescribe how they can be implemented in a distributed computing system. We also\ndiscuss generalizations, including the treatment of arc gains, linear side\nconstraints, and related special structures.'"
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1308.3814,https://arxiv.org/abs/1308.3814,"b""Abstract:  We consider stochastic control models with Borel spaces and universally\nmeasurable policies. For such models the standard policy iteration is known to\nhave difficult measurability issues and cannot be carried out in general. We\npresent a mixed value and policy iteration method that circumvents this\ndifficulty. The method allows the use of stationary policies in computing the\noptimal cost function, in a manner that resembles policy iteration. It can also\nbe used to address similar difficulties of policy iteration in the context of\nupper and lower semicontinuous models. We analyze the convergence of the method\nin infinite horizon total cost problems, for the discounted case where the\none-stage costs are bounded, and for the undiscounted case where the one-stage\ncosts are nonpositive or nonnegative.\nFor undiscounted total cost problems with nonnegative one-stage costs, we\nalso give a new convergence theorem for value iteration, which shows that value\niteration converges whenever it is initialized with a function that is above\nthe optimal cost function and yet bounded by a multiple of the optimal cost\nfunction. This condition resembles Whittle's bridging condition and is partly\nmotivated by it. The theorem is also partly motivated by a result of Maitra and\nSudderth, which showed that value iteration, when initialized with the constant\nfunction zero, could require a transfinite number of iterations to converge. We\nuse the new convergence theorem for value iteration to establish the\nconvergence of our mixed value and policy iteration method for the nonnegative\ncost case."""
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1207.4154,https://arxiv.org/abs/1207.4154,"b'Abstract:  In this paper, we propose a new lower approximation scheme for POMDP with\ndiscounted and average cost criterion. The approximating functions are\ndetermined by their values at a finite number of belief points, and can be\ncomputed efficiently using value iteration algorithms for finite-state MDP.\nWhile for discounted problems several lower approximation schemes have been\nproposed earlier, ours seems the first of its kind for average cost problems.\nWe focus primarily on the average cost case, and we show that the corresponding\napproximation can be computed efficiently using multi-chain algorithms for\nfinite-state MDP. We give a preliminary analysis showing that regardless of the\nexistence of the optimal average cost J in the POMDP, the approximation\nobtained is a lower bound of the liminf optimal average cost function, and can\nalso be used to calculate an upper bound on the limsup optimal average cost\nfunction, as well as bounds on the cost of executing the stationary policy\nassociated with the approximation. Weshow the convergence of the cost\napproximation, when the optimal average cost is constant and the optimal\ndifferential cost is continuous.'"
Robert Berwick,Berwick_Robert,arXiv:1811.02611,https://arxiv.org/abs/1811.02611,"b""Abstract:  While long short-term memory (LSTM) neural net architectures are designed to\ncapture sequence information, human language is generally composed of\nhierarchical structures. This raises the question as to whether LSTMs can learn\nhierarchical structures. We explore this question with a well-formed bracket\nprediction task using two types of brackets modeled by an LSTM. Demonstrating\nthat such a system is learnable by an LSTM is the first step in demonstrating\nthat the entire class of CFLs is also learnable. We observe that the model\nrequires exponential memory in terms of the number of characters and embedded\ndepth, where a sub-linear memory should suffice. Still, the model does more\nthan memorize the training input. It learns how to distinguish between relevant\nand irrelevant information. On the other hand, we also observe that the model\ndoes not generalize well. We conclude that LSTMs do not learn the relevant\nunderlying context-free rules, suggesting the good overall performance is\nattained rather by an efficient way of evaluating nuisance variables. LSTMs are\na way to quickly reach good results for many natural language tasks, but to\nunderstand and generate natural language one has to investigate other concepts\nthat can make more direct use of natural language's structural nature."""
Robert Berwick,Berwick_Robert,arXiv:1803.09832,https://arxiv.org/abs/1803.09832,"b'Abstract:  We consider two different data sets of syntactic parameters and we discuss\nhow to detect relations between parameters through a heat kernel method\ndeveloped by Belkin-Niyogi, which produces low dimensional representations of\nthe data, based on Laplace eigenfunctions, that preserve neighborhood\ninformation. We analyze the different connectivity and clustering structures\nthat arise in the two datasets, and the regions of maximal variance in the\ntwo-parameter space of the Belkin-Niyogi construction, which identify\npreferable choices of independent variables. We compute clustering coefficients\nand their variance.'"
Robert Berwick,Berwick_Robert,arXiv:1712.01719,https://arxiv.org/abs/1712.01719,"b""Abstract:  Using Phylogenetic Algebraic Geometry, we analyze computationally the\nphylogenetic tree of subfamilies of the Indo-European language family, using\ndata of syntactic structures. The two main sources of syntactic data are the\nSSWL database and Longobardi's recent data of syntactic parameters. We compute\nphylogenetic invariants and likelihood functions for two sets of Germanic\nlanguages, a set of Romance languages, a set of Slavic languages and a set of\nearly Indo-European languages, and we compare the results with what is known\nthrough historical linguistics."""
Robert Berwick,Berwick_Robert,arXiv:cmp-lg/9503012,https://arxiv.org/abs/cmp-lg/9503012,"b""Abstract:  In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the\nbasis of Zipf rank frequency data that noncoding DNA sequence regions are more\nlike natural languages than coding regions. We argue on the contrary that an\nempirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to\nnatural languages. Although DNA is a presumably an ``organized system of\nsigns'' in Mandelbrot's (1961) sense, an observation of statistical features of\nthe sort presented in the Mantegna et al. paper does not shed light on the\nsimilarity between DNA's ``grammar'' and natural language grammars, just as the\nobservation of exact Zipf-like behavior cannot distinguish between the\nunderlying processes of tossing an $M$ sided die or a finite-state branching\nprocess."""
Sangeeta Bhatia,Bhatia_Sangeeta,arXiv:1610.00077,https://arxiv.org/abs/1610.00077,"b""Abstract:  Modellers of large scale genome rearrangement events, in which segments of\nDNA are inverted, moved, swapped, or even inserted or deleted, have found a\nnatural syntax in the language of permutations. Despite this, there has been a\nwide range of modelling choices, assumptions and interpretations that make\nnavigating the literature a significant challenge. Indeed, even authors of\npapers that use permutations to model genome rearrangement can struggle to\ninterpret each others' work, because of subtle differences in basic assumptions\nthat are often deeply ingrained (and consequently sometimes not even\nmentioned). In this paper, we describe the different ways in which permutations\nhave been used to model genomes and genome rearrangement events, presenting\nsome features and limitations of each approach, and show how the various models\nare related. This paper will help researchers navigate the landscape of genome\nrearrangement models, and make it easier for authors to present clear and\nconsistent models."""
Sangeeta Bhatia,Bhatia_Sangeeta,arXiv:1409.7146,https://arxiv.org/abs/1409.7146,"b'Abstract:  Establishing a distance between genomes is a significant problem in\ncomputational genomics, because its solution can be used to establish\nevolutionary relationships including phylogeny.\nThe ""double cut and join"" (DCJ) model of chromosomal rearrangement proposed\nby Yancopoulos et al. has received attention as it can model inversions,\ntranslocations, fusion and fission on a multichromosomal genome that may\ncontain both linear and circular chromosomes. In this paper, we realize the DCJ\noperator as a group action on the space of multichromosomal genomes. We study\nthis group action, deriving some properties of the group and finding\ngroup-theoretic analogues for the key results in the DCJ theory.'"
Duane Boning,Boning_Duane,arXiv:1902.10660,https://arxiv.org/abs/1902.10660,"b'Abstract:  Although adversarial examples and model robustness have been extensively\nstudied in the context of linear models and neural networks, research on this\nissue in tree-based models and how to make tree-based models robust against\nadversarial examples is still limited. In this paper, we show that tree based\nmodels are also vulnerable to adversarial examples and develop a novel\nalgorithm to learn robust trees. At its core, our method aims to optimize the\nperformance under the worst-case perturbation of input features, which leads to\na max-min saddle point problem. Incorporating this saddle point objective into\nthe decision tree building procedure is non-trivial due to the discrete nature\nof trees --- a naive approach to finding the best split according to this\nsaddle point objective will take exponential time. To make our approach\npractical and scalable, we propose efficient tree building algorithms by\napproximating the inner minimizer in this saddle point problem, and present\nefficient implementations for classical information gain based trees as well as\nstate-of-the-art tree boosting models such as XGBoost. Experimental results on\nreal world datasets demonstrate that the proposed algorithms can substantially\nimprove the robustness of tree-based models against adversarial examples.'"
Duane Boning,Boning_Duane,arXiv:1901.04684,https://arxiv.org/abs/1901.04684,"b'Abstract:  The adversarial training procedure proposed by Madry et al. (2018) is one of\nthe most effective methods to defend against adversarial examples in deep\nneural networks (DNNs). In our paper, we shed some lights on the practicality\nand the hardness of adversarial training by showing that the effectiveness\n(robustness on test set) of adversarial training has a strong correlation with\nthe distance between a test point and the manifold of training data embedded by\nthe network. Test examples that are relatively far away from this manifold are\nmore likely to be vulnerable to adversarial attacks. Consequentially, an\nadversarial training based defense is susceptible to a new class of attacks,\nthe ""blind-spot attack"", where the input images reside in ""blind-spots"" (low\ndensity regions) of the empirical distribution of training data but is still on\nthe ground-truth data manifold. For MNIST, we found that these blind-spots can\nbe easily found by simply scaling and shifting image pixel values. Most\nimportantly, for large datasets with high dimensional and complex data manifold\n(CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training\nmakes defending on any valid test examples difficult due to the curse of\ndimensionality and the scarcity of training data. Additionally, we find that\nblind-spots also exist on provable defenses including (Wong & Kolter, 2018) and\n(Sinha et al., 2018) because these trainable robustness certificates can only\nbe practically optimized on a limited set of training data.'"
Duane Boning,Boning_Duane,arXiv:1804.09699,https://arxiv.org/abs/1804.09699,"b'Abstract:  Verifying the robustness property of a general Rectified Linear Unit (ReLU)\nnetwork is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer\nCAV17]. Although finding the exact minimum adversarial distortion is hard,\ngiving a certified lower bound of the minimum distortion is possible. Current\navailable methods of computing such a bound are either time-consuming or\ndelivering low quality bounds that are too loose to be useful. In this paper,\nwe exploit the special structure of ReLU networks and provide two\ncomputationally efficient algorithms Fast-Lin and Fast-Lip that are able to\ncertify non-trivial lower bounds of minimum distortions, by bounding the ReLU\nunits with appropriate linear functions Fast-Lin, or by bounding the local\nLipschitz constant Fast-Lip. Experiments show that (1) our proposed methods\ndeliver bounds close to (the gap is 2-3X) exact minimum distortion found by\nReluplex in small MNIST networks while our algorithms are more than 10,000\ntimes faster; (2) our methods deliver similar quality of bounds (the gap is\nwithin 35% and usually around 10%; sometimes our bounds are even better) for\nlarger networks compared to the methods based on solving linear programming\nproblems but our algorithms are 33-14,000 times faster; (3) our method is\ncapable of solving large MNIST and CIFAR networks up to 7 layers with more than\n10,000 neurons within tens of seconds on a single CPU core.\nIn addition, we show that, in fact, there is no polynomial time algorithm\nthat can approximately find the minimum $\\ell_1$ adversarial distortion of a\nReLU network with a $0.99\\ln n$ approximation ratio unless\n$\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network.'"
Duane Boning,Boning_Duane,arXiv:1703.09876,https://arxiv.org/abs/1703.09876,"b'Abstract:  In this paper, we propose a novel method to estimate and characterize spatial\nvariations on dies or wafers. This new technique exploits recent developments\nin matrix completion, enabling estimation of spatial variation across wafers or\ndies with a small number of randomly picked sampling points while still\nachieving fairly high accuracy. This new approach can be easily generalized,\nincluding for estimation of mixed spatial and structure or device type\ninformation.'"
Guy Bresler,Bresler_Guy,arXiv:1902.07380,https://arxiv.org/abs/1902.07380,"b'Abstract:  In the past decade, sparse principal component analysis has emerged as an\narchetypal problem for illustrating statistical-computational tradeoffs. This\ntrend has largely been driven by a line of research aiming to characterize the\naverage-case complexity of sparse PCA through reductions from the planted\nclique (PC) conjecture - which conjectures that there is no polynomial-time\nalgorithm to detect a planted clique of size $K = o(N^{1/2})$ in\n$\\mathcal{G}(N, \\frac{1}{2})$. All previous reductions to sparse PCA either\nfail to show tight computational lower bounds matching existing algorithms or\nshow lower bounds for formulations of sparse PCA other than its canonical\ngenerative model, the spiked covariance model. Also, these lower bounds all\nquickly degrade with the exponent in the PC conjecture. Specifically, when only\ngiven the PC conjecture up to $K = o(N^\\alpha)$ where $\\alpha < 1/2$, there is\nno sparsity level $k$ at which these lower bounds remain tight. If $\\alpha \\le\n1/3$ these reductions fail to even show the existence of a\nstatistical-computational tradeoff at any sparsity $k$. We give a reduction\nfrom PC that yields the first full characterization of the computational\nbarrier in the spiked covariance model, providing tight lower bounds at all\nsparsities $k$. We also show the surprising result that weaker forms of the PC\nconjecture up to clique size $K = o(N^\\alpha)$ for any given $\\alpha \\in (0,\n1/2]$ imply tight computational lower bounds for sparse PCA at sparsities $k =\no(n^{\\alpha/3})$. This shows that even a mild improvement in the signal\nstrength needed by the best known polynomial-time sparse PCA algorithms would\nimply that the hardness threshold for PC is subpolynomial. This is the first\ninstance of a suboptimal hardness assumption implying optimal lower bounds for\nanother problem in unsupervised learning.'"
Guy Bresler,Bresler_Guy,arXiv:1902.06916,https://arxiv.org/abs/1902.06916,"b'Abstract:  In the general submatrix detection problem, the task is to detect the\npresence of a small $k \\times k$ submatrix with entries sampled from a\ndistribution $\\mathcal{P}$ in an $n \\times n$ matrix of samples from\n$\\mathcal{Q}$. This formulation includes a number of well-studied problems,\nsuch as biclustering when $\\mathcal{P}$ and $\\mathcal{Q}$ are Gaussians and the\nplanted dense subgraph formulation of community detection when the submatrix is\na principal minor and $\\mathcal{P}$ and $\\mathcal{Q}$ are Bernoulli random\nvariables. These problems all seem to exhibit a universal phenomenon: there is\na statistical-computational gap depending on $\\mathcal{P}$ and $\\mathcal{Q}$\nbetween the minimum $k$ at which this task can be solved and the minimum $k$ at\nwhich it can be solved in polynomial time. Our main result is to tightly\ncharacterize this computational barrier as a tradeoff between $k$ and the KL\ndivergences between $\\mathcal{P}$ and $\\mathcal{Q}$ through average-case\nreductions from the planted clique conjecture. These computational lower bounds\nhold given mild assumptions on $\\mathcal{P}$ and $\\mathcal{Q}$ arising\nnaturally from classical binary hypothesis testing. Our results recover and\ngeneralize the planted clique lower bounds for Gaussian biclustering in Ma-Wu\n(2015) and Brennan et al. (2018) and for the sparse and general regimes of\nplanted dense subgraph in Hajek et al. (2015) and Brennan et al. (2018). This\nyields the first universality principle for computational lower bounds obtained\nthrough average-case reductions.'"
Guy Bresler,Bresler_Guy,arXiv:1811.10106,https://arxiv.org/abs/1811.10106,"b'Abstract:  Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR)\nhave a wide range of applications and have attracted a tremendous amount of\nattention in the last two decades as canonical examples of statistical problems\nin high dimension. A variety of algorithms have been proposed for both SPCA and\nSLR, but an explicit connection between the two had not been made. We show how\nto efficiently transform a black-box solver for SLR into an algorithm for SPCA:\nassuming the SLR solver satisfies prediction error guarantees achieved by\nexisting efficient algorithms such as those based on the Lasso, the SPCA\nalgorithm derived from it achieves near state of the art guarantees for testing\nand for support recovery for the single spiked covariance model as obtained by\nthe current best polynomialtime algorithms. Our reduction not only highlights\nthe inherent similarity between the two problems, but also, from a practical\nstandpoint, allows one to obtain a collection of algorithms for SPCA directly\nfrom known algorithms for SLR. We provide experimental results on simulated\ndata comparing our proposed framework to other algorithms for SPCA.'"
Guy Bresler,Bresler_Guy,arXiv:1806.07508,https://arxiv.org/abs/1806.07508,"b'Abstract:  The prototypical high-dimensional statistics problem entails finding a\nstructured signal in noise. Many of these problems exhibit an intriguing\nphenomenon: the amount of data needed by all known computationally efficient\nalgorithms far exceeds what is needed for inefficient algorithms that search\nover all possible structures. A line of work initiated by Berthet and Rigollet\nin 2013 has aimed to explain these statistical-computational gaps by reducing\nfrom conjecturally hard average-case problems in computer science. However, the\ndelicate nature of average-case reductions has limited the applicability of\nthis approach. In this work we introduce several new techniques to give a web\nof average-case reductions showing strong computational lower bounds based on\nthe planted clique conjecture using natural problems as intermediates. These\ninclude tight lower bounds for Planted Independent Set, Planted Dense Subgraph,\nSparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block\nModel and a biased variant of Sparse PCA. We also give algorithms matching our\nlower bounds and identify the information-theoretic limits of the models we\nconsider.'"
Guy Bresler,Bresler_Guy,arXiv:1805.10262,https://arxiv.org/abs/1805.10262,"b'Abstract:  Graphical models are a rich language for describing high-dimensional\ndistributions in terms of their dependence structure. While there are\nalgorithms with provable guarantees for learning undirected graphical models in\na variety of settings, there has been much less progress in the important\nscenario when there are latent variables. Here we study Restricted Boltzmann\nMachines (or RBMs), which are a popular model with wide-ranging applications in\ndimensionality reduction, collaborative filtering, topic modeling, feature\nextraction and deep learning.\nThe main message of our paper is a strong dichotomy in the feasibility of\nlearning RBMs, depending on the nature of the interactions between variables:\nferromagnetic models can be learned efficiently, while general models cannot.\nIn particular, we give a simple greedy algorithm based on influence\nmaximization to learn ferromagnetic RBMs with bounded degree. In fact, we learn\na description of the distribution on the observed variables as a Markov Random\nField. Our analysis is based on tools from mathematical physics that were\ndeveloped to show the concavity of magnetization. Our algorithm extends\nstraighforwardly to general ferromagnetic Ising models with latent variables.\nConversely, we show that even for a contant number of latent variables with\nconstant degree, without ferromagneticity the problem is as hard as sparse\nparity with noise. This hardness result is based on a sharp and surprising\ncharacterization of the representational power of bounded degree RBMs: the\ndistribution on their observed variables can simulate any bounded order MRF.\nThis result is of independent interest since RBMs are the building blocks of\ndeep belief networks.'"
Guy Bresler,Bresler_Guy,arXiv:1805.03027,https://arxiv.org/abs/1805.03027,"b'Abstract:  Most information systems store data by modifying the local state of matter,\nin the hope that atomic (or sub-atomic) local interactions would stabilize the\nstate for a sufficiently long time, thereby allowing later recovery. In this\nwork we initiate the study of information retention in locally-interacting\nsystems. The evolution in time of the interacting particles is modeled via the\nstochastic Ising model (SIM). The initial spin configuration $X_0$ serves as\nthe user-controlled input. The output configuration $X_t$ is produced by\nrunning $t$ steps of the Glauber chain. Our main goal is to evaluate the\ninformation capacity $I_n(t)\\triangleq\\max_{p_{X_0}}I(X_0;X_t)$ when the time\n$t$ scales with the size of the system $n$. For the zero-temperature SIM on the\ntwo-dimensional $\\sqrt{n}\\times\\sqrt{n}$ grid and free boundary conditions, it\nis easy to show that $I_n(t) = \\Theta(n)$ for $t=O(n)$. In addition, we show\nthat on the order of $\\sqrt{n}$ bits can be stored for infinite time in striped\nconfigurations. The $\\sqrt{n}$ achievability is optimal when $t\\to\\infty$ and\n$n$ is fixed.\nOne of the main results of this work is an achievability scheme that stores\nmore than $\\sqrt{n}$ bits (in orders of magnitude) for superlinear (in $n$)\ntimes. The analysis of the scheme decomposes the system into $\\Omega(\\sqrt{n})$\nindependent Z-channels whose crossover probability is found via the (recently\nrigorously established) Lifshitz law of phase boundary movement. We also\nprovide results for the positive but small temperature regime. We show that an\ninitial configuration drawn according to the Gibbs measure cannot retain more\nthan a single bit for $t\\geq e^{cn^{\\frac{1}{4}+\\epsilon}}$. On the other hand,\nwhen scaling time with $\\beta$, the stripe-based coding scheme (that stores for\ninfinite time at zero temperature) is shown to retain its bits for time that is\nexponential in $\\beta$.'"
Guy Bresler,Bresler_Guy,arXiv:1802.06186,https://arxiv.org/abs/1802.06186,"b""Abstract:  We study the problem of testing, using only a single sample, between mean\nfield distributions (like Curie-Weiss, Erd\xc5\x91s-R\xc3\xa9nyi) and structured Gibbs\ndistributions (like Ising model on sparse graphs and Exponential Random\nGraphs). Our goal is to test without knowing the parameter values of the\nunderlying models: only the \\emph{structure} of dependencies is known. We\ndevelop a new approach that applies to both the Ising and Exponential Random\nGraph settings based on a general and natural statistical test. The test can\ndistinguish the hypotheses with high probability above a certain threshold in\nthe (inverse) temperature parameter, and is optimal in that below the threshold\nno test can distinguish the hypotheses.\nThe thresholds do not correspond to the presence of long-range order in the\nmodels. By aggregating information at a global scale, our test works even at\nvery high temperatures.\nThe proofs are based on distributional approximation and sharp concentration\nof quadratic forms, when restricted to Hamming spheres. The restriction to\nHamming spheres is necessary, since otherwise any scalar statistic is useless\nwithout explicit knowledge of the temperature parameter. At the same time, this\nrestriction radically changes the behavior of the functions under\nconsideration, resulting in a much smaller variance than in the independent\nsetting; this makes it hard to directly apply standard methods (i.e., Stein's\nmethod) for concentration of weakly dependent variables. Instead, we carry out\nan additional tensorization argument using a Markov chain that respects the\nsymmetry of the Hamming sphere."""
Guy Bresler,Bresler_Guy,arXiv:1712.05743,https://arxiv.org/abs/1712.05743,"b""Abstract:  We develop a new technique, based on Stein's method, for comparing two\nstationary distributions of irreducible Markov Chains whose update rules are\n`close enough'. We apply this technique to compare Ising models on $d$-regular\nexpander graphs to the Curie-Weiss model (complete graph) in terms of pairwise\ncorrelations and more generally $k$th order moments. Concretely, we show that\n$d$-regular Ramanujan graphs approximate the $k$th order moments of the\nCurie-Weiss model to within average error $k/\\sqrt{d}$ (averaged over the size\n$k$ subsets). The result applies even in the low-temperature regime; we also\nderive some simpler approximation results for functionals of Ising models that\nhold only at high enough temperatures."""
Guy Bresler,Bresler_Guy,arXiv:1711.02198,https://arxiv.org/abs/1711.02198,"b""Abstract:  We consider an online model for recommendation systems, with each user being\nrecommended an item at each time-step and providing 'like' or 'dislike'\nfeedback. A latent variable model specifies the user preferences: both users\nand items are clustered into types. All users of a given type have identical\npreferences for the items, and similarly, items of a given type are either all\nliked or all disliked by a given user. The model captures structure in both the\nitem and user spaces, and in this paper, we assume that the type preference\nmatrix is randomly generated. We describe two algorithms inspired by user-user\nand item-item collaborative filtering (CF), modified to explicitly make\nexploratory recommendations, and prove performance guarantees in terms of their\nexpected regret. For two regimes of model parameters, with structure only in\nitem space or only in user space, we prove information-theoretic lower bounds\non regret that match our upper bounds up to logarithmic factors. Our analysis\nelucidates system operating regimes in which existing CF algorithms are nearly\noptimal."""
Guy Bresler,Bresler_Guy,arXiv:1604.06749,https://arxiv.org/abs/1604.06749,"b'Abstract:  We study the problem of learning a tree Ising model from samples such that\nsubsequent predictions made using the model are accurate. The prediction task\nconsidered in this paper is that of predicting the values of a subset of\nvariables given values of some other subset of variables. Virtually all\nprevious work on graphical model learning has focused on recovering the true\nunderlying graph. We define a distance (""small set TV"" or ssTV) between\ndistributions $P$ and $Q$ by taking the maximum, over all subsets $\\mathcal{S}$\nof a given size, of the total variation between the marginals of $P$ and $Q$ on\n$\\mathcal{S}$; this distance captures the accuracy of the prediction task of\ninterest. We derive non-asymptotic bounds on the number of samples needed to\nget a distribution (from the same class) with small ssTV relative to the one\ngenerating the samples. One of the main messages of this paper is that far\nfewer samples are needed than for recovering the underlying tree, which means\nthat accurate predictions are possible using the wrong tree.'"
Guy Bresler,Bresler_Guy,arXiv:1507.05371,https://arxiv.org/abs/1507.05371,"b'Abstract:  There is much empirical evidence that item-item collaborative filtering works\nwell in practice. Motivated to understand this, we provide a framework to\ndesign and analyze various recommendation algorithms. The setup amounts to\nonline binary matrix completion, where at each time a random user requests a\nrecommendation and the algorithm chooses an entry to reveal in the user\'s row.\nThe goal is to minimize regret, or equivalently to maximize the number of +1\nentries revealed at any time. We analyze an item-item collaborative filtering\nalgorithm that can achieve fundamentally better performance compared to\nuser-user collaborative filtering. The algorithm achieves good ""cold-start""\nperformance (appropriately defined) by quickly making good recommendations to\nnew users about whom there is little information.'"
Guy Bresler,Bresler_Guy,arXiv:1412.1443,https://arxiv.org/abs/1412.1443,"b'Abstract:  In this paper we investigate the computational complexity of learning the\ngraph structure underlying a discrete undirected graphical model from i.i.d.\nsamples. We first observe that the notoriously difficult problem of learning\nparities with noise can be captured as a special case of learning graphical\nmodels. This leads to an unconditional computational lower bound of $\\Omega\n(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree\n$d$, for the class of so-called statistical algorithms recently introduced by\nFeldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime\nrequired to exhaustively search over neighborhoods cannot be significantly\nimproved without restricting the class of models.\nAside from structural assumptions on the graph such as it being a tree,\nhypertree, tree-like, etc., many recent papers on structure learning assume\nthat the model has the correlation decay property. Indeed, focusing on\nferromagnetic Ising models, Bento and Montanari (2009) showed that all known\nlow-complexity algorithms fail to learn simple graphs when the interaction\nstrength exceeds a number related to the correlation decay threshold. Our\nsecond set of results gives a class of repelling (antiferromagnetic) models\nthat have the opposite behavior: very strong interaction allows efficient\nlearning in time $O(p^2)$. We provide an algorithm whose performance\ninterpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the\nrepulsion.'"
Guy Bresler,Bresler_Guy,arXiv:1411.6591,https://arxiv.org/abs/1411.6591,"b'Abstract:  Despite the prevalence of collaborative filtering in recommendation systems,\nthere has been little theoretical development on why and how well it works,\nespecially in the ""online"" setting, where items are recommended to users over\ntime. We address this theoretical gap by introducing a model for online\nrecommendation systems, cast item recommendation under the model as a learning\nproblem, and analyze the performance of a cosine-similarity collaborative\nfiltering method. In our model, each of $n$ users either likes or dislikes each\nof $m$ items. We assume there to be $k$ types of users, and all the users of a\ngiven type share a common string of probabilities determining the chance of\nliking each item. At each time step, we recommend an item to each user, where a\nkey distinction from related bandit literature is that once a user consumes an\nitem (e.g., watches a movie), then that item cannot be recommended to the same\nuser again. The goal is to maximize the number of likable items recommended to\nusers over time. Our main result establishes that after nearly $\\log(km)$\ninitial learning time steps, a simple collaborative filtering algorithm\nachieves essentially optimal performance without knowing $k$. The algorithm has\nan exploitation step that uses cosine similarity and two types of exploration\nsteps, one to explore the space of items (standard in the literature) and the\nother to explore similarity between users (novel to this work).'"
Guy Bresler,Bresler_Guy,arXiv:1411.6156,https://arxiv.org/abs/1411.6156,"b'Abstract:  We consider the problem of reconstructing the graph underlying an Ising model\nfrom i.i.d. samples. Over the last fifteen years this problem has been of\nsignificant interest in the statistics, machine learning, and statistical\nphysics communities, and much of the effort has been directed towards finding\nalgorithms with low computational cost for various restricted classes of\nmodels. Nevertheless, for learning Ising models on general graphs with $p$\nnodes of degree at most $d$, it is not known whether or not it is possible to\nimprove upon the $p^{d}$ computation needed to exhaustively search over all\npossible neighborhoods for each node.\nIn this paper we show that a simple greedy procedure allows to learn the\nstructure of an Ising model on an arbitrary bounded-degree graph in time on the\norder of $p^2$. We make no assumptions on the parameters except what is\nnecessary for identifiability of the model, and in particular the results hold\nat low-temperatures as well as for highly non-uniform models. The proof rests\non a new structural property of Ising models: we show that for any node there\nexists at least one neighbor with which it has a high mutual information. This\nstructural property may be of independent interest.'"
Guy Bresler,Bresler_Guy,arXiv:1410.7659,https://arxiv.org/abs/1410.7659,"b'Abstract:  In this paper we consider the problem of learning undirected graphical models\nfrom data generated according to the Glauber dynamics. The Glauber dynamics is\na Markov chain that sequentially updates individual nodes (variables) in a\ngraphical model and it is frequently used to sample from the stationary\ndistribution (to which it converges given sufficient time). Additionally, the\nGlauber dynamics is a natural dynamical model in a variety of settings. This\nwork deviates from the standard formulation of graphical model learning in the\nliterature, where one assumes access to i.i.d. samples from the distribution.\nMuch of the research on graphical model learning has been directed towards\nfinding algorithms with low computational cost. As the main result of this\nwork, we establish that the problem of reconstructing binary pairwise graphical\nmodels is computationally tractable when we observe the Glauber dynamics.\nSpecifically, we show that a binary pairwise graphical model on $p$ nodes with\nmaximum degree $d$ can be learned in time $f(d)p^2\\log p$, for a function\n$f(d)$, using nearly the information-theoretic minimum number of samples.'"
Guy Bresler,Bresler_Guy,arXiv:1409.3836,https://arxiv.org/abs/1409.3836,"b'Abstract:  We consider the problem of learning the canonical parameters specifying an\nundirected graphical model (Markov random field) from the mean parameters. For\ngraphical models representing a minimal exponential family, the canonical\nparameters are uniquely determined by the mean parameters, so the problem is\nfeasible in principle. The goal of this paper is to investigate the\ncomputational feasibility of this statistical task. Our main result shows that\nparameter estimation is in general intractable: no algorithm can learn the\ncanonical parameters of a generic pair-wise binary graphical model from the\nmean parameters in time bounded by a polynomial in the number of variables\n(unless RP = NP). Indeed, such a result has been believed to be true (see the\nmonograph by Wainwright and Jordan (2008)) but no proof was known.\nOur proof gives a polynomial time reduction from approximating the partition\nfunction of the hard-core model, known to be hard, to learning approximate\nparameters. Our reduction entails showing that the marginal polytope boundary\nhas an inherent repulsive property, which validates an optimization procedure\nover the polytope that does not use any knowledge of its structure (as required\nby the ellipsoid method and others).'"
Guy Bresler,Bresler_Guy,arXiv:1303.5678,https://arxiv.org/abs/1303.5678,"b'Abstract:  We study vector space interference alignment for the MIMO interference\nchannel with no time or frequency diversity, and no symbol extensions. We prove\nboth necessary and sufficient conditions for alignment. In particular, we\ncharacterize the feasibility of alignment for the symmetric three-user channel\nwhere all users transmit along d dimensions, all transmitters have M antennas\nand all receivers have N antennas, as well as feasibility of alignment for the\nfully symmetric (M=N) channel with an arbitrary number of users.\nAn implication of our results is that the total degrees of freedom available\nin a K-user interference channel, using only spatial diversity from the\nmultiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees\nof freedom shown to be possible by Cadambe and Jafar with arbitrarily large\ntime or frequency diversity.\nMoving beyond the question of feasibility, we additionally discuss\ncomputation of the number of solutions using Schubert calculus in cases where\nthere are a finite number of solutions.'"
Guy Bresler,Bresler_Guy,arXiv:1301.0068,https://arxiv.org/abs/1301.0068,"b""Abstract:  We present a framework for the design of optimal assembly algorithms for\nshotgun sequencing under the criterion of complete reconstruction. We derive a\nlower bound on the read length and the coverage depth required for\nreconstruction in terms of the repeat statistics of the genome. Building on\nearlier works, we design a de Brujin graph based assembly algorithm which can\nachieve very close to the lower bound for repeat statistics of a wide range of\nsequenced genomes, including the GAGE datasets. The results are based on a set\nof necessary and sufficient conditions on the DNA sequence and the reads for\nreconstruction. The conditions can be viewed as the shotgun sequencing analogue\nof Ukkonen-Pevzner's necessary and sufficient conditions for Sequencing by\nHybridization."""
Guy Bresler,Bresler_Guy,arXiv:1203.6233,https://arxiv.org/abs/1203.6233,"b'Abstract:  DNA sequencing is the basic workhorse of modern day biology and medicine.\nShotgun sequencing is the dominant technique used: many randomly located short\nfragments called reads are extracted from the DNA sequence, and these reads are\nassembled to reconstruct the original sequence. A basic question is: given a\nsequencing technology and the statistics of the DNA sequence, what is the\nminimum number of reads required for reliable reconstruction? This number\nprovides a fundamental limit to the performance of {\\em any} assembly\nalgorithm. For a simple statistical model of the DNA sequence and the read\nprocess, we show that the answer admits a critical phenomena in the asymptotic\nlimit of long DNA sequences: if the read length is below a threshold,\nreconstruction is impossible no matter how many reads are observed, and if the\nread length is above the threshold, having enough reads to cover the DNA\nsequence is sufficient to reconstruct. The threshold is computed in terms of\nthe Renyi entropy rate of the DNA sequence. We also study the impact of noise\nin the read process on the performance.'"
Guy Bresler,Bresler_Guy,arXiv:1110.5092,https://arxiv.org/abs/1110.5092,"b'Abstract:  This paper studies vector space interference alignment for the three-user\nMIMO interference channel with no time or frequency diversity. The main result\nis a characterization of the feasibility of interference alignment in the\nsymmetric case where all transmitters have M antennas and all receivers have N\nantennas. If N >= M and all users desire d transmit dimensions, then alignment\nis feasible if and only if (2r+1)d <= max(rN,(r+1)M) for all nonnegative\nintegers r. The analogous result holds with M and N switched if M >= N.\nIt turns out that, just as for the 3-user parallel interference channel\n\\cite{BT09}, the length of alignment paths captures the essence of the problem.\nIn fact, for each feasible value of M and N the maximum alignment path length\ndictates both the converse and achievability arguments.\nOne of the implications of our feasibility criterion is that simply counting\nequations and comparing to the number of variables does not predict\nfeasibility. Instead, a more careful investigation of the geometry of the\nalignment problem is required. The necessary condition obtained by counting\nequations is implied by our new feasibility criterion.'"
Guy Bresler,Bresler_Guy,arXiv:1104.0888,https://arxiv.org/abs/1104.0888,"b'Abstract:  Determining the feasibility conditions for vector space interference\nalignment in the K-user MIMO interference channel with constant channel\ncoefficients has attracted much recent attention yet remains unsolved. The main\nresult of this paper is restricted to the symmetric square case where all\ntransmitters and receivers have N antennas, and each user desires d transmit\ndimensions. We prove that alignment is possible if and only if the number of\nantennas satisfies N>= d(K+1)/2. We also show a necessary condition for\nfeasibility of alignment with arbitrary system parameters. An algebraic\ngeometry approach is central to the results.'"
Guy Bresler,Bresler_Guy,arXiv:0812.2265,https://arxiv.org/abs/0812.2265,"b'Abstract:  Exponential random graphs are used extensively in the sociology literature.\nThis model seeks to incorporate in random graphs the notion of reciprocity,\nthat is, the larger than expected number of triangles and other small\nsubgraphs. Sampling from these distributions is crucial for parameter\nestimation hypothesis testing, and more generally for understanding basic\nfeatures of the network model itself. In practice sampling is typically carried\nout using Markov chain Monte Carlo, in particular either the Glauber dynamics\nor the Metropolis-Hasting procedure.\nIn this paper we characterize the high and low temperature regimes of the\nexponential random graph model. We establish that in the high temperature\nregime the mixing time of the Glauber dynamics is $\\Theta(n^2 \\log n)$, where\n$n$ is the number of vertices in the graph; in contrast, we show that in the\nlow temperature regime the mixing is exponentially slow for any local Markov\nchain. Our results, moreover, give a rigorous basis for criticisms made of such\nmodels. In the high temperature regime, where sampling with MCMC is possible,\nwe show that any finite collection of edges are asymptotically independent;\nthus, the model does not possess the desired reciprocity property, and is not\nappreciably different from the Erd\xc5\x91s-R\xc3\xa9nyi random graph.'"
Guy Bresler,Bresler_Guy,arXiv:0809.3554,https://arxiv.org/abs/0809.3554,"b'Abstract:  Recently, Etkin, Tse, and Wang found the capacity region of the two-user\nGaussian interference channel to within one bit/s/Hz. A natural goal is to\napply this approach to the Gaussian interference channel with an arbitrary\nnumber of users. We make progress towards this goal by finding the capacity\nregion of the many-to-one and one-to-many Gaussian interference channels to\nwithin a constant number of bits. The result makes use of a deterministic model\nto provide insight into the Gaussian channel. The deterministic model makes\nexplicit the dimension of signal scale. A central theme emerges: the use of\nlattice codes for alignment of interfering signals on the signal scale.'"
Guy Bresler,Bresler_Guy,arXiv:0807.3222,https://arxiv.org/abs/0807.3222,"b'Abstract:  This paper explores the two-user Gaussian interference channel through the\nlens of a natural deterministic channel model. The main result is that the\ndeterministic channel uniformly approximates the Gaussian channel, the capacity\nregions differing by a universal constant. The problem of finding the capacity\nof the Gaussian channel to within a constant error is therefore reduced to that\nof finding the capacity of the far simpler deterministic channel. Thus, the\npaper provides an alternative derivation of the recent constant gap capacity\ncharacterization of Etkin, Tse, and Wang. Additionally, the deterministic model\ngives significant insight towards the Gaussian channel.'"
Guy Bresler,Bresler_Guy,arXiv:0712.1402,https://arxiv.org/abs/0712.1402,"b'Abstract:  Markov random fields are used to model high dimensional distributions in a\nnumber of applied areas. Much recent interest has been devoted to the\nreconstruction of the dependency structure from independent samples from the\nMarkov random fields. We analyze a simple algorithm for reconstructing the\nunderlying graph defining a Markov random field on $n$ nodes and maximum degree\n$d$ given observations. We show that under mild non-degeneracy conditions it\nreconstructs the generating graph with high probability using $\\Theta(d\n\\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the\nlocal interactions. For most local interaction $\\eps,\\delta$ are of order\n$\\exp(-O(d))$.\nOur results are optimal as a function of $n$ up to a multiplicative constant\ndepending on $d$ and the strength of the local interactions. Our results seem\nto be the first results for general models that guarantee that {\\em the}\ngenerating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}\n\\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the\nmeasure on the graph has correlation decay, the running time is $O(n^2 \\log n)$\nfor all fixed $d$. We also discuss the effect of observing noisy samples and\nshow that as long as the noise level is low, our algorithm is effective. On the\nother hand, we construct an example where large noise implies\nnon-identifiability even for generic noise and interactions. Finally, we\nbriefly show that in some simple cases, models with hidden nodes can also be\nrecovered.'"
Tamara Broderick,Broderick_Tamara,arXiv:1811.11790,https://arxiv.org/abs/1811.11790,"b'Abstract:  Until recently, transcriptomics was limited to bulk RNA sequencing, obscuring\nthe underlying expression patterns of individual cells in favor of a global\naverage. Thanks to technological advances, we can now profile gene expression\nacross thousands or millions of individual cells in parallel. This new type of\ndata has led to the intriguing discovery that individual cell profiles can\nreflect the imprint of time or dynamic processes. However, synthesizing this\ninformation to reconstruct dynamic biological phenomena from data that are\nnoisy, heterogenous, and sparse---and from processes that may unfold\nasynchronously---poses a complex computational and statistical challenge. Here,\nwe develop a full generative model for probabilistically reconstructing trees\nof cellular differentiation from single-cell RNA-seq data. Specifically, we\nextend the framework of the classical Dirichlet diffusion tree to\nsimultaneously infer branch topology and latent cell states along continuous\ntrajectories over the full tree. In tandem, we construct a novel Markov chain\nMonte Carlo sampler that interleaves Metropolis-Hastings and message passing to\nleverage model structure for efficient inference. Finally, we demonstrate that\nthese techniques can recover latent trajectories from simulated single-cell\ntranscriptomes. While this work is motivated by cellular differentiation, we\nderive a tractable model that provides flexible densities for any data (coupled\nwith an appropriate noise model) that arise from continuous evolution along a\nlatent nonparametric tree.'"
Tamara Broderick,Broderick_Tamara,arXiv:1810.06587,https://arxiv.org/abs/1810.06587,"b'Abstract:  A central question in many probabilistic clustering problems is how many\ndistinct clusters are present in a particular dataset. A Bayesian nonparametric\n(BNP) model addresses this question by placing a generative process on cluster\nassignment. However, like all Bayesian approaches, BNP requires the\nspecification of a prior. In practice, it is important to quantitatively\nestablish that the prior is not too informative, particularly when the\nparticular form of the prior is chosen for mathematical convenience rather than\nbecause of a considered subjective belief.\nWe derive local sensitivity measures for a truncated variational Bayes (VB)\napproximation and approximate nonlinear dependence of a VB optimum on prior\nparameters using a local Taylor series approximation. Using a stick-breaking\nrepresentation of a Dirichlet process, we consider perturbations both to the\nscalar concentration parameter and to the functional form of the stick-\nbreaking distribution.\nUnlike previous work on local Bayesian sensitivity for BNP, we pay special\nattention to the ability of our sensitivity measures to extrapolate to\ndifferent priors, rather than treating the sensitivity as a measure of\nrobustness per se. Extrapolation motivates the use of multiplicative\nperturbations to the functional form of the prior for VB. Additionally, we\nlinearly approximate only the computationally intensive part of inference --\nthe optimization of the global parameters -- and retain the nonlinearity of\neasily computed quantities as functions of the global parameters.\nWe apply our methods to estimate sensitivity of the expected number of\ndistinct clusters present in the Iris dataset to the BNP prior specification.\nWe evaluate the accuracy of our approximations by comparing to the much more\nexpensive process of re-fitting the model.'"
Tamara Broderick,Broderick_Tamara,arXiv:1810.04249,https://arxiv.org/abs/1810.04249,"b'Abstract:  Kernel methods offer the flexibility to learn complex relationships in\nmodern, large data sets while enjoying strong theoretical guarantees on\nquality. Unfortunately, these methods typically require cubic running time in\nthe data set size, a prohibitive cost in the large-data setting. Random feature\nmaps (RFMs) and the Nystrom method both consider low-rank approximations to the\nkernel matrix as a potential solution. But, in order to achieve desirable\ntheoretical guarantees, the former may require a prohibitively large number of\nfeatures J+, and the latter may be prohibitively expensive for high-dimensional\nproblems. We propose to combine the simplicity and generality of RFMs with a\ndata-dependent feature selection scheme to achieve desirable theoretical\napproximation properties of Nystrom with just O(log J+) features. Our key\ninsight is to begin with a large set of random features, then reduce them to a\nsmall number of weighted features in a data-dependent, computationally\nefficient way, while preserving the statistical guarantees of using the\noriginal large set of features. We demonstrate the efficacy of our method with\ntheory and experiments--including on a data set with over 50 million\nobservations. In particular, we show that our method achieves small kernel\nmatrix approximation error and better test set accuracy with provably fewer\nrandom features than state-of-the-art methods.'"
Tamara Broderick,Broderick_Tamara,arXiv:1809.09505,https://arxiv.org/abs/1809.09505,"b'Abstract:  Bayesian inference typically requires the computation of an approximation to\nthe posterior distribution. An important requirement for an approximate\nBayesian inference algorithm is to output high-accuracy posterior mean and\nuncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain\nMonte Carlo, remain the gold standard for approximate Bayesian inference\nbecause they have a robust finite-sample theory and reliable convergence\ndiagnostics. However, alternative methods, which are more scalable or apply to\nproblems where Markov Chain Monte Carlo cannot be used, lack the same\nfinite-data approximation theory and tools for evaluating their accuracy. In\nthis work, we develop a flexible new approach to bounding the error of mean and\nuncertainty estimates of scalable inference algorithms. Our strategy is to\ncontrol the estimation errors in terms of Wasserstein distance, then bound the\nWasserstein distance via a generalized notion of Fisher distance. Unlike\ncomputing the Wasserstein distance, which requires access to the normalized\nposterior distribution, the Fisher distance is tractable to compute because it\nrequires access only to the gradient of the log posterior density. We\ndemonstrate the usefulness of our Fisher distance approach by deriving bounds\non the Wasserstein error of the Laplace approximation and Hilbert coresets. We\nanticipate that our approach will be applicable to many other approximate\ninference methods such as the integrated Laplace approximation, variational\ninference, and approximate Bayesian computation'"
Tamara Broderick,Broderick_Tamara,arXiv:1806.10234,https://arxiv.org/abs/1806.10234,"b'Abstract:  Gaussian processes (GPs) offer a flexible class of priors for nonparametric\nBayesian regression, but popular GP posterior inference methods are typically\nprohibitively slow or lack desirable finite-data guarantees on quality. We\ndevelop an approach to scalable approximate GP regression with finite-data\nguarantees on the accuracy of pointwise posterior mean and variance estimates.\nOur main contribution is a novel objective for approximate inference in the\nnonparametric setting: the preconditioned Fisher (pF) divergence. We show that\nunlike the Kullback--Leibler divergence (used in variational inference), the pF\ndivergence bounds the 2-Wasserstein distance, which in turn provides tight\nbounds the pointwise difference of the mean and variance functions. We\ndemonstrate that, for sparse GP likelihood approximations, we can minimize the\npF divergence efficiently. Our experiments show that optimizing the pF\ndivergence has the same computational requirements as variational sparse GPs\nwhile providing comparable empirical performance--in addition to our novel\nfinite-data quality guarantees.'"
Tamara Broderick,Broderick_Tamara,arXiv:1806.00550,https://arxiv.org/abs/1806.00550,"b'Abstract:  The error or variability of machine learning algorithms is often assessed by\nrepeatedly re-fitting a model with different weighted versions of the observed\ndata. The ubiquitous tools of cross-validation (CV) and the bootstrap are\nexamples of this technique. These methods are powerful in large part due to\ntheir model agnosticism but can be slow to run on modern, large data sets due\nto the need to repeatedly re-fit the model. In this work, we use a linear\napproximation to the dependence of the fitting procedure on the weights,\nproducing results that can be faster than repeated re-fitting by orders of\nmagnitude. This linear approximation is sometimes known as the ""infinitesimal\njackknife"" in the statistics literature, where it is mostly used to as a\ntheoretical tool to prove asymptotic results. We provide explicit finite-sample\nerror bounds for the infinitesimal jackknife in terms of a small number of\nsimple, verifiable assumptions. Our results apply whether the weights and data\nare stochastic, deterministic, or even adversarially chosen, and so can be used\nas a tool for proving the accuracy of the infinitesimal jackknife on a wide\nvariety of problems. As a corollary, we state mild regularity conditions under\nwhich our approximation consistently estimates true leave-k-out\ncross-validation for any fixed k. These theoretical results, together with\nmodern automatic differentiation software, support the application of the\ninfinitesimal jackknife to a wide variety of practical problems in machine\nlearning, providing a ""Swiss Army infinitesimal jackknife."" We demonstrate the\naccuracy of our methods on a range of simulated and real datasets.'"
Tamara Broderick,Broderick_Tamara,arXiv:1803.05554,https://arxiv.org/abs/1803.05554,"b'Abstract:  Learning a Bayesian network (BN) from data can be useful for decision-making\nor discovering causal relationships. However, traditional methods often fail in\nmodern applications, which exhibit a larger number of observed variables than\ndata points. The resulting uncertainty about the underlying network as well as\nthe desire to incorporate prior information recommend a Bayesian approach to\nlearning the BN, but the highly combinatorial structure of BNs poses a striking\nchallenge for inference. The current state-of-the-art methods such as order\nMCMC are faster than previous methods but prevent the use of many natural\nstructural priors and still have running time exponential in the maximum\nindegree of the true directed acyclic graph (DAG) of the BN. We here propose an\nalternative posterior approximation based on the observation that, if we\nincorporate empirical conditional independence tests, we can focus on a\nhigh-probability DAG associated with each order of the vertices. We show that\nour method allows the desired flexibility in prior specification, removes\ntiming dependence on the maximum indegree and yields provably good posterior\napproximations; in addition, we show that it achieves superior accuracy,\nscalability, and sampler mixing on several datasets.'"
Tamara Broderick,Broderick_Tamara,arXiv:1802.01737,https://arxiv.org/abs/1802.01737,"b'Abstract:  Coherent uncertainty quantification is a key strength of Bayesian methods.\nBut modern algorithms for approximate Bayesian posterior inference often\nsacrifice accurate posterior uncertainty estimation in the pursuit of\nscalability. This work shows that previous Bayesian coreset construction\nalgorithms---which build a small, weighted subset of the data that approximates\nthe full dataset---are no exception. We demonstrate that these algorithms scale\nthe coreset log-likelihood suboptimally, resulting in underestimated posterior\nuncertainty. To address this shortcoming, we develop greedy iterative geodesic\nascent (GIGA), a novel algorithm for Bayesian coreset construction that scales\nthe coreset log-likelihood optimally. GIGA provides geometric decay in\nposterior approximation error as a function of coreset size, and maintains the\nfast running time of its predecessors. The paper concludes with validation of\nGIGA on both synthetic and real datasets, demonstrating that it reduces\nposterior approximation error by orders of magnitude compared with previous\ncoreset constructions.'"
Tamara Broderick,Broderick_Tamara,arXiv:1712.01435,https://arxiv.org/abs/1712.01435,"b'Abstract:  Clustering procedures typically estimate which data points are clustered\ntogether, a quantity of primary importance in many analyses. Often used as a\npreliminary step for dimensionality reduction or to facilitate interpretation,\nfinding robust and stable clusters is often crucial for appropriate for\ndownstream analysis. In the present work, we consider Bayesian nonparametric\n(BNP) models, a particularly popular set of Bayesian models for clustering due\nto their flexibility. Because of its complexity, the Bayesian posterior often\ncannot be computed exactly, and approximations must be employed. Mean-field\nvariational Bayes forms a posterior approximation by solving an optimization\nproblem and is widely used due to its speed. An exact BNP posterior might vary\ndramatically when presented with different data. As such, stability and\nrobustness of the clustering should be assessed.\nA popular mean to assess stability is to apply the bootstrap by resampling\nthe data, and rerun the clustering for each simulated data set. The time cost\nis thus often very expensive, especially for the sort of exploratory analysis\nwhere clustering is typically used. We propose to use a fast and automatic\napproximation to the full bootstrap called the ""linear bootstrap"", which can be\nseen by local data perturbation. In this work, we demonstrate how to apply this\nidea to a data analysis pipeline, consisting of an MFVB approximation to a BNP\nclustering posterior of time course gene expression data. We show that using\nauto-differentiation tools, the necessary calculations can be done\nautomatically, and that the linear bootstrap is a fast but approximate\nalternative to the bootstrap.'"
Tamara Broderick,Broderick_Tamara,arXiv:1710.05053,https://arxiv.org/abs/1710.05053,"b'Abstract:  The automation of posterior inference in Bayesian data analysis has enabled\nexperts and nonexperts alike to use more sophisticated models, engage in faster\nexploratory modeling and analysis, and ensure experimental reproducibility.\nHowever, standard automated posterior inference algorithms are not tractable at\nthe scale of massive modern datasets, and modifications to make them so are\ntypically model-specific, require expert tuning, and can break theoretical\nguarantees on inferential quality. Building on the Bayesian coresets framework,\nthis work instead takes advantage of data redundancy to shrink the dataset\nitself as a preprocessing step, providing fully-automated, scalable Bayesian\ninference with theoretical guarantees. We begin with an intuitive reformulation\nof Bayesian coreset construction as sparse vector sum approximation, and\ndemonstrate that its automation and performance-based shortcomings arise from\nthe use of the supremum norm. To address these shortcomings we develop Hilbert\ncoresets, i.e., Bayesian coresets constructed under a norm induced by an\ninner-product on the log-likelihood function space. We propose two Hilbert\ncoreset construction algorithms---one based on importance sampling, and one\nbased on the Frank-Wolfe algorithm---along with theoretical guarantees on\napproximation quality as a function of coreset size. Since the exact\ncomputation of the proposed inner-products is model-specific, we automate the\nconstruction with a random finite-dimensional projection of the log-likelihood\nfunctions. The resulting automated coreset construction algorithm is simple to\nimplement, and experiments on a variety of models with real and synthetic\ndatasets show that it provides high-quality posterior approximations and a\nsignificant reduction in the computational cost of inference.'"
Tamara Broderick,Broderick_Tamara,arXiv:1709.09216,https://arxiv.org/abs/1709.09216,"b'Abstract:  Generalized linear models (GLMs) -- such as logistic regression, Poisson\nregression, and robust regression -- provide interpretable models for diverse\ndata types. Probabilistic approaches, particularly Bayesian ones, allow\ncoherent estimates of uncertainty, incorporation of prior information, and\nsharing of power across experiments via hierarchical models. In practice,\nhowever, the approximate Bayesian methods necessary for inference have either\nfailed to scale to large data sets or failed to provide theoretical guarantees\non the quality of inference. We propose a new approach based on constructing\npolynomial approximate sufficient statistics for GLMs (PASS-GLM). We\ndemonstrate that our method admits a simple algorithm as well as trivial\nstreaming and distributed extensions that do not compound error across\ncomputations. We provide theoretical guarantees on the quality of point (MAP)\nestimates, the approximate posterior, and posterior mean and uncertainty\nestimates. We validate our approach empirically in the case of logistic\nregression using a quadratic approximation and show competitive performance\nwith stochastic gradient descent, MCMC, and the Laplace approximation in terms\nof speed and multiple measures of accuracy -- including on an advertising data\nset with 40 million data points and 20,000 covariates.'"
Tamara Broderick,Broderick_Tamara,arXiv:1709.02536,https://arxiv.org/abs/1709.02536,"b'Abstract:  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior\ninference technique that is increasingly popular due to its fast runtimes on\nlarge-scale datasets. However, even when MFVB provides accurate posterior means\nfor certain parameters, it often mis-estimates variances and covariances.\nFurthermore, prior robustness measures have remained undeveloped for MFVB. By\nderiving a simple formula for the effect of infinitesimal model perturbations\non MFVB posterior means, we provide both improved covariance estimates and\nlocal robustness measures for MFVB, thus greatly expanding the practical\nusefulness of MFVB posterior approximations. The estimates for MFVB posterior\ncovariances rely on a result from the classical Bayesian robustness literature\nrelating derivatives of posterior expectations to posterior covariances and\ninclude the Laplace approximation as a special case. Our key condition is that\nthe MFVB approximation provides good estimates of a select subset of posterior\nmeans---an assumption that has been shown to hold in many practical settings.\nIn our experiments, we demonstrate that our methods are simple, general, and\nfast, providing accurate posterior uncertainty estimates and robustness\nmeasures with runtimes that can be an order of magnitude faster than MCMC.'"
Tamara Broderick,Broderick_Tamara,arXiv:1612.05519,https://arxiv.org/abs/1612.05519,"b'Abstract:  Many popular network models rely on the assumption of (vertex)\nexchangeability, in which the distribution of the graph is invariant to\nrelabelings of the vertices. However, the Aldous-Hoover theorem guarantees that\nthese graphs are dense or empty with probability one, whereas many real-world\ngraphs are sparse. We present an alternative notion of exchangeability for\nrandom graphs, which we call edge exchangeability, in which the distribution of\na graph sequence is invariant to the order of the edges. We demonstrate that\nedge-exchangeable models, unlike models that are traditionally vertex\nexchangeable, can exhibit sparsity. To do so, we outline a general framework\nfor graph generative models; by contrast to the pioneering work of Caron and\nFox (2015), models within our framework are stationary across steps of the\ngraph sequence. In particular, our model grows the graph by instantiating more\nlatent atoms of a single random measure as the dataset size increases, rather\nthan adding new atoms to the measure.'"
Tamara Broderick,Broderick_Tamara,arXiv:1611.07469,https://arxiv.org/abs/1611.07469,"b'Abstract:  In Bayesian analysis, the posterior follows from the data and a choice of a\nprior and a likelihood. One hopes that the posterior is robust to reasonable\nvariation in the choice of prior, since this choice is made by the modeler and\nis often somewhat subjective. A different, equally subjectively plausible\nchoice of prior may result in a substantially different posterior, and so\ndifferent conclusions drawn from the data. Were this to be the case, our\nconclusions would not be robust to the choice of prior. To determine whether\nour model is robust, we must quantify how sensitive our posterior is to\nperturbations of our prior. Despite the importance of the problem and a\nconsiderable body of literature, generic, easy-to-use methods to quantify\nBayesian robustness are still lacking.\nAbstract In this paper, we demonstrate that powerful measures of robustness\ncan be easily calculated from Variational Bayes (VB) approximate posteriors. We\nbegin with local robustness, which measures the effect of infinitesimal changes\nto the prior on a posterior mean of interest. In particular, we show that the\ninfluence function of Gustafson (2012) has a simple, easy-to-calculate closed\nform expression for VB approximations. We then demonstrate how local robustness\nmeasures can be inadequate for non-local prior changes, such as replacing one\nprior entirely with another. We propose a simple approximate non-local\nrobustness measure and demonstrate its effectiveness on a simulated data set.'"
Tamara Broderick,Broderick_Tamara,arXiv:1611.05559,https://arxiv.org/abs/1611.05559,"b'Abstract:  Variational inference (VI) provides fast approximations of a Bayesian\nposterior in part because it formulates posterior approximation as an\noptimization problem: to find the closest distribution to the exact posterior\nover some family of distributions. For practical reasons, the family of\ndistributions in VI is usually constrained so that it does not include the\nexact posterior, even as a limit point. Thus, no matter how long VI is run, the\nresulting approximation will not approach the exact posterior. We propose to\ninstead consider a more flexible approximating family consisting of all\npossible finite mixtures of a parametric base distribution (e.g., Gaussian).\nFor efficient inference, we borrow ideas from gradient boosting to develop an\nalgorithm we call boosting variational inference (BVI). BVI iteratively\nimproves the current approximation by mixing it with a new component from the\nbase distribution family and thereby yields progressively more accurate\nposterior approximations as more computing time is spent. Unlike a number of\ncommon VI variants including mean-field VI, BVI is able to capture\nmultimodality, general posterior covariance, and nonstandard posterior shapes.'"
Tamara Broderick,Broderick_Tamara,arXiv:1609.09147,https://arxiv.org/abs/1609.09147,"b'Abstract:  Trait allocations are a class of combinatorial structures in which data may\nbelong to multiple groups and may have different levels of belonging in each\ngroup. Often the data are also exchangeable, i.e., their joint distribution is\ninvariant to reordering. In clustering---a special case of trait\nallocation---exchangeability implies the existence of both a de Finetti\nrepresentation and an exchangeable partition probability function (EPPF),\ndistributional representations useful for computational and theoretical\npurposes. In this work, we develop the analogous de Finetti representation and\nexchangeable trait probability function (ETPF) for trait allocations, along\nwith a characterization of all trait allocations with an ETPF. Unlike previous\nfeature allocation characterizations, our proofs fully capture\nsingle-occurrence ""dust"" groups. We further introduce a novel constrained\nversion of the ETPF that we use to establish an intuitive connection between\nthe probability functions for clustering, feature allocations, and trait\nallocations. As an application of our general theory, we characterize the\ndistribution of all edge-exchangeable graphs, a class of recently-developed\nmodels that captures realistic sparse graph sequences.'"
Tamara Broderick,Broderick_Tamara,arXiv:1606.07153,https://arxiv.org/abs/1606.07153,"b'Abstract:  Bayesian hierarchical models are increasing popular in economics. When using\nhierarchical models, it is useful not only to calculate posterior expectations,\nbut also to measure the robustness of these expectations to reasonable\nalternative prior choices. We use variational Bayes and linear response methods\nto provide fast, accurate posterior means and robustness measures with an\napplication to measuring the effectiveness of microcredit in the developing\nworld.'"
Tamara Broderick,Broderick_Tamara,arXiv:1605.06423,https://arxiv.org/abs/1605.06423,"b'Abstract:  The use of Bayesian methods in large-scale data settings is attractive\nbecause of the rich hierarchical models, uncertainty quantification, and prior\nspecification they provide. Standard Bayesian inference algorithms are\ncomputationally expensive, however, making their direct application to large\ndatasets difficult or infeasible. Recent work on scaling Bayesian inference has\nfocused on modifying the underlying algorithms to, for example, use only a\nrandom data subsample at each iteration. We leverage the insight that data is\noften redundant to instead obtain a weighted subset of the data (called a\ncoreset) that is much smaller than the original dataset. We can then use this\nsmall coreset in any number of existing posterior inference algorithms without\nmodification. In this paper, we develop an efficient coreset construction\nalgorithm for Bayesian logistic regression models. We provide theoretical\nguarantees on the size and approximation quality of the coreset -- both for\nfixed, known datasets, and in expectation for a wide class of data generative\nmodels. Crucially, the proposed approach also permits efficient construction of\nthe coreset in both streaming and parallel settings, with minimal additional\neffort. We demonstrate the efficacy of our approach on a number of synthetic\nand real-world datasets, and find that, in practice, the size of the coreset is\nindependent of the original dataset size. Furthermore, constructing the coreset\ntakes a negligible amount of time compared to that required to run MCMC on it.'"
Tamara Broderick,Broderick_Tamara,arXiv:1603.06915,https://arxiv.org/abs/1603.06915,"b'Abstract:  Network data appear in a number of applications, such as online social\nnetworks and biological networks, and there is growing interest in both\ndeveloping models for networks as well as studying the properties of such data.\nSince individual network datasets continue to grow in size, it is necessary to\ndevelop models that accurately represent the real-life scaling properties of\nnetworks. One behavior of interest is having a power law in the degree\ndistribution. However, other types of power laws that have been observed\nempirically and considered for applications such as clustering and feature\nallocation models have not been studied as frequently in models for graph data.\nIn this paper, we enumerate desirable asymptotic behavior that may be of\ninterest for modeling graph data, including sparsity and several types of power\nlaws. We outline a general framework for graph generative models using\ncompletely random measures; by contrast to the pioneering work of Caron and Fox\n(2015), we consider instantiating more of the existing atoms of the random\nmeasure as the dataset size increases rather than adding new atoms to the\nmeasure. We see that these two models can be complementary; they respectively\nyield interpretations as (1) time passing among existing members of a network\nand (2) new individuals joining a network. We detail a particular instance of\nthis framework and show simulated results that suggest this model exhibits some\ndesirable asymptotic power-law behavior.'"
Tamara Broderick,Broderick_Tamara,arXiv:1603.06898,https://arxiv.org/abs/1603.06898,"b'Abstract:  A known failing of many popular random graph models is that the Aldous-Hoover\nTheorem guarantees these graphs are dense with probability one; that is, the\nnumber of edges grows quadratically with the number of nodes. This behavior is\nconsidered unrealistic in observed graphs. We define a notion of edge\nexchangeability for random graphs in contrast to the established notion of\ninfinite exchangeability for random graphs --- which has traditionally relied\non exchangeability of nodes (rather than edges) in a graph. We show that,\nunlike node exchangeability, edge exchangeability encompasses models that are\nknown to provide a projective sequence of random graphs that circumvent the\nAldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the\nnumber of edges with the number of nodes. We show how edge-exchangeability of\ngraphs relates naturally to existing notions of exchangeability from clustering\n(a.k.a. partitions) and other familiar combinatorial structures.'"
Tamara Broderick,Broderick_Tamara,arXiv:1603.00861,https://arxiv.org/abs/1603.00861,"b'Abstract:  Completely random measures (CRMs) and their normalizations are a rich source\nof Bayesian nonparametric priors. Examples include the beta, gamma, and\nDirichlet processes. In this paper we detail two major classes of sequential\nCRM representations---series representations and superposition\nrepresentations---within which we organize both novel and existing sequential\nrepresentations that can be used for simulation and posterior inference. These\ntwo classes and their constituent representations subsume existing ones that\nhave previously been developed in an ad hoc manner for specific processes.\nSince a complete infinite-dimensional CRM cannot be used explicitly for\ncomputation, sequential representations are often truncated for tractability.\nWe provide truncation error analyses for each type of sequential\nrepresentation, as well as their normalized versions, thereby generalizing and\nimproving upon existing truncation error bounds in the literature. We analyze\nthe computational complexity of the sequential representations, which in\nconjunction with our error bounds allows us to directly compare representations\nand discuss their relative efficiency. We include numerous applications of our\ntheoretical results to commonly-used (normalized) CRMs, demonstrating that our\nresults enable a straightforward representation and analysis of CRMs that has\nnot previously been available in a Bayesian nonparametric context.'"
Tamara Broderick,Broderick_Tamara,arXiv:1512.02578,https://arxiv.org/abs/1512.02578,"b'Abstract:  In Bayesian analysis, the posterior follows from the data and a choice of a\nprior and a likelihood. One hopes that the posterior is robust to reasonable\nvariation in the choice of prior and likelihood, since this choice is made by\nthe modeler and is necessarily somewhat subjective. Despite the fundamental\nimportance of the problem and a considerable body of literature, the tools of\nrobust Bayes are not commonly used in practice. This is in large part due to\nthe difficulty of calculating robustness measures from MCMC draws. Although\nmethods for computing robustness measures from MCMC draws exist, they lack\ngenerality and often require additional coding or computation.\nIn contrast to MCMC, variational Bayes (VB) techniques are readily amenable\nto robustness analysis. The derivative of a posterior expectation with respect\nto a prior or data perturbation is a measure of local robustness to the prior\nor likelihood. Because VB casts posterior inference as an optimization problem,\nits methodology is built on the ability to calculate derivatives of posterior\nquantities with respect to model parameters, even in very complex models. In\nthe present work, we develop local prior robustness measures for mean-field\nvariational Bayes(MFVB), a VB technique which imposes a particular\nfactorization assumption on the variational posterior approximation. We start\nby outlining existing local prior measures of robustness. Next, we use these\nresults to derive closed-form measures of the sensitivity of mean-field\nvariational posterior approximation to prior specification. We demonstrate our\nmethod on a meta-analysis of randomized controlled interventions in access to\nmicrocredit in developing countries.'"
Tamara Broderick,Broderick_Tamara,arXiv:1512.01229,https://arxiv.org/abs/1512.01229,"b'Abstract:  This article is a translation of Bruno de Finetti\'s paper ""Funzione\nCaratteristica di un fenomeno aleatorio"" which appeared in Atti del Congresso\nInternazionale dei Matematici, Bologna 3-10 Settembre 1928, Tomo VI, pp.\n179-190, originally published by Nicola Zanichelli Editore S.p.A. The\ntranslation was made as close as possible to the original in form and style,\nexcept for apparent mistakes found in the original document, which were\ncorrected and are mentioned as footnotes. Most of these were resolved by\ncomparing against a longer version of this work by de Finetti, published\nshortly after this one under the same titlea. The interested reader is highly\nencouraged to consult this other version for a more detailed treatment of the\ntopics covered here. Footnotes regarding the translation are labeled with\nletters to distinguish them from de Finetti\'s original footnotes.'"
Tamara Broderick,Broderick_Tamara,arXiv:1506.04088,https://arxiv.org/abs/1506.04088,"b'Abstract:  Mean field variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is that it underestimates the uncertainty of\nmodel variables (sometimes severely) and provides no information about model\nvariable covariance.\nWe generalize linear response methods from statistical physics to deliver\naccurate uncertainty estimates for model variables---both for individual\nvariables and coherently across variables. We call our method linear response\nvariational Bayes (LRVB). When the MFVB posterior approximation is in the\nexponential family, LRVB has a simple, analytic form, even for non-conjugate\nmodels. Indeed, we make no assumptions about the form of the true posterior. We\ndemonstrate the accuracy and scalability of our method on a range of models for\nboth simulated and real data.'"
Tamara Broderick,Broderick_Tamara,arXiv:1502.07685,https://arxiv.org/abs/1502.07685,"b'Abstract:  Mean field variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is that it underestimates the uncertainty of\nmodel variables (sometimes severely) and provides no information about model\nvariable covariance. We develop a fast, general methodology for exponential\nfamilies that augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We also show how LRVB can be\nused to quickly calculate a measure of the influence of individual data points\non parameter point estimates. We demonstrate the accuracy and scalability of\nour method by learning Gaussian mixture models for both simulated and real\ndata.'"
Tamara Broderick,Broderick_Tamara,arXiv:1410.6853,https://arxiv.org/abs/1410.6853,"b'Abstract:  Mean Field Variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is its (sometimes severe) underestimates of\nthe uncertainty of model variables and lack of information about model variable\ncovariance. We develop a fast, general methodology for exponential families\nthat augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We demonstrate the accuracy of\nour method on simulated data sets.'"
Tamara Broderick,Broderick_Tamara,arXiv:1410.6843,https://arxiv.org/abs/1410.6843,"b'Abstract:  We demonstrate how to calculate posteriors for general CRM-based priors and\nlikelihoods for Bayesian nonparametric models. We further show how to represent\nBayesian nonparametric priors as a sequence of finite draws using a\nsize-biasing approach---and how to represent full Bayesian nonparametric models\nvia finite marginals. Motivated by conjugate priors based on exponential family\nrepresentations of likelihoods, we introduce a notion of exponential families\nfor CRMs, which we call exponential CRMs. This construction allows us to\nspecify automatic Bayesian nonparametric conjugate priors for exponential CRM\nlikelihoods. We demonstrate that our exponential CRMs allow particularly\nstraightforward recipes for size-biased and marginal representations of\nBayesian nonparametric models. Along the way, we prove that the gamma process\nis a conjugate prior for the Poisson likelihood process and the beta prime\nprocess is a conjugate prior for a process we call the odds Bernoulli process.\nWe deliver a size-biased representation of the gamma process and a marginal\nrepresentation of the gamma process coupled with a Poisson likelihood process.'"
Tamara Broderick,Broderick_Tamara,arXiv:1410.4792,https://arxiv.org/abs/1410.4792,"b'Abstract:  Bayesian entity resolution merges together multiple, noisy databases and\nreturns the minimal collection of unique individuals represented, together with\ntheir true, latent record values. Bayesian methods allow flexible generative\nmodels that share power across databases as well as principled quantification\nof uncertainty for queries of the final, resolved database. However, existing\nBayesian methods for entity resolution use Markov monte Carlo method (MCMC)\napproximations and are too slow to run on modern databases containing millions\nor billions of records. Instead, we propose applying variational approximations\nto allow scalable Bayesian inference in these models. We derive a\ncoordinate-ascent approximation for mean-field variational Bayes, qualitatively\ncompare our algorithm to existing methods, note unique challenges for inference\nthat arise from the expected distribution of cluster sizes in entity\nresolution, and discuss directions for future work in this domain.'"
Tamara Broderick,Broderick_Tamara,arXiv:1307.8049,https://arxiv.org/abs/1307.8049,"b'Abstract:  Research on distributed machine learning algorithms has focused primarily on\none of two extremes - algorithms that obey strict concurrency constraints or\nalgorithms that obey few or no such constraints. We consider an intermediate\nalternative in which algorithms optimistically assume that conflicts are\nunlikely and if conflicts do arise a conflict-resolution protocol is invoked.\nWe view this ""optimistic concurrency control"" paradigm as particularly\nappropriate for large-scale machine learning algorithms, particularly in the\nunsupervised setting. We demonstrate our approach in three problem areas:\nclustering, feature learning and online facility location. We evaluate our\nmethods via large-scale experiments in a cluster computing environment.'"
Tamara Broderick,Broderick_Tamara,arXiv:1307.6769,https://arxiv.org/abs/1307.6769,"b'Abstract:  We present SDA-Bayes, a framework for (S)treaming, (D)istributed,\n(A)synchronous computation of a Bayesian posterior. The framework makes\nstreaming updates to the estimated posterior according to a user-specified\napproximation batch primitive. We demonstrate the usefulness of our framework,\nwith variational Bayes (VB) as the primitive, by fitting the latent Dirichlet\nallocation model to two large-scale document collections. We demonstrate the\nadvantages of our algorithm over stochastic variational inference (SVI) by\ncomparing the two after a single pass through a known amount of data---a case\nwhere SVI may be applied---and in the streaming setting, where SVI does not\napply.'"
Tamara Broderick,Broderick_Tamara,arXiv:1301.6647,https://arxiv.org/abs/1301.6647,"b'Abstract:  The problem of inferring a clustering of a data set has been the subject of\nmuch research in Bayesian analysis, and there currently exists a solid\nmathematical foundation for Bayesian approaches to clustering. In particular,\nthe class of probability distributions over partitions of a data set has been\ncharacterized in a number of ways, including via exchangeable partition\nprobability functions (EPPFs) and the Kingman paintbox. Here, we develop a\ngeneralization of the clustering problem, called feature allocation, where we\nallow each data point to belong to an arbitrary, non-negative integer number of\ngroups, now called features or topics. We define and study an ""exchangeable\nfeature probability function"" (EFPF)---analogous to the EPPF in the clustering\nsetting---for certain types of feature models. Moreover, we introduce a\n""feature paintbox"" characterization---analogous to the Kingman paintbox for\nclustering---of the class of exchangeable feature models. We provide a further\ncharacterization of the subclass of feature allocations that have EFPF\nrepresentations.'"
Tamara Broderick,Broderick_Tamara,arXiv:1212.2126,https://arxiv.org/abs/1212.2126,"b'Abstract:  The classical mixture of Gaussians model is related to K-means via\nsmall-variance asymptotics: as the covariances of the Gaussians tend to zero,\nthe negative log-likelihood of the mixture of Gaussians model approaches the\nK-means objective, and the EM algorithm approaches the K-means algorithm. Kulis\n& Jordan (2012) used this observation to obtain a novel K-means-like algorithm\nfrom a Gibbs sampler for the Dirichlet process (DP) mixture. We instead\nconsider applying small-variance asymptotics directly to the posterior in\nBayesian nonparametric models. This framework is independent of any specific\nBayesian inference algorithm, and it has the major advantage that it\ngeneralizes immediately to a range of models beyond the DP mixture. To\nillustrate, we apply our framework to the feature learning setting, where the\nbeta process and Indian buffet process provide an appropriate Bayesian\nnonparametric prior. We obtain a novel objective function that goes beyond\nclustering to learn (and penalize new) groupings for which we relax the mutual\nexclusivity and exhaustivity assumptions of clustering. We demonstrate several\nother algorithms, all of which are scalable and simple to implement. Empirical\nresults demonstrate the benefits of the new framework.'"
Tamara Broderick,Broderick_Tamara,arXiv:1209.3550,https://arxiv.org/abs/1209.3550,"b'Abstract:  We develop algorithms for performing semiparametric regression analysis in\nreal time, with data processed as it is collected and made immediately\navailable via modern telecommunications technologies. Our definition of\nsemiparametric regression is quite broad and includes, as special cases,\ngeneralized linear mixed models, generalized additive models, geostatistical\nmodels, wavelet nonparametric regression models and their various combinations.\nFast updating of regression fits is achieved by couching semiparametric\nregression into a Bayesian hierarchical model or, equivalently, graphical model\nframework and employing online mean field variational ideas. An internet site\nattached to this article, realtime-semiparametric-regression.net, illustrates\nthe methodology for continually arriving stock market, real estate and airline\ndata. Flexible real-time analyses, based on increasingly ubiquitous streaming\ndata sources stand to benefit.'"
Tamara Broderick,Broderick_Tamara,arXiv:1206.5862,https://arxiv.org/abs/1206.5862,"b'Abstract:  One of the focal points of the modern literature on Bayesian nonparametrics\nhas been the problem of clustering, or partitioning, where each data point is\nmodeled as being associated with one and only one of some collection of groups\ncalled clusters or partition blocks. Underlying these Bayesian nonparametric\nmodels are a set of interrelated stochastic processes, most notably the\nDirichlet process and the Chinese restaurant process. In this paper we provide\na formal development of an analogous problem, called feature modeling, for\nassociating data points with arbitrary nonnegative integer numbers of groups,\nnow called features or topics. We review the existing combinatorial stochastic\nprocess representations for the clustering problem and develop analogous\nrepresentations for the feature modeling problem. These representations include\nthe beta process and the Indian buffet process as well as new representations\nthat provide insight into the connections between these processes. We thereby\nbring the same level of completeness to the treatment of Bayesian nonparametric\nfeature modeling that has previously been achieved for Bayesian nonparametric\nclustering.'"
Tamara Broderick,Broderick_Tamara,arXiv:1203.3486,https://arxiv.org/abs/1203.3486,"b'Abstract:  We introduce a new graphical model for tracking radio-tagged animals and\nlearning their movement patterns. The model provides a principled way to\ncombine radio telemetry data with an arbitrary set of userdefined, spatial\nfeatures. We describe an efficient stochastic gradient algorithm for fitting\nmodel parameters to data and demonstrate its effectiveness via asymptotic\nanalysis and synthetic experiments. We also apply our model to real datasets,\nand show that it outperforms the most popular radio telemetry software package\nused in ecology. We conclude that integration of different data sources under a\nsingle statistical framework, coupled with appropriate parameter and state\nestimation procedures, produces both accurate location estimates and an\ninterpretable statistical model of animal movement.'"
Tamara Broderick,Broderick_Tamara,arXiv:1112.3654,https://arxiv.org/abs/1112.3654,"b'Abstract:  As the number of observed Gamma-Ray Bursts (GRBs) continues to grow,\nfollow-up resources need to be used more efficiently in order to maximize\nscience output from limited telescope time. As such, it is becoming\nincreasingly important to rapidly identify bursts of interest as soon as\npossible after the event, before the afterglows fade beyond detectability.\nStudying the most distant (highest redshift) events, for instance, remains a\nprimary goal for many in the field. Here we present our Random forest Automated\nTriage Estimator for GRB redshifts (RATE GRB-z) for rapid identification of\nhigh-redshift candidates using early-time metrics from the three telescopes\nonboard Swift. While the basic RATE methodology is generalizable to a number of\nresource allocation problems, here we demonstrate its utility for\ntelescope-constrained follow-up efforts with the primary goal to identify and\nstudy high-z GRBs. For each new GRB, RATE GRB-z provides a recommendation -\nbased on the available telescope time - of whether the event warrants\nadditional follow-up resources. We train RATE GRB-z using a set consisting of\n135 Swift bursts with known redshifts, only 18 of which are z > 4.\nCross-validated performance metrics on this training data suggest that ~56% of\nhigh-z bursts can be captured from following up the top 20% of the ranked\ncandidates, and ~84% of high-z bursts are identified after following up the top\n~40% of candidates. We further use the method to rank 200+ Swift bursts with\nunknown redshifts according to their likelihood of being high-z.'"
Tamara Broderick,Broderick_Tamara,arXiv:1111.1802,https://arxiv.org/abs/1111.1802,"b'Abstract:  We develop a Bayesian nonparametric approach to a general family of latent\nclass problems in which individuals can belong simultaneously to multiple\nclasses and where each class can be exhibited multiple times by an individual.\nWe introduce a combinatorial stochastic process known as the negative binomial\nprocess (NBP) as an infinite-dimensional prior appropriate for such problems.\nWe show that the NBP is conjugate to the beta process, and we characterize the\nposterior distribution under the beta-negative binomial process (BNBP) and\nhierarchical models based on the BNBP (the HBNBP). We study the asymptotic\nproperties of the BNBP and develop a three-parameter extension of the BNBP that\nexhibits power-law behavior. We derive MCMC algorithms for posterior inference\nunder the HBNBP, and we present experiments using these algorithms in the\ndomains of image segmentation, object recognition, and document analysis.'"
Tamara Broderick,Broderick_Tamara,arXiv:1106.0539,https://arxiv.org/abs/1106.0539,"b'Abstract:  The beta-Bernoulli process provides a Bayesian nonparametric prior for models\ninvolving collections of binary-valued features. A draw from the beta process\nyields an infinite collection of probabilities in the unit interval, and a draw\nfrom the Bernoulli process turns these into binary-valued features. Recent work\nhas provided stick-breaking representations for the beta process analogous to\nthe well-known stick-breaking representation for the Dirichlet process. We\nderive one such stick-breaking representation directly from the\ncharacterization of the beta process as a completely random measure. This\napproach motivates a three-parameter generalization of the beta process, and we\nstudy the power laws that can be obtained from this generalized beta process.\nWe present a posterior inference algorithm for the beta-Bernoulli process that\nexploits the stick-breaking representation, and we present experimental results\nfor a discrete factor-analysis model.'"
Tamara Broderick,Broderick_Tamara,arXiv:0909.2450,https://arxiv.org/abs/0909.2450,"b'Abstract:  Selection methods that require only a single-switch input, such as a button\nclick or blink, are potentially useful for individuals with motor impairments,\nmobile technology users, and individuals wishing to transmit information\nsecurely. We present a single-switch selection method, ""Nomon,"" that is general\nand efficient. Existing single-switch selection methods require selectable\noptions to be arranged in ways that limit potential applications. By contrast,\ntraditional operating systems, web browsers, and free-form applications (such\nas drawing) place options at arbitrary points on the screen. Nomon, however,\nhas the flexibility to select any point on a screen. Nomon adapts automatically\nto an individual\'s clicking ability; it allows a person who clicks precisely to\nmake a selection quickly and allows a person who clicks imprecisely more time\nto make a selection without error. Nomon reaps gains in information rate by\nallowing the specification of beliefs (priors) about option selection\nprobabilities and by avoiding tree-based selection schemes in favor of direct\n(posterior) inference. We have developed both a Nomon-based writing application\nand a drawing application. To evaluate Nomon\'s performance, we compared the\nwriting application with a popular existing method for single-switch writing\n(row-column scanning). Novice users wrote 35% faster with the Nomon interface\nthan with the scanning interface. An experienced user (author TB, with > 10\nhours practice) wrote at speeds of 9.3 words per minute with Nomon, using 1.2\nclicks per character and making no errors in the final text.'"
Tamara Broderick,Broderick_Tamara,arXiv:0904.4891,https://arxiv.org/abs/0904.4891,"b'Abstract:  Recognizing the successes of treed Gaussian process (TGP) models as an\ninterpretable and thrifty model for nonparametric regression, we seek to extend\nthe model to classification. Both treed models and Gaussian processes (GPs)\nhave, separately, enjoyed great success in application to classification\nproblems. An example of the former is Bayesian CART. In the latter, real-valued\nGP output may be utilized for classification via latent variables, which\nprovide classification rules by means of a softmax function. We formulate a\nBayesian model averaging scheme to combine these two models and describe a\nMonte Carlo method for sampling from the full posterior distribution with joint\nproposals for the tree topology and the GP parameters corresponding to latent\nvariables at the leaves. We concentrate on efficient sampling of the latent\nvariables, which is important to obtain good mixing in the expanded parameter\nspace. The tree structure is particularly helpful for this task and also for\ndeveloping an efficient scheme for handling categorical predictors, which\ncommonly arise in classification problems. Our proposed classification TGP\n(CTGP) methodology is illustrated on a collection of synthetic and real data\nsets. We assess performance relative to existing methods and thereby show how\nCTGP is highly flexible, offers tractable inference, produces rules that are\neasy to interpret, and performs well out of sample.'"
Tamara Broderick,Broderick_Tamara,arXiv:0712.2437,https://arxiv.org/abs/0712.2437,"b'Abstract:  Recent work has shown that probabilistic models based on pairwise\ninteractions-in the simplest case, the Ising model-provide surprisingly\naccurate descriptions of experiments on real biological networks ranging from\nneurons to genes. Finding these models requires us to solve an inverse problem:\ngiven experimentally measured expectation values, what are the parameters of\nthe underlying Hamiltonian? This problem sits at the intersection of\nstatistical physics and machine learning, and we suggest that more efficient\nsolutions are possible by merging ideas from the two fields. We use a\ncombination of recent coordinate descent algorithms with an adaptation of the\nhistogram Monte Carlo method, and implement these techniques to take advantage\nof the sparseness found in data on real neurons. The resulting algorithm learns\nthe parameters of an Ising model describing a network of forty neurons within a\nfew minutes. This opens the possibility of analyzing much larger data sets now\nemerging, and thus testing hypotheses about the collective behaviors of these\nnetworks.'"
Tamara Broderick,Broderick_Tamara,arXiv:astro-ph/0507108,https://arxiv.org/abs/astro-ph/0507108,"b'Abstract:  We present the results of attempts to detect the ellipticity of dark matter\nhalos using galaxy-galaxy weak lensing with SDSS data. We use 2,020,256\ngalaxies brighter than r=19 with photometric redshifts (divided into colour and\nluminosity subsamples) as lenses and 31,697,869 source galaxies. We search for\nand identify several signal contaminants, which if not removed lead to a\nspurious detection. These include systematic shear that leads to a slight\nspurious alignment of lens and source ellipticities, intrinsic alignments (due\nto contamination of the source sample by physically-associated lens source\npairs), and anisotropic magnification bias. We develop methods that allow us to\nremove these contaminants to the signal. We split the analysis into blue\n(spiral) and red (elliptical) galaxies. Assuming Gaussian errors as in previous\nwork and a power-law profile, we find f_h=e_h/e_g=0.1+/-0.06 for red galaxies\nand -0.8+/-0.4 for blue galaxies using 20-300 kpc/h, averaged over luminosity.\nInclusion of the more realistic non-Gaussian error distributions and of the NFW\ndensity profile (which predicts much smaller ellipticity of the shear for\nscales above the scale radius) yields 0.60+/-0.38 for ellipticals and\n-1.4+1.7-2.0 for spirals. While there is no concrete detection of alignment in\neither case, there is a suggestion in the data of a positive alignment in the\nbrightest lens sample of ellipticals. Our results appear to be mildly\ninconsistent with a previously reported detection by Hoekstra et al. (2004),\nbut more data and further tests are needed to clarify whether the discrepancy\nis real or a consequence of differences in the lens galaxy samples used and\nanalysis methods.'"
Tamara Broderick,Broderick_Tamara,arXiv:astro-ph/0402002,https://arxiv.org/abs/astro-ph/0402002,"b'Abstract:  We investigate the required redshift accuracy of type Ia supernova and\ncluster number-count surveys in order for the redshift uncertainties not to\ncontribute appreciably to the dark energy parameter error budget. For the SNAP\nsupernova experiment, we find that, without the assistance of ground-based\nmeasurements, individual supernova redshifts would need to be determined to\nabout 0.002 or better, which is a challenging but feasible requirement for a\nlow-resolution spectrograph. However, we find that accurate redshifts for z<0.1\nsupernovae, obtained with ground-based experiments, are sufficient to immunize\nthe results against even relatively large redshift errors at high z. For the\nfuture cluster number-count surveys such as the South Pole Telescope, Planck or\nDUET, we find that the purely statistical error in photometric redshift is less\nimportant, and that the irreducible, systematic bias in redshift drives the\nrequirements. The redshift bias will have to be kept below 0.001-0.005 per\nredshift bin (which is determined by the filter set), depending on the sky\ncoverage and details of the definition of the minimal mass of the survey.\nFurthermore, we find that X-ray surveys have a more stringent required redshift\naccuracy than Sunyaev-Zeldovich (SZ) effect surveys since they use a shorter\nlever arm in redshift; conversely, SZ surveys benefit from their high redshift\nreach only so long as some redshift information is available for distant (z>1)\nclusters.'"
Rodney Brooks,Brooks_Rodney,arXiv:1710.10291,https://arxiv.org/abs/1710.10291,"b'Abstract:  The measurement problem and three other vexing experiments in quantum physics\nare described. It is shown how Quantum Field Theory, as formulated by Julian\nSchwinger, provides simple solutions for all four experiments. It is also shown\nhow this theory resolves many other problems of Quantum Mechanics and\nRelativity, including a new and simple derivation of E = mc2.'"
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1902.05973,https://arxiv.org/abs/1902.05973,"b'Abstract:  Lead halide-based perovskite thin films have attracted great attention due to\nthe explosive increase in perovskite solar cell efficiencies. The same\noptoelectronic properties that make perovskites ideal absorber materials in\nsolar cells are also beneficial in other light-harvesting applications and make\nthem prime candidates as triplet sensitizers in upconversion via\ntriplet-triplet annihilation in rubrene. In this contribution, we take\nadvantage of long carrier lifetimes and carrier diffusion lengths in perovskite\nthin films, their high absorption cross sections throughout the visible\nspectrum, as well as the strong spin-orbit coupling owing to the abundance of\nheavy atoms to sensitize the upconverter rubrene. Employing bulk perovskite\nthin films as the absorber layer and spin-mixer in inorganic/organic\nheterojunction upconversion devices allows us to forego the additional\ntunneling barrier owing from the passivating ligands required for colloidal\nsensitizers. Our bilayer device exhibits an upconversion efficiency in excess\nof 3% under 785 nm illumination.'"
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1901.08637,https://arxiv.org/abs/1901.08637,"b'Abstract:  Photon recycling is required for a solar cell to achieve an open-circuit\nvoltage ($V_{OC}$) and power conversion efficiency (PCE) approaching the\nShockley-Queisser theoretical limit. In metal halide perovskite solar cells,\nthe achievable performance gains from photon recycling remain uncertain due to\nhigh variability in perovskite material quality and the non-radiative\nrecombination rate ($k_{1}$). In this work, we study state-of-the-art\n$\\textrm{Cs}_{0.05}(\\textrm{MA}_{0.17}\\textrm{FA}_{0.83})_{0.95}\\textrm{Pb}(\\textrm{I}_{0.83}\\textrm{Br}_{0.17})_{3}$\nfilms and analyze the impact of varying non-radiative recombination rates on\nphoton recycling and device performance. Importantly, we predict the impact of\nphoton recycling at the maximum power point (MPP), demonstrating an absolute\nPCE increase of up to 2.0% in the radiative limit, primarily due to a 77 mV\nincrease in $V_{MPP}$. Even with finite non-radiative recombination, benefits\nfrom photon recycling can be achieved when non-radiative lifetimes and external\nLED electroluminescence efficiencies measured at open-circuit,\n$Q_{e}^{LED}(\\textrm{V}_{OC})$, exceed 2 $\\mu$s and 10%, respectively. This\nanalysis clarifies the opportunity to fully exploit photon recycling to push\nthe real-world performance of perovskite solar cells toward theoretical limits.'"
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1803.01192,https://arxiv.org/abs/1803.01192,"b'Abstract:  Halide perovskites are promising semiconductors for inexpensive,\nhigh-performance optoelectronics. Despite a remarkable defect tolerance\ncompared to conventional semiconductors, perovskite thin films still show\nsubstantial microscale heterogeneity in key properties such as luminescence\nefficiency and device performance. This behavior has been attributed to spatial\nfluctuations in the population of sub-bandgap electronic states that act as\ntrap-mediated non-radiative recombination sites. However, the origin of the\nvariations, trap states and extent of the defect tolerance remains a topic of\ndebate, and a precise understanding is critical to the rational design of\ndefect management strategies. By combining scanning X-ray diffraction beamlines\nat two different synchrotrons with high-resolution transmission electron\nmicroscopy, we reveal levels of heterogeneity on the ten-micrometer scale\n(super-grains) and even ten-nanometer scale (sub-grain domains). We find that\nlocal strain is associated with enhanced defect concentrations, and\ncorrelations between the local structure and time-resolved photoluminescence\nreveal that these strain-related defects are the cause of non-radiative\nrecombination. We reveal a direct connection between defect concentrations and\nnon-radiative losses, as well as complex heterogeneity across multiple length\nscales, shedding new light on the presence and influence of structural defects\nin halide perovskites.'"
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1609.04643,https://arxiv.org/abs/1609.04643,"b'Abstract:  Unique optical properties of colloidal semiconductor quantum dots (QDs),\narising from quantum mechanical confinement of charge within these structures,\npresent a versatile testbed for the study of how high electric fields affect\nthe electronic structure of nanostructured solids. Earlier studies of quasi-DC\nelectric field modulation of QD properties have been limited by the\nelectrostatic breakdown processes under the high externally applied electric\nfields, which have restricted the range of modulation of QD properties. In\ncontrast, in the present work we drive CdSe:CdS core:shell QD films with\nhigh-field THz-frequency electromagnetic pulses whose duration is only a few\npicoseconds. Surprisingly, in response to the THz excitation we observe QD\nluminescence even in the absence of an external charge source. Our experiments\nshow that QD luminescence is associated with a remarkably high and rapid\nmodulation of the QD band-gap, which is changing by more than 0.5 eV\n(corresponding to 25% of the unperturbed bandgap energy) within the picosecond\ntimeframe of THz field profile. We show that these colossal energy shifts can\nbe consistently explained by the quantum confined Stark effect. Our work\ndemonstrates a route to extreme modulation of material properties without\nconfigurational changes in material sets or geometries. Additionally, we expect\nthat this platform can be adapted to a novel compact THz detection scheme where\nconversion of THz fields (with meV-scale photon energies) to the\nvisible/near-IR band (with eV-scale photon energies) can be achieved at room\ntemperature with high bandwidth and sensitivity.'"
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1509.03687,https://arxiv.org/abs/1509.03687,b'Abstract:  Plexcitons are polaritonic modes that result from the strong coupling between\nexcitons and plasmons. We consider plexcitons emerging from the interaction of\nexcitons in an organic molecular layer with surface plasmons in a metallic\nfilm. We predict the emergence of Dirac cones in the two-dimensional\nbandstructure of plexcitons due to the inherent alignment of the excitonic\ntransitions in the organic layer. These Dirac cones may open up in energy by\nsimultaneously interfacing the metal with a magneto-optical layer and\nsubjecting the whole system to a perpendicular magnetic field. The resulting\nenergy gap becomes populated with topologically protected one-way modes which\ntravel at the interface of this plexcitonic system. Our theoretical proposal\nsuggests that plexcitons are a convenient and simple platform for the\nexploration of exotic phases of matter as well as of novel ways to direct\nenergy flow at the nanoscale.'
Michael Carbin,Carbin_Michael,arXiv:1903.01611,https://arxiv.org/abs/1903.01611,"b'Abstract:  Recent work on the ""lottery ticket hypothesis"" proposes that\nrandomly-initialized, dense neural networks contain much smaller, fortuitously\ninitialized subnetworks (""winning tickets"") capable of training to similar\naccuracy as the original network at a similar speed. While strong evidence\nexists for the hypothesis across many settings, it has not yet been evaluated\non large, state-of-the-art networks and there is even evidence against the\nhypothesis on deeper networks.\nWe modify the lottery ticket pruning procedure to make it possible to\nidentify winning tickets on deeper networks. Rather than set the weights of a\nwinning ticket to their original initializations, we set them to the weights\nobtained after a small number of training iterations (""late resetting""). Using\nlate resetting, we identify the first winning tickets for Resnet-50 on Imagenet\nTo understand the efficacy of late resetting, we study the ""stability"" of\nneural network training to pruning, which we define as the consistency of the\noptimization trajectories followed by a winning ticket when it is trained in\nisolation and as part of the larger network. We find that later resetting\nproduces stabler winning tickets and that improved stability correlates with\nhigher winning ticket accuracy. This analysis offers new insights into the\nlottery ticket hypothesis and the dynamics of neural network learning.'"
Michael Carbin,Carbin_Michael,arXiv:1809.05859,https://arxiv.org/abs/1809.05859,"b'Abstract:  When a computational task tolerates a relaxation of its specification or when\nan algorithm tolerates the effects of noise in its execution, hardware,\nprogramming languages, and system software can trade deviations from correct\nbehavior for lower resource usage. We present, for the first time, a synthesis\nof research results on computing systems that only make as many errors as their\nusers can tolerate, from across the disciplines of computer aided design of\ncircuits, digital system design, computer architecture, programming languages,\noperating systems, and information theory.\nRather than over-provisioning resources at each layer to avoid errors, it can\nbe more efficient to exploit the masking of errors occurring at one layer which\ncan prevent them from propagating to a higher layer. We survey tradeoffs for\nindividual layers of computing systems from the circuit level to the operating\nsystem level and illustrate the potential benefits of end-to-end approaches\nusing two illustrative examples. To tie together the survey, we present a\nconsistent formalization of terminology, across the layers, which does not\nsignificantly deviate from the terminology traditionally used by research\ncommunities in their layer of focus.'"
Michael Carbin,Carbin_Michael,arXiv:1808.07412,https://arxiv.org/abs/1808.07412,"b""Abstract:  Statically estimating the number of processor clock cycles it takes to\nexecute a basic block of assembly instructions in steady state (throughput) is\nimportant for compiler backend optimizations such as register allocation,\ninstruction selection and instruction scheduling. This is complicated specially\nin modern x86-64 Complex Instruction Set Computer (CISC) machines with\nsophisticated processor microarchitectures. Traditionally, compiler writers\ninvest time experimenting and referring to processor manuals to analytically\nmodel modern processors with incomplete specifications. This is tedious, error\nprone and should be done for each processor generation. We present Ithemal, the\nfirst automatically learnt estimator to statically predict throughput of a set\nof basic block instructions using machine learning. Ithemal uses a novel\nDirected Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven\napproach for throughput estimation. We show that Ithemal is accurate than\nstate-of-the-art hand written tools used in compiler backends and static\nmachine code analyzers. In particular, our model has a worst case average error\nof 10.53% on actual throughput values when compared to best case average errors\nof 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's\nmachine code analyzer when compared on three different microarchitectures,\nwhile predicting throughput values at a faster rate than aforementioned tools.\nWe also show that Ithemal is portable, learning throughput estimation for Intel\nNehalem, Haswell and Skylake microarchitectures without requiring changes to\nits structure."""
Michael Carbin,Carbin_Michael,arXiv:1805.06090,https://arxiv.org/abs/1805.06090,"b""Abstract:  Researchers have recently designed a number of application-specific fault\ntolerance mechanisms that enable applications to either be naturally resilient\nto errors or include additional detection and correction steps that can bring\nthe overall execution of an application back into an envelope for which an\nacceptable execution is eventually guaranteed. A major challenge to building an\napplication that leverages these mechanisms, however, is to verify that the\nimplementation satisfies the basic invariants that these mechanisms\nrequire--given a model of how faults may manifest during the application's\nexecution.\nTo this end we present Leto, an SMT based automatic verification system that\nenables developers to verify their applications with respect to a first-class\nexecution model specification. Namely, Leto enables software and platform\ndevelopers to programmatically specify the execution semantics of the\nunderlying hardware system as well as verify assertions about the behavior of\nthe application's resulting execution. In this paper, we present the Leto\nprogramming language and its corresponding verification system. We also\ndemonstrate Leto on several applications that leverage application-specific\nfault tolerance mechanisms."""
Michael Carbin,Carbin_Michael,arXiv:1805.01863,https://arxiv.org/abs/1805.01863,"b""Abstract:  Researchers have recently proposed several systems that ease the process of\nperforming Bayesian probabilistic inference. These include systems for\nautomatic inference algorithm synthesis as well as stronger abstractions for\nmanual algorithm development. However, existing systems whose performance\nrelies on the developer manually constructing a part of the inference algorithm\nhave limited support for reasoning about the correctness of the resulting\nalgorithm.\nIn this paper, we present Shuffle, a programming language for manually\ndeveloping inference procedures that 1) enforces the basic rules of probability\ntheory, 2) enforces the statistical dependencies of the algorithm's\ncorresponding probabilistic model, and 3) generates an optimized\nimplementation. We have used Shuffle to develop inference algorithms for\nseveral standard probabilistic models. Our results demonstrate that Shuffle\nenables a developer to deliver correct and performant implementations of these\nalgorithms."""
Michael Carbin,Carbin_Michael,arXiv:1805.00468,https://arxiv.org/abs/1805.00468,"b'Abstract:  Though many safety-critical software systems use floating point to represent\nreal-world input and output, programmers usually have idealized versions in\nmind that compute with real numbers. Significant deviations from the ideal can\ncause errors and jeopardize safety. Some programming systems implement exact\nreal arithmetic, which resolves this matter but complicates others, such as\ndecision making. In these systems, it is impossible to compute (total and\ndeterministic) discrete decisions based on connected spaces such as\n$\\mathbb{R}$. We present programming-language semantics based on constructive\ntopology with variants allowing nondeterminism and/or partiality. Either\nnondeterminism or partiality suffices to allow computable decision making on\nconnected spaces such as $\\mathbb{R}$. We then introduce pattern matching on\nspaces, a language construct for creating programs on spaces, generalizing\npattern matching in functional programming, where patterns need not represent\ndecidable predicates and also may overlap or be inexhaustive, giving rise to\nnondeterminism or partiality, respectively. Nondeterminism and/or partiality\nalso yield formal logics for constructing approximate decision procedures. We\nimplemented these constructs in the Marshall language for exact real\narithmetic.'"
Michael Carbin,Carbin_Michael,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"b'Abstract:  In this position paper, we describe our vision of the future of machine\nprogramming through a categorical examination of three pillars of research.\nThose pillars are: (i) intention, (ii) invention, and(iii) adaptation.\nIntention emphasizes advancements in the human-to-computer and\ncomputer-to-machine-learning interfaces. Invention emphasizes the creation or\nrefinement of algorithms or core hardware and software building blocks through\nmachine learning (ML). Adaptation emphasizes advances in the use of ML-based\nconstructs to autonomously evolve software.'"
Michael Carbin,Carbin_Michael,arXiv:1803.03635,https://arxiv.org/abs/1803.03635,"b'Abstract:  Neural network pruning techniques can reduce the parameter counts of trained\nnetworks by over 90%, decreasing storage requirements and improving\ncomputational performance of inference without compromising accuracy. However,\ncontemporary experience is that the sparse architectures produced by pruning\nare difficult to train from the start, which would similarly improve training\nperformance.\nWe find that a standard pruning technique naturally uncovers subnetworks\nwhose initializations made them capable of training effectively. Based on these\nresults, we articulate the ""lottery ticket hypothesis:"" dense,\nrandomly-initialized, feed-forward networks contain subnetworks (""winning\ntickets"") that - when trained in isolation - reach test accuracy comparable to\nthe original network in a similar number of iterations. The winning tickets we\nfind have won the initialization lottery: their connections have initial\nweights that make training particularly effective.\nWe present an algorithm to identify winning tickets and a series of\nexperiments that support the lottery ticket hypothesis and the importance of\nthese fortuitous initializations. We consistently find winning tickets that are\nless than 10-20% of the size of several fully-connected and convolutional\nfeed-forward architectures for MNIST and CIFAR10. Above this size, the winning\ntickets that we find learn faster than the original network and reach higher\ntest accuracy.'"
Michael Carbin,Carbin_Michael,arXiv:1202.0359,https://arxiv.org/abs/1202.0359,"b'Abstract:  We propose a novel approach to improving software security called\nCryptographic Path Hardening, which is aimed at hiding security vulnerabilities\nin software from attackers through the use of provably secure and obfuscated\ncryptographic devices to harden paths in programs.\nBy ""harden"" we mean that certain error-checking if-conditionals in a given\nprogram P are replaced by equivalent"" we mean that adversaries cannot use\nsemi-automatic program analysis techniques to reason about the hardened program\npaths and thus cannot discover as-yet-unknown errors along those paths, except\nperhaps through black-box dictionary attacks or random testing (which we can\nnever prevent).\nOther than these unpreventable attack methods, we can make program analysis\naimed at error-finding ""provably hard"" for a resource-bounded attacker, in the\nsame sense that cryptographic schemes are hard to break. Unlike\nsecurity-through-obscurity, in Cryptographic Path Hardening we use\nprovably-secure crypto devices to hide errors and our mathematical arguments of\nsecurity are the same as the standard ones used in cryptography.\nOne application of Cryptographic Path Hardening is that software patches or\nfilters often reveal enough information to an attacker that they can be used to\nconstruct error-revealing inputs to exploit an unpatched version of the\nprogram. By ""hardening"" the patch we make it difficult for the attacker to\nanalyze the patched program to construct error-revealing inputs, and thus\nprevent him from potentially constructing exploits.'"
Vincent Chan,Chan_Vincent,arXiv:1307.1174,https://arxiv.org/abs/1307.1174,"b'Abstract:  Let $E \\subseteq R^n$ be a closed set of Hausdorff dimension $\\alpha$. For $m\n\\geq n$, let $\\{B_1,\\ldots,B_k\\}$ be $n \\times (m-n)$ matrices. We prove that\nif the system of matrices $B_j$ is non-degenerate in a suitable sense, $\\alpha$\nis sufficiently close to $n$, and if $E$ supports a probability measure obeying\nappropriate dimensionality and Fourier decay conditions, then for a range of\n$m$ depending on $n$ and $k$, the set $E$ contains a translate of a non-trivial\n$k$-point configuration $\\{B_1y,\\ldots,B_ky\\}$. As a consequence, we are able\nto establish existence of certain geometric configurations in Salem sets (such\nas parallelograms in $ R^n$ and isosceles right triangles in $R^2$). This can\nbe viewed as a multidimensional analogue of an earlier result of Laba and\nPramanik on 3-term arithmetic progressions in subsets of $R$.'"
Vincent Chan,Chan_Vincent,arXiv:1107.1000,https://arxiv.org/abs/1107.1000,"b'Abstract:  We study the spindown of isolated neutron stars from initially rapid rotation\nrates, driven by two factors: (i) gravitational wave emission due to r-modes\nand (ii) magnetic braking. In the context of isolated neutron stars, we present\nthe first study including self-consistently the magnetic damping of r-modes in\nthe spin evolution. We track the spin evolution employing the RNS code, which\naccounts for the rotating structure of neutron stars for various equations of\nstate. We find that, despite the strong damping due to the magnetic field,\nr-modes alter the braking rate from pure magnetic braking for B<10^{13}G. For\nrealistic values of the saturation amplitude, the r-mode can also decrease the\ntime to reach the threshold central density for quark deconfinement. Within a\nphenomenological model, we assess the gravitational waveform that would result\nfrom r-mode driven spindown of a magnetized neutron star. To contrast with the\npersistent signal during the spindown phase, we also present a preliminary\nestimate of the transient gravitational wave signal from an explosive\nquark-hadron phase transition, which can be a signal for the deconfinement of\nquarks inside neutron stars.'"
Anantha Chandrakasan,Chandrakasan_Anantha,arXiv:1903.04570,https://arxiv.org/abs/1903.04570,"b'Abstract:  This paper presents a configurable lattice cryptography processor which\nenables quantum-resistant security protocols for IoT. Efficient sampling\narchitectures, coupled with a low-power SHA-3 core, provide two orders of\nmagnitude energy savings over software. A single-port RAM-based NTT\narchitecture is proposed, which provides ~124k-gate area savings. This is the\nfirst ASIC implementation which demonstrates multiple lattice-based protocols\nproposed for NIST post-quantum standardization.'"
Anantha Chandrakasan,Chandrakasan_Anantha,arXiv:1903.04387,https://arxiv.org/abs/1903.04387,b'Abstract:  This paper presents a reconfigurable cryptographic engine that implements the\nDTLS protocol to enable end-to-end security for IoT. This implementation of the\nDTLS engine demonstrates 10x reduction in code size and 438x improvement in\nenergy-efficiency over software. Our ECC primitive is 237x and 9x more\nenergy-efficient compared to software and state-of-the-art hardware\nrespectively. Pairing the DTLS engine with an on-chip RISC-V allows us to\ndemonstrate applications beyond DTLS with up to 2 orders of magnitude energy\nsavings.'
Anantha Chandrakasan,Chandrakasan_Anantha,arXiv:1801.05507,https://arxiv.org/abs/1801.05507,"b""Abstract:  The growing popularity of cloud-based machine learning raises a natural\nquestion about the privacy guarantees that can be provided in such a setting.\nOur work tackles this problem in the context where a client wishes to classify\nprivate images using a convolutional neural network (CNN) trained by a server.\nOur goal is to build efficient protocols whereby the client can acquire the\nclassification result without revealing their input to the server, while\nguaranteeing the privacy of the server's neural network.\nTo this end, we design Gazelle, a scalable and low-latency system for secure\nneural network inference, using an intricate combination of homomorphic\nencryption and traditional two-party computation techniques (such as garbled\ncircuits). Gazelle makes three contributions. First, we design the Gazelle\nhomomorphic encryption library which provides fast algorithms for basic\nhomomorphic operations such as SIMD (single instruction multiple data)\naddition, SIMD multiplication and ciphertext permutation. Second, we implement\nthe Gazelle homomorphic linear algebra kernels which map neural network layers\nto optimized homomorphic matrix-vector multiplication and convolution routines.\nThird, we design optimized encryption switching protocols which seamlessly\nconvert between homomorphic and garbled circuit encodings to enable\nimplementation of complete neural network inference.\nWe evaluate our protocols on benchmark neural networks trained on the MNIST\nand CIFAR-10 datasets and show that Gazelle outperforms the best existing\nsystems such as MiniONN (ACM CCS 2017) by 20 times and Chameleon (Crypto Eprint\n2017/1164) by 30 times in online runtime. Similarly when compared with fully\nhomomorphic approaches like CryptoNets (ICML 2016) we demonstrate three orders\nof magnitude faster online run-time."""
Anantha Chandrakasan,Chandrakasan_Anantha,arXiv:0710.4815,https://arxiv.org/abs/0710.4815,b'Abstract:  Ultra-wideband (UWB) communication is an emerging wireless technology that\npromises high data rates over short distances and precise locationing. The\nlarge available bandwidth and the constraint of a maximum power spectral\ndensity drives a unique set of system challenges. This paper addresses these\nchallenges using two UWB transceivers and a discrete prototype platform.'
Anantha Chandrakasan,Chandrakasan_Anantha,arXiv:0710.4732,https://arxiv.org/abs/0710.4732,"b'Abstract:  Wireless microsensor networks, which have been the topic of intensive\nresearch in recent years, are now emerging in industrial applications. An\nimportant milestone in this transition has been the release of the IEEE\n802.15.4 standard that specifies interoperable wireless physical and medium\naccess control layers targeted to sensor node radios. In this paper, we\nevaluate the potential of an 802.15.4 radio for use in an ultra low power\nsensor node operating in a dense network. Starting from measurements carried\nout on the off-the-shelf radio, effective radio activation and link adaptation\npolicies are derived. It is shown that, in a typical sensor network scenario,\nthe average power per node can be reduced down to 211m mm mW. Next, the energy\nconsumption breakdown between the different phases of a packet transmission is\npresented, indicating which part of the transceiver architecture can most\neffectively be optimized in order to further reduce the radio power, enabling\nself-powered wireless microsensor networks.'"
Adam Chlipala,Chlipala_Adam,arXiv:1805.00468,https://arxiv.org/abs/1805.00468,"b'Abstract:  Though many safety-critical software systems use floating point to represent\nreal-world input and output, programmers usually have idealized versions in\nmind that compute with real numbers. Significant deviations from the ideal can\ncause errors and jeopardize safety. Some programming systems implement exact\nreal arithmetic, which resolves this matter but complicates others, such as\ndecision making. In these systems, it is impossible to compute (total and\ndeterministic) discrete decisions based on connected spaces such as\n$\\mathbb{R}$. We present programming-language semantics based on constructive\ntopology with variants allowing nondeterminism and/or partiality. Either\nnondeterminism or partiality suffices to allow computable decision making on\nconnected spaces such as $\\mathbb{R}$. We then introduce pattern matching on\nspaces, a language construct for creating programs on spaces, generalizing\npattern matching in functional programming, where patterns need not represent\ndecidable predicates and also may overlap or be inexhaustive, giving rise to\nnondeterminism or partiality, respectively. Nondeterminism and/or partiality\nalso yield formal logics for constructing approximate decision procedures. We\nimplemented these constructs in the Marshall language for exact real\narithmetic.'"
Adam Chlipala,Chlipala_Adam,arXiv:1803.04870,https://arxiv.org/abs/1803.04870,"b'Abstract:  It is a neat result from functional programming that libraries of parser\ncombinators can support rapid construction of decoders for quite a range of\nformats. With a little more work, the same combinator program can denote both a\ndecoder and an encoder. Unfortunately, the real world is full of gnarly\nformats, as with the packet formats that make up the standard Internet protocol\nstack. Most past parser-combinator approaches cannot handle these formats, and\nthe few exceptions require redundancy -- one part of the natural grammar needs\nto be hand-translated into hints in multiple parts of a parser program. We show\nhow to recover very natural and nonredundant format specifications, covering\nall popular network packet formats and generating both decoders and encoders\nautomatically. The catch is that we use the Coq proof assistant to derive both\nkinds of artifacts using tactics, automatically, in a way that guarantees that\nthey form inverses of each other. We used our approach to reimplement packet\nprocessing for a full Internet protocol stack, inserting our replacement into\nthe OCaml-based MirageOS unikernel, resulting in minimal performance\ndegradation.'"
Adam Chlipala,Chlipala_Adam,arXiv:1401.7694,https://arxiv.org/abs/1401.7694,"b""Abstract:  We describe our experience implementing a broad category-theory library in\nCoq. Category theory and computational performance are not usually mentioned in\nthe same breath, but we have needed substantial engineering effort to teach Coq\nto cope with large categorical constructions without slowing proof script\nprocessing unacceptably. In this paper, we share the lessons we have learned\nabout how to represent very abstract mathematical objects and arguments in Coq\nand how future proof assistants might be designed to better support such\nreasoning. One particular encoding trick to which we draw attention allows\ncategory-theoretic arguments involving duality to be internalized in Coq's\nlogic with definitional equality. Ours may be the largest Coq development to\ndate that uses the relatively new Coq version developed by homotopy type\ntheorists, and we reflect on which new features were especially helpful."""
Adam Chlipala,Chlipala_Adam,arXiv:1305.6543,https://arxiv.org/abs/1305.6543,"b""Abstract:  We describe a method for building composable and extensible verification\nprocedures within the Coq proof assistant. Unlike traditional methods that rely\non run-time generation and checking of proofs, we use verified-correct\nprocedures with Coq soundness proofs. Though they are internalized in Coq's\nlogic, our provers support sound extension by users with hints over new\ndomains, enabling automated reasoning about user-defined abstract predicates.\nWe maintain soundness by developing an architecture for modular packaging,\nconstruction, and composition of hint databases, which had previously only been\nimplemented in Coq at the level of its dynamically typed, proof-generating\ntactic language. Our provers also include rich handling of unification\nvariables, enabling integration with other tactic-based deduction steps within\nCoq. We have implemented our techniques in MirrorShard, an open-source\nframework for reflective verification. We demonstrate its applicability by\ninstantiating it to separation logic in order to reason about imperative\nprogram verification."""
Adam Chlipala,Chlipala_Adam,arXiv:1301.4779,https://arxiv.org/abs/1301.4779,"b'Abstract:  We report on the implementation of a certified compiler for a high-level\nhardware description language (HDL) called Fe-Si (FEatherweight SynthesIs).\nFe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded\natomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq.\nThe target language of the compiler corresponds to a synthesisable subset of\nVerilog or VHDL. A key aspect of our approach is that input programs to the\ncompiler can be defined and proved correct inside Coq. Then, we use extraction\nand a Verilog back-end (written in OCaml) to get a certified version of a\nhardware design.'"
Isaac Chuang,Chuang_Isaac,arXiv:1811.02124,https://arxiv.org/abs/1811.02124,"b'Abstract:  While quantum devices rely on interactions between constituent subsystems and\nwith their environment to operate, native interactions alone often fail to\ndeliver targeted performance. Coherent pulsed control provides the ability to\ntailor effective interactions, known as Hamiltonian engineering. We propose a\nHamiltonian engineering method that maximizes desired interactions while\nmitigating deleterious ones by conducting a pulse sequence search using\nconstrained optimization. The optimization formulation incorporates pulse\nsequence length and cardinality penalties consistent with linear or integer\nprogramming. We apply the general technique to magnetometry with solid state\nspin ensembles in which inhomogeneous interactions between sensing spins limit\ncoherence. Defining figures of merit for broadband Ramsey magnetometry, we\npresent novel pulse sequences which outperform known techniques for homonuclear\nspin decoupling in both spin-1/2 and spin-1 systems. When applied to nitrogen\nvacancy (NV) centers in diamond, this scheme partially preserves the Zeeman\ninteraction while zeroing dipolar coupling between negatively charged\nNV$^{\\text -}$ centers. Such a scheme is of interest for NV$^\\text{-}$\nmagnetometers which have reached the NV$^\\text{-}$-NV$^\\text{-}$ coupling\nlimit. We discuss experimental implementation in NV ensembles, as well as\napplicability of the current approach to more general spin bath decoupling and\nsuperconducting qubit control.'"
Isaac Chuang,Chuang_Isaac,arXiv:1807.09912,https://arxiv.org/abs/1807.09912,"b'Abstract:  Compared to humans, machine learning models generally require significantly\nmore training examples and fail to extrapolate from experience to solve\npreviously unseen challenges. To help close this performance gap, we augment\nsingle-task neural networks with a meta-recognition model which learns a\nsuccinct model code via its autoencoder structure, using just a few informative\nexamples. The model code is then employed by a meta-generative model to\nconstruct parameters for the task-specific model. We demonstrate that for\npreviously unseen tasks, without additional training, this Meta-Learning\nAutoencoder (MeLA) framework can build models that closely match the true\nunderlying models, with loss significantly lower than given by fine-tuned\nbaseline networks, and performance that compares favorably with\nstate-of-the-art meta-learning algorithms. MeLA also adds the ability to\nidentify influential training examples and predict which additional data will\nbe most valuable to acquire to improve model prediction.'"
Isaac Chuang,Chuang_Isaac,arXiv:1801.07618,https://arxiv.org/abs/1801.07618,"b'Abstract:  Each time a learner in a self-paced online course is trying to answer an\nassessment question, it takes some time to submit the answer, and if multiple\nattempts are allowed and the first answer was incorrect, it takes some time to\nsubmit the second attempt, and so on. Here we study the distribution of such\n""response times"". We find that the log-normal statistical model for such times,\npreviously suggested in the literature, holds for online courses qualitatively.\nUsers who, according to this model, tend to take longer on submits are more\nlikely to complete the course, have a higher level of engagement and achieve a\nhigher grade. This finding can be the basis for designing interventions in\nonline courses, such as MOOCs, which would encourage some users to slow down.'"
Isaac Chuang,Chuang_Isaac,arXiv:1801.01081,https://arxiv.org/abs/1801.01081,"b'Abstract:  We present a novel set of reversible modular multipliers applicable to\nquantum computing, derived from three classical techniques: 1) traditional\ninteger division, 2) Montgomery residue arithmetic, and 3) Barrett reduction.\nEach multiplier computes an exact result for all binary input values, while\nmaintaining the asymptotic resource complexity of a single (non-modular)\ninteger multiplier. We additionally conduct an empirical resource analysis of\nour designs in order to determine the total gate count and circuit depth of\neach fully constructed circuit, with inputs as large as 2048 bits. Our\ncomparative analysis considers both circuit implementations which allow for\narbitrary (controlled) rotation gates, as well as those restricted to a typical\nfault-tolerant gate set.'"
Isaac Chuang,Chuang_Isaac,arXiv:1709.05302,https://arxiv.org/abs/1709.05302,"b'Abstract:  We establish a symmetry-operator framework for designing quantum error\ncorrecting~(QEC) codes based on fundamental properties of the underlying system\ndynamics. Based on this framework, we propose three hardware-efficient bosonic\nQEC codes that are suitable for $\\chi^{(2)}$-interaction based quantum\ncomputation: the $\\chi^{(2)}$ parity-check code, the $\\chi^{(2)}$ embedded\nerror-correcting code, and the $\\chi^{(2)}$ binomial code, all of which detect\nphoton-loss or photon-gain errors by means of photon-number parity measurements\nand then correct them via $\\chi^{(2)}$ Hamiltonian evolutions and linear-optics\ntransformations. Our symmetry-operator framework provides a systematic\nprocedure for finding QEC codes that are not stabilizer codes. The $\\chi^{(2)}$\nbinomial code is of special interest because, with $m\\le N$ identified from\nchannel monitoring, it can correct $m$-photon loss errors, $m$-photon gain\nerrors, and $(m-1)$th-order dephasing errors using logical qudits that are\nencoded in $O(N)$ photons. In comparison, other bosonic QEC codes require\n$O(N^2)$ photons to correct the same degree of bosonic errors. Such improved\nphoton-efficiency underscores the additional error-correction power that can be\nprovided by channel monitoring. We develop quantum Hamming bounds for\nphoton-loss errors in the code subspaces associated with the $\\chi^{(2)}$\nparity-check code and the $\\chi^{(2)}$ embedded error-correcting code, and we\nprove that these codes saturate their respective bounds. Our $\\chi^{(2)}$ QEC\ncodes exhibit hardware efficiency in that they address the principal error\nmechanisms and exploit the available physical interactions of the underlying\nhardware, thus reducing the physical resources required for implementing their\nencoding, decoding, and error-correction operations, and their universal\nencoded-basis gate sets.'"
Isaac Chuang,Chuang_Isaac,arXiv:1707.05391,https://arxiv.org/abs/1707.05391,"b'Abstract:  The exponential speedups promised by Hamiltonian simulation on a quantum\ncomputer depends crucially on structure in both the Hamiltonian $\\hat{H}$, and\nthe quantum circuit $\\hat{U}$ that encodes its description. In the quest to\nbetter approximate time-evolution $e^{-i\\hat{H}t}$ with error $\\epsilon$, we\nmotivate a systematic approach to understanding and exploiting structure, in a\nsetting where Hamiltonians are encoded as measurement operators of unitary\ncircuits $\\hat{U}$ for generalized measurement. This allows us to define a\n\\emph{uniform spectral amplification} problem on this framework for expanding\nthe spectrum of encoded Hamiltonian with exponentially small distortion. We\npresent general solutions to uniform spectral amplification in a hierarchy\nwhere factoring $\\hat{U}$ into $n=1,2,3$ unitary oracles represents increasing\nstructural knowledge of the encoding. Combined with structural knowledge of the\nHamiltonian, specializing these results allow us simulate time-evolution by\n$d$-sparse Hamiltonians using $\\mathcal{O}\\left(t(d \\|\\hat\nH\\|_{\\text{max}}\\|\\hat H\\|_{1})^{1/2}\\log{(t\\|\\hat{H}\\|/\\epsilon)}\\right)$\nqueries, where $\\|\\hat H\\|\\le \\|\\hat H\\|_1\\le d\\|\\hat H\\|_{\\text{max}}$. Up to\nlogarithmic factors, this is a polynomial improvement upon prior art using\n$\\mathcal{O}\\left(td\\|\\hat\nH\\|_{\\text{max}}+\\frac{\\log{(1/\\epsilon)}}{\\log\\log{(1/\\epsilon)}}\\right)$ or\n$\\mathcal{O}(t^{3/2}(d \\|\\hat H\\|_{\\text{max}}\\|\\hat H\\|_{1}\\|\\hat\nH\\|/\\epsilon)^{1/2})$ queries. In the process, we also prove a matching lower\nbound of $\\Omega(t(d\\|\\hat H\\|_{\\text{max}}\\|\\hat H\\|_{1})^{1/2})$ queries,\npresent a distortion-free generalization of spectral gap amplification, and an\namplitude amplification algorithm that performs multiplication on unknown state\namplitudes.'"
Isaac Chuang,Chuang_Isaac,arXiv:1707.00012,https://arxiv.org/abs/1707.00012,"b'Abstract:  A non-Clifford gate is required for universal quantum computation, and,\ntypically, this is the most error-prone and resource intensive logical\noperation on an error-correcting code. Small, single-qubit rotations are\npopular choices for this non-Clifford gate, but certain three-qubit gates, such\nas Toffoli or controlled-controlled-Z (CCZ), are equivalent options that are\nalso more suited for implementing some quantum algorithms, for instance those\nwith coherent classical subroutines. Here, we calculate error rates and\nresource overheads for implementing logical CCZ with pieceable fault-tolerance,\na non-transversal method for implementing logical gates. We provide a\ncomparison with a non-local magic-state scheme on a concatenated code and a\nlocal magic-state scheme on the surface code. We find the pieceable\nfault-tolerance scheme particularly advantaged over magic states on\nconcatenated codes and in certain regimes over magic states on the surface\ncode. Our results suggest that pieceable fault-tolerance is a promising\ncandidate for fault-tolerance in a near-future quantum computer.'"
Isaac Chuang,Chuang_Isaac,arXiv:1705.01936,https://arxiv.org/abs/1705.01936,"b'Abstract:  Noisy PN learning is the problem of binary classification when training\nexamples may be mislabeled (flipped) uniformly with noise rate rho1 for\npositive examples and rho0 for negative examples. We propose Rank Pruning (RP)\nto solve noisy PN learning and the open problem of estimating the noise rates,\ni.e. the fraction of wrong positive and negative labels. Unlike prior\nsolutions, RP is time-efficient and general, requiring O(T) for any\nunrestricted choice of probabilistic classifier with T fitting time. We prove\nRP has consistent noise estimation and equivalent expected risk as learning\nwith uncorrupted labels in ideal conditions, and derive closed-form solutions\nwhen conditions are non-ideal. RP achieves state-of-the-art noise estimation\nand F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the\namount of noise and performs similarly impressively when a large portion of\ntraining examples are noise drawn from a third distribution. To highlight, RP\nwith a CNN classifier can predict if an MNIST digit is a ""one""or ""not"" with\nonly 0.25% error, and 0.46 error across all digits, even when 50% of positive\nexamples are mislabeled and 50% of observed positive labels are mislabeled\nnegative examples.'"
Isaac Chuang,Chuang_Isaac,arXiv:1704.03431,https://arxiv.org/abs/1704.03431,"b'Abstract:  We prove that universal quantum computation can be realized---using only\nlinear optics and $\\chi^{(2)}$ (three-wave mixing) interactions---in any\n$(n+1)$-dimensional qudit basis of the $n$-pump-photon subspace. First, we\nexhibit a strictly universal gate set for the qubit basis in the\none-pump-photon subspace. Next, we demonstrate qutrit-basis universality by\nproving that $\\chi^{(2)}$ Hamiltonians and photon-number operators generate the\nfull $\\mathfrak{u}(3)$ Lie algebra in the two-pump-photon subspace, and showing\nhow the qutrit controlled-$Z$ gate can be implemented with only linear optics\nand $\\chi^{(2)}$ interactions. We then use proof by induction to obtain our\ngeneral qudit result. Our induction proof relies on coherent photon\ninjection/subtraction, a technique enabled by $\\chi^{(2)}$ interaction between\nthe encoding modes and ancillary modes. Finally, we show that coherent photon\ninjection is more than a conceptual tool in that it offers a route to preparing\nhigh-photon-number Fock states from single-photon Fock states.'"
Isaac Chuang,Chuang_Isaac,arXiv:1610.06546,https://arxiv.org/abs/1610.06546,"b""Abstract:  Given a Hermitian operator $\\hat{H}=\\langle G|\\hat{U}|G\\rangle$ that is the\nprojection of an oracle $\\hat{U}$ by state $|G\\rangle$ created with oracle\n$\\hat{G}$, the problem of Hamiltonian simulation is approximating the time\nevolution operator $e^{-i\\hat{H}t}$ at time $t$ with error $\\epsilon$. We show\nthat this can be done with query complexity\n$\\mathcal{O}\\big(t+\\frac{\\log{(1/\\epsilon)}}{\\log\\log{(1/\\epsilon)}}\\big)$ to\n$\\hat{G},\\hat{U}$ that is optimal, not just in asymptotic limits, but for all\nvalues $t,\\epsilon$. Furthermore, only $2$ additional ancilla qubits are\nrequired in total, together with $\\mathcal{O}(1)$ additional single and\ntwo-qubit gates per query. Our approach to Hamiltonian simulation subsumes\nimportant prior art considering Hamiltonians which are $d$-sparse or a linear\ncombination of unitaries, leading to significant improvements in space\ncomplexity, as well as a quadratic speed-up for precision simulations. It also\nmotivates useful new instances, such as where $\\langle G|\\hat{U}|G\\rangle$ is a\ndensity matrix. A key technical result is `qubitization' which uses\ncontrolled-$\\hat{U}$ and controlled-$\\hat{G}$ to embed $\\hat{H}$ in an\ninvariant $\\text{SU}(2)$ subspace. A large class of operator functions of\n$\\hat{H}$ can then be computed with optimal query complexity, of which\n$e^{-i\\hat{H}t}$ is a special case."""
Isaac Chuang,Chuang_Isaac,arXiv:1609.03603,https://arxiv.org/abs/1609.03603,"b""Abstract:  Fixed-point quantum search algorithms succeed at finding one of $M$ target\nitems among $N$ total items even when the run time of the algorithm is longer\nthan necessary. While the famous Grover's algorithm can search quadratically\nfaster than a classical computer, it lacks the fixed-point property --- the\nfraction of target items must be known precisely to know when to terminate the\nalgorithm. Recently, Yoder, Low, and Chuang gave an optimal gate-model search\nalgorithm with the fixed-point property. Meanwhile, it is known that an\nadiabatic quantum algorithm, operating by continuously varying a Hamiltonian,\ncan reproduce the quadratic speedup of gate-model Grover search. We ask, can an\nadiabatic algorithm also reproduce the fixed-point property? We show that the\nanswer depends on what interpolation schedule is used, so as in the gate model,\nthere are both fixed-point and non-fixed-point versions of adiabatic search,\nonly some of which attain the quadratic quantum speedup. Guided by geometric\nintuition on the Bloch sphere, we rigorously justify our claims with an\nexplicit upper bound on the error in the adiabatic approximation. We also show\nthat the fixed-point adiabatic search algorithm can be simulated in the gate\nmodel with neither loss of the quadratic Grover speedup nor of the fixed-point\nproperty. Finally, we discuss natural uses of fixed-point algorithms such as\npreparation of a relatively prime state and oblivious amplitude amplification."""
Isaac Chuang,Chuang_Isaac,arXiv:1606.02685,https://arxiv.org/abs/1606.02685,"b'Abstract:  The physics of quantum mechanics is the inspiration for, and underlies,\nquantum computation. As such, one expects physical intuition to be highly\ninfluential in the understanding and design of many quantum algorithms,\nparticularly simulation of physical systems. Surprisingly, this has been\nchallenging, with current Hamiltonian simulation algorithms remaining abstract\nand often the result of sophisticated but unintuitive constructions. We contend\nthat physical intuition can lead to optimal simulation methods by showing that\na focus on simple single-qubit rotations elegantly furnishes an optimal\nalgorithm for Hamiltonian simulation, a universal problem that encapsulates all\nthe power of quantum computation. Specifically, we show that the query\ncomplexity of implementing time evolution by a $d$-sparse Hamiltonian $\\hat{H}$\nfor time-interval $t$ with error $\\epsilon$ is\n$\\mathcal{O}(td\\|\\hat{H}\\|_{\\text{max}}+\\frac{\\log{(1/\\epsilon)}}{\\log{\\log{(1/\\epsilon)}}})$,\nwhich matches lower bounds in all parameters. This connection is made through\ngeneral three-step ""quantum signal processing"" methodology, comprised of (1)\ntransducing eigenvalues of $\\hat{H}$ into a single ancilla qubit, (2)\ntransforming these eigenvalues through an optimal-length sequence of\nsingle-qubit rotations, and (3) projecting this ancilla with near unity success\nprobability.'"
Isaac Chuang,Chuang_Isaac,arXiv:1606.02188,https://arxiv.org/abs/1606.02188,"b'Abstract:  Classical imaging works by scattering photons from an object to be imaged,\nand achieves resolution scaling as $1/\\sqrt{t}$, with $t$ the imaging time. By\ncontrast, the laws of quantum mechanics allow one to utilize quantum coherence\nto obtain imaging resolution that can scale as quickly as $1/t$ -- the\nso-called ""Heisenberg limit."" However, ambiguities in the obtained signal often\npreclude taking full advantage of this quantum enhancement, while imaging\ntechniques designed to be unambiguous often lose this optimal Heisenberg\nscaling. Here, we demonstrate an imaging technique which combines unambiguous\ndetection of the target with Heisenberg scaling of the resolution. We also\ndemonstrate a binary search algorithm which can efficiently locate a coherent\ntarget using the technique, resolving a target trapped ion to within 0.3% of\nthe $1/e^2$ diameter of the excitation beam.'"
Isaac Chuang,Chuang_Isaac,arXiv:1605.04210,https://arxiv.org/abs/1605.04210,"b'Abstract:  We report on a method for measuring the branching ratios of dipole\ntransitions of trapped atomic ions by performing nested sequences of population\ninversions. This scheme is broadly applicable and does not use ultrafast pulsed\nor narrow linewidth lasers. It is simple to perform and insensitive to\nexperimental variables such as laser and magnetic field noise as well as ion\nheating. To demonstrate its effectiveness, we make the most accurate\nmeasurements thus far of the branching ratios of both 5P1/2 and 5P3/2 states in\n88Sr+ with sub-1% uncertainties. We measure 17.175(27) for the branching ratio\nof 5P1/2-5S1/2, 15.845(71) for 5P3/2-5S1/2, and 0.05609(21) for 5P3/2-4D5/2,\nten- fold and thirty-fold improvements in precision for 5P1/2 and 5P3/2\nbranching ratios respectively over the best previous experimental values.'"
Isaac Chuang,Chuang_Isaac,arXiv:1603.03996,https://arxiv.org/abs/1603.03996,"b'Abstract:  The creation of composite quantum gates that implement quantum response\nfunctions $\\hat{U}(\\theta)$ dependent on some parameter of interest $\\theta$ is\noften more of an art than a science. Through inspired design, a sequence of $L$\nprimitive gates also depending on $\\theta$ can engineer a highly nontrivial\n$\\hat{U}(\\theta)$ that enables myriad precision metrology, spectroscopy, and\ncontrol techniques. However, discovering new, useful examples of\n$\\hat{U}(\\theta)$ requires great intuition to perceive the possibilities, and\noften brute-force to find optimal implementations. We present a systematic and\nefficient methodology for composite gate design of arbitrary length, where\nphase-controlled primitive gates all rotating by $\\theta$ act on a single spin.\nWe fully characterize the realizable family of $\\hat{U}(\\theta)$, provide an\nefficient algorithm that decomposes a choice of $\\hat{U}(\\theta)$ into its\nshortest sequence of gates, and show how to efficiently choose an achievable\n$\\hat{U}(\\theta)$ that for fixed $L$, is an optimal approximation to objective\nfunctions on its quadratures. A strong connection is forged with\n\\emph{classical} discrete-time signal processing, allowing us to swiftly\nconstruct, as examples, compensated gates with optimal bandwidth that implement\narbitrary single spin rotations with sub-wavelength spatial selectivity.'"
Isaac Chuang,Chuang_Isaac,arXiv:1603.03948,https://arxiv.org/abs/1603.03948,"b'Abstract:  It is an oft-cited fact that no quantum code can support a set of\nfault-tolerant logical gates that is both universal and transversal. This no-go\ntheorem is generally responsible for the interest in alternative universality\nconstructions including magic state distillation. Widely overlooked, however,\nis the possibility of non-transversal, yet still fault-tolerant, gates that\nwork directly on small quantum codes. Here we demonstrate precisely the\nexistence of such gates. In particular, we show how the limits of\nnon-transversality can be overcome by performing rounds of intermediate\nerror-correction to create logical gates on stabilizer codes that use no\nancillas other than those required for syndrome measurement. Moreover, the\nlogical gates we construct, the most prominent examples being Toffoli and\ncontrolled-controlled-Z, often complete universal gate sets on their codes. We\ndetail such universal constructions for the smallest quantum codes, the 5-qubit\nand 7-qubit codes, and then proceed to generalize the approach. One remarkable\nresult of this generalization is that any nondegenerate stabilizer code with a\ncomplete set of fault-tolerant single-qubit Clifford gates has a universal set\nof fault-tolerant gates. Another is the interaction of logical qubits across\ndifferent stabilizer codes, which, for instance, implies a broadly applicable\nmethod of code switching.'"
Isaac Chuang,Chuang_Isaac,arXiv:1508.05699,https://arxiv.org/abs/1508.05699,"b'Abstract:  We describe a cheating strategy enabled by the features of massive open\nonline courses (MOOCs) and detectable by virtue of the sophisticated data\nsystems that MOOCs provide. The strategy, Copying Answers using Multiple\nExistences Online (CAMEO), involves a user who gathers solutions to assessment\nquestions using a ""harvester"" account and then submits correct answers using a\nseparate ""master"" account. We use ""clickstream"" learner data to detect CAMEO\nuse among 1.9 million course participants in 115 MOOCs from two universities.\nUsing conservative thresholds, we estimate CAMEO prevalence at 1,237\ncertificates, accounting for 1.3% of the certificates in the 69 MOOCs with\nCAMEO users. Among earners of 20 or more certificates, 25% have used the CAMEO\nstrategy. CAMEO users are more likely to be young, male, and international than\nother MOOC certificate earners. We identify preventive strategies that can\ndecrease CAMEO rates and show evidence of their effectiveness in science\ncourses.'"
Isaac Chuang,Chuang_Isaac,arXiv:1507.08852,https://arxiv.org/abs/1507.08852,"b'Abstract:  Quantum computers are able to outperform classical algorithms. This was long\nrecognized by the visionary Richard Feynman who pointed out in the 1980s that\nquantum mechanical problems were better solved with quantum machines. It was\nonly in 1994 that Peter Shor came up with an algorithm that is able to\ncalculate the prime factors of a large number vastly more efficiently than\nknown possible with a classical computer. This paradigmatic algorithm\nstimulated the flourishing research in quantum information processing and the\nquest for an actual implementation of a quantum computer. Over the last fifteen\nyears, using skillful optimizations, several instances of a Shor algorithm have\nbeen implemented on various platforms and clearly proved the feasibility of\nquantum factoring. For general scalability, though, a different approach has to\nbe pursued. Here, we report the realization of a fully scalable Shor algorithm\nas proposed by Kitaev. For this, we demonstrate factoring the number fifteen by\neffectively employing and controlling seven qubits and four ""cache-qubits"",\ntogether with the implementation of generalized arithmetic operations, known as\nmodular multipliers. The scalable algorithm has been realized with an ion-trap\nquantum computer exhibiting success probabilities in excess of 90%.'"
Isaac Chuang,Chuang_Isaac,arXiv:1505.03381,https://arxiv.org/abs/1505.03381,"b'Abstract:  We study the vacuum-induced degradation of high-finesse optical cavities with\nmirror coatings composed of SiO$_2$-Ta$_{2}$O$_{5}$ dielectric stacks, and\npresent methods to protect these coatings and to recover their initial quality\nfactor. For separate coatings with reflectivities centered at 370 nm and 422\nnm, a vacuum-induced continuous increase in optical loss occurs if the\nsurface-layer coating is made of Ta$_{2}$O$_{5}$, while it does not occur if it\nis made of SiO$_2$. The incurred optical loss can be reversed by filling the\nvacuum chamber with oxygen at atmospheric pressure, and the recovery rate can\nbe strongly accelerated by continuous laser illumination at 422 nm. Both the\ndegradation and the recovery processes depend strongly on temperature. We find\nthat a 1 nm-thick layer of SiO$_2$ passivating the Ta$_{2}$O$_{5}$ surface\nlayer is sufficient to reduce the degradation rate by more than a factor of 10,\nstrongly supporting surface oxygen depletion as the primary degradation\nmechanism.'"
Isaac Chuang,Chuang_Isaac,arXiv:1502.05739,https://arxiv.org/abs/1502.05739,"b'Abstract:  Scaling-up from prototype systems to dense arrays of ions on chip, or vast\nnetworks of ions connected by photonic channels, will require developing\nentirely new technologies that combine miniaturized ion trapping systems with\ndevices to capture, transmit and detect light, while refining how ions are\nconfined and controlled. Building a cohesive ion system from such diverse parts\ninvolves many challenges, including navigating materials incompatibilities and\nundesired coupling between elements. Here, we review our recent efforts to\ncreate scalable ion systems incorporating unconventional materials such as\ngraphene and indium tin oxide, integrating devices like optical fibers and\nmirrors, and exploring alternative ion loading and trapping techniques.'"
Isaac Chuang,Chuang_Isaac,arXiv:1409.7993,https://arxiv.org/abs/1409.7993,"b'Abstract:  Conventional wisdom dictates that to image the position of fluorescent atoms\nor molecules, one should stimulate as much emission and collect as many photons\nas possible. That is, in this classical case, it has always been assumed that\nthe coherence time of the system should be made short, and that the statistical\nscaling $\\sim1/\\sqrt{t}$ defines the resolution limit for imaging time $t$.\nHowever, here we show in contrast that given the same resources, a long\ncoherence time permits a higher resolution image. In this quantum regime, we\ngive a procedure for determining the position of a single two-level system, and\ndemonstrate that the standard errors of our position estimates scale at the\nHeisenberg limit as $\\sim 1/t$, a quadratic, and notably optimal, improvement\nover the classical case.'"
Isaac Chuang,Chuang_Isaac,arXiv:1409.3305,https://arxiv.org/abs/1409.3305,"b""Abstract:  Grover's quantum search and its generalization, quantum amplitude\namplification, provide quadratic advantage over classical algorithms for a\ndiverse set of tasks, but are tricky to use without knowing beforehand what\nfraction $\\lambda$ of the initial state is comprised of the target states. In\ncontrast, fixed-point search algorithms need only a reliable lower bound on\nthis fraction, but, as a consequence, lose the very quadratic advantage that\nmakes Grover's algorithm so appealing. Here we provide the first version of\namplitude amplification that achieves fixed-point behavior without sacrificing\nthe quantum speedup. Our result incorporates an adjustable bound on the failure\nprobability, and, for a given number of oracle queries, guarantees that this\nbound is satisfied over the broadest possible range of $\\lambda$."""
Isaac Chuang,Chuang_Isaac,arXiv:1402.7359,https://arxiv.org/abs/1402.7359,"b""Abstract:  Performing exact inference on Bayesian networks is known to be #P-hard.\nTypically approximate inference techniques are used instead to sample from the\ndistribution on query variables given the values $e$ of evidence variables.\nClassically, a single unbiased sample is obtained from a Bayesian network on\n$n$ variables with at most $m$ parents per node in time\n$\\mathcal{O}(nmP(e)^{-1})$, depending critically on $P(e)$, the probability the\nevidence might occur in the first place. By implementing a quantum version of\nrejection sampling, we obtain a square-root speedup, taking\n$\\mathcal{O}(n2^mP(e)^{-\\frac12})$ time per sample. We exploit the Bayesian\nnetwork's graph structure to efficiently construct a quantum state, a q-sample,\nrepresenting the intended classical distribution, and also to efficiently apply\namplitude amplification, the source of our speedup. Thus, our speedup is\nnotable as it is unrelativized -- we count primitive operations and require no\nblackbox oracle queries."""
Isaac Chuang,Chuang_Isaac,arXiv:1310.3173,https://arxiv.org/abs/1310.3173,"b'Abstract:  Massive Open Online Courses are an exciting new avenue for instruction and\nresearch, yet they are full of unknowns. In the Spring of 2013, MITx released\nits first introductory physics MOOC through the edX platform, generating a\ntotal enrollment of 43,000 students from around the world. We describe the\npopulation of participants in terms of their age, gender, level of education,\nand country of origin, highlighting both the diversity of 8.02x enrollees as\nwell as gender gap and retention. Using three midterm exams and the final as\nwaypoints, we highlight performance by different demographic subpopulations and\ntheir retention rates. Our work is generally aimed at making a bridge between\navailable MOOC data and topics associated with the Physics Education Research\ncommunity.'"
Isaac Chuang,Chuang_Isaac,arXiv:1307.2211,https://arxiv.org/abs/1307.2211,"b'Abstract:  Implementing a single qubit unitary is often hampered by imperfect control.\nSystematic amplitude errors $\\epsilon$, caused by incorrect duration or\nstrength of a pulse, are an especially common problem. But a sequence of\nimperfect pulses can provide a better implementation of a desired operation, as\ncompared to a single primitive pulse. We find optimal pulse sequences\nconsisting of $L$ primitive $\\pi$ or $2\\pi$ rotations that suppress such errors\nto arbitrary order $\\mathcal{O}(\\epsilon^{n})$ on arbitrary initial states.\nOptimality is demonstrated by proving an $L=\\mathcal{O}(n)$ lower bound and\nsaturating it with $L=2n$ solutions. Closed-form solutions for arbitrary\nrotation angles are given for $n=1,2,3,4$. Perturbative solutions for any $n$\nare proven for small angles, while arbitrary angle solutions are obtained by\nanalytic continuation up to $n=12$. The derivation proceeds by a novel\nalgebraic and non-recursive approach, in which finding amplitude error\ncorrecting sequences can be reduced to solving polynomial equations.'"
Isaac Chuang,Chuang_Isaac,arXiv:1302.2904,https://arxiv.org/abs/1302.2904,b'Abstract:  We present a novel hybrid system where an optical cavity is integrated with a\nmicrofabricated planar-electrode ion trap. The trap electrodes produce a\ntunable periodic potential allowing the trapping of up to 50 separate ion\nchains spaced by 160 $\\mu$m along the cavity axis. Each chain can contain up to\n20 individually addressable Yb\\textsuperscript{+} ions coupled to the cavity\nmode. We demonstrate deterministic distribution of ions between the sites of\nthe electrostatic periodic potential and control of the ion-cavity coupling.\nThe measured strength of this coupling should allow access to the strong\ncollective coupling regime with $\\lesssim$10 ions. The optical cavity could\nserve as a quantum information bus between ions or be used to generate a strong\nwavelength-scale periodic optical potential.'
Isaac Chuang,Chuang_Isaac,arXiv:1212.1443,https://arxiv.org/abs/1212.1443,"b'Abstract:  Fluorescence collection sets the efficiency of state detection and the rate\nof entanglement generation between remote trapped ion qubits. Despite efforts\nto improve light collection using various optical elements, solid angle capture\nis limited to ~10% for implementations that are scalable to many ions. We\npresent an approach based on fluorescence detection through a transparent trap\nusing an integrated photodetector, combining collection efficiency approaching\n50% with scalability. We microfabricate transparent surface traps with indium\ntin oxide and verify stable trapping of single ions. The fluorescence from a\ncloud of ions is detected using a photodiode sandwiched with a transparent\ntrap.'"
Isaac Chuang,Chuang_Isaac,arXiv:1207.5846,https://arxiv.org/abs/1207.5846,"b'Abstract:  Fermions, as a major class of quantum particles, provide platforms for\nquantum information processing beyond the possibilities of spins or bosons\nwhich have been studied more extensively. One particularly interesting model to\nstudy, in view of recent progress in manipulating ultracold fermion gases, is\nthe fermionic version of measurement-based quantum computation (MBQC), which\nimplements full quantum computation with only single site measurements on a\nproper fermionic many-body resource state. However, it is not known which\nfermionic states can be used as the resource states for MBQC and how to find\nthem. In this paper, we generalize the framework of spin MBQC to fermions. In\nparticular, we provide a general formalism to construct many-body entangled\nfermion resource states for MBQC based on the fermionic projected entangled\npair state representation. We give a specific fermionic state which enables\nuniversal MBQC and demonstrate that the non-locality inherent in fermion\nsystems can be properly taken care of with suitable measurement schemes. Such a\nframework opens up possibilities of finding MBQC resource states which can be\nmore readily realized in the lab.'"
Isaac Chuang,Chuang_Isaac,arXiv:1202.6640,https://arxiv.org/abs/1202.6640,"b'Abstract:  Previous analyses of conditional \\phi-phase gates for photonic qubits that\ntreat cross-phase modulation (XPM) in a causal, multimode, quantum field\nsetting suggest that a large (~\\pi rad) nonlinear phase shift is always\naccompanied by fidelity-degrading noise [J. H. Shapiro, Phys. Rev. A 73, 062305\n(2006); J. Gea-Banacloche, Phys. Rev. A 81, 043823 (2010)]. Using an atomic\nV-system to model an XPM medium, we present a conditional phase gate that, for\nsufficiently small nonzero \\phi, has high fidelity. The gate is made cascadable\nby using using a special measurement, principal mode projection, to exploit the\nquantum Zeno effect and preclude the accumulation of fidelity-degrading\ndepartures from the principal-mode Hilbert space when both control and target\nphotons illuminate the gate.'"
Isaac Chuang,Chuang_Isaac,arXiv:1109.2995,https://arxiv.org/abs/1109.2995,"b""Abstract:  We model electric field noise from fluctuating patch potentials on conducting\nsurfaces by taking into account the finite geometry of the ion trap electrodes\nto gain insight into the origin of anomalous heating in ion traps. The scaling\nof anomalous heating rates with surface distance, $d$, is obtained for several\ngeneric geometries of relevance to current ion trap designs, ranging from\nplanar to spheroidal electrodes. The influence of patch size is studied both by\nsolving Laplace's equation in terms of the appropriate Green's function as well\nas through an eigenfunction expansion. Scaling with surface distance is found\nto be highly dependent on the choice of geometry and the relative scale between\nthe spatial extent of the electrode, the ion-electrode distance, and the patch\nsize. Our model generally supports the $d^{-4}$ dependence currently found by\nmost experiments and models, but also predicts geometry-driven deviations from\nthis trend."""
Isaac Chuang,Chuang_Isaac,arXiv:1108.0092,https://arxiv.org/abs/1108.0092,"b""Abstract:  Electrical charging of metal surfaces due to photoelectric generation of\ncarriers is of concern in trapped ion quantum computation systems, due to the\nhigh sensitivity of the ions' motional quantum states to deformation of the\ntrapping potential. The charging induced by typical laser frequencies involved\nin doppler cooling and quantum control is studied here, with microfabricated\nsurface electrode traps made of aluminum, copper, and gold, operated at 6 K\nwith a single Sr$^+$ ion trapped 100 $\\mu$m above the trap surface. The lasers\nused are at 370, 405, 460, and 674 nm, and the typical photon flux at the trap\nis 10$^{14}$ photons/cm$^2$/sec. Charging is detected by monitoring the ion's\nmicromotion signal, which is related to the number of charges created on the\ntrap. A wavelength and material dependence of the charging behavior is\nobserved: lasers at lower wavelengths cause more charging, and aluminum\nexhibits more charging than copper or gold. We describe the charging dynamic\nbased on a rate equation approach."""
Isaac Chuang,Chuang_Isaac,arXiv:1103.5256,https://arxiv.org/abs/1103.5256,"b'Abstract:  An atomic ion is trapped at the tip of a single-mode optical fiber in a\ncryogenic (8 K) surface-electrode ion trap. The fiber serves as an integrated\nsource of laser light, which drives the quadrupole qubit transition of\n$^{88}$Sr$^+$. Through \\emph{in situ} translation of the nodal point of the\ntrapping field, the Gaussian beam profile of the fiber output is imaged, and\nthe fiber-ion displacement, in units of the mode waist at the ion, is optimized\nto within $0.13\\pm0.10$ of the mode center despite an initial offset of\n$3.30\\pm0.10$. Fiber-induced charging at $125 \\mu$W is observed to be\n${\\sim}10$ V/m at an ion height of $670 \\mu$m, with charging and discharging\ntime constants of $1.6\\pm0.3$ s and $4.7\\pm0.6$ s respectively. This work is of\nimportance to large-scale, ion-based quantum information processing, where\noptics integration in surface-electrode designs may be a crucial enabling\ntechnology.'"
Isaac Chuang,Chuang_Isaac,arXiv:1011.5259,https://arxiv.org/abs/1011.5259,"b'Abstract:  A novel approach to optics integration in ion traps is demonstrated based on\na surface electrode ion trap that is microfabricated on top of a dielectric\nmirror. Additional optical losses due to fabrication are found to be as low as\n80 ppm for light at 422 nm. The integrated mirror is used to demonstrate light\ncollection from, and imaging of, a single 88 Sr+ ion trapped $169\\pm4 \\mu$m\nabove the mirror.'"
Isaac Chuang,Chuang_Isaac,arXiv:1010.6108,https://arxiv.org/abs/1010.6108,"b'Abstract:  We fabricate superconducting ion traps with niobium and niobium nitride and\ntrap single 88Sr ions at cryogenic temperatures. The superconducting transition\nis verified and characterized by measuring the resistance and critical current\nusing a 4-wire measurement on the trap structure, and observing change in the\nrf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz\nat 6 K and shows no significant change across the superconducting transition,\nsuggesting that anomalous heating is primarily caused by noise sources on the\nsurface. This demonstration of superconducting ion traps opens up possibilities\nfor integrating trapped ions and molecular ions with superconducting devices.'"
Isaac Chuang,Chuang_Isaac,arXiv:1010.2717,https://arxiv.org/abs/1010.2717,"b'Abstract:  It is well known that the ground state energy of many-particle Hamiltonians\ninvolving only 2-body interactions can be obtained using constrained\noptimizations over density matrices which arise from reducing an N-particle\nstate. While determining which 2-particle density matrices are ""N-\nrepresentable"" is a computationally hard problem, all known extreme\nN-representable 2-particle reduced density matrices arise from a unique\nN-particle pre-image, satisfying a conjecture established in 1972. We present\nexplicit counterexamples to this conjecture through giving Hamiltonians with\n2-body interactions which have degenerate ground states that cannot be\ndistinguished by any 2-body operator. We relate the existence of such\ncounterexamples to quantum error correction codes and topologically ordered\nspin systems.'"
Isaac Chuang,Chuang_Isaac,arXiv:1009.0036,https://arxiv.org/abs/1009.0036,"b'Abstract:  Two-dimensional crystals of trapped ions are a promising system with which to\nimplement quantum simulations of challenging problems such as spin frustration.\nHere, we present a design for a surface-electrode elliptical ion trap which\nproduces a 2-D ion crystal and is amenable to microfabrication, which would\nenable higher simulated coupling rates, as well as interactions based on\nmagnetic forces generated by on-chip currents. Working in an 11 K cryogenic\nenvironment, we experimentally verify to within 5% a numerical model of the\nstructure of ion crystals in the trap. We also explore the possibility of\nimplementing quantum simulation using magnetic forces, and calculate J-coupling\nrates on the order of 10^3 / s for an ion crystal height of 10 microns, using a\ncurrent of 1 A.'"
Isaac Chuang,Chuang_Isaac,arXiv:1008.1603,https://arxiv.org/abs/1008.1603,"b'Abstract:  We present a model as well as experimental results for a surface electrode\nradio-frequency Paul trap that has a circular electrode geometry well-suited\nfor trapping of single ions and two-dimensional planar ion crystals. The trap\ndesign is compatible with microfabrication and offers a simple method by which\nthe height of the trapped ions above the surface may be changed \\emph{in situ}.\nWe demonstrate trapping of single and few Sr+ ions over an ion height range of\n200-1000 microns for several hours under Doppler laser cooling, and use these\nto characterize the trap, finding good agreement with our model.'"
Isaac Chuang,Chuang_Isaac,arXiv:1003.1774,https://arxiv.org/abs/1003.1774,"b'Abstract:  The tensor product representation of quantum states leads to a promising\nvariational approach to study quantum phase and quantum phase transitions,\nespecially topological ordered phases which are impossible to handle with\nconventional methods due to their long range entanglement. However, an\nimportant issue arises when we use tensor product states (TPS) as variational\nstates to find the ground state of a Hamiltonian: can arbitrary variations in\nthe tensors that represent ground state of a Hamiltonian be induced by local\nperturbations to the Hamiltonian? Starting from a tensor product state which is\nthe exact ground state of a Hamiltonian with $\\mathbb{Z}_2$ topological order,\nwe show that, surprisingly, not all variations of the tensors correspond to the\nvariation of the ground state caused by local perturbations of the Hamiltonian.\nEven in the absence of any symmetry requirement of the perturbed Hamiltonian,\none necessary condition for the variations of the tensors to be physical is\nthat they respect certain $\\mathbb{Z}_2$ symmetry. We support this claim by\ncalculating explicitly the change in topological entanglement entropy with\ndifferent variations in the tensors. This finding will provide important\nguidance to numerical variational study of topological phase and phase\ntransitions. It is also a crucial step in using TPS to study universal\nproperties of a quantum phase and its topological order.'"
Isaac Chuang,Chuang_Isaac,arXiv:1002.0085,https://arxiv.org/abs/1002.0085,"b'Abstract:  Entanglement, as studied in quantum information science, and non-local\nquantum correlations, as studied in condensed matter physics, are fundamentally\nakin to each other. However, their relationship is often hard to quantify due\nto the lack of a general approach to study both on the same footing. In\nparticular, while entanglement and non-local correlations are properties of\nstates, both arise from symmetries of global operators that commute with the\nsystem Hamiltonian. Here, we introduce a framework for completely classifying\nthe local and non-local properties of all such global operators, given the\nHamiltonian and a bi-partitioning of the system. This framework is limited to\ndescriptions based on stabilizer quantum codes, but may be generalized. We\nillustrate the use of this framework to study entanglement and non-local\ncorrelations by analyzing global symmetries in topological order, distribution\nof entanglement and entanglement entropy.'"
Isaac Chuang,Chuang_Isaac,arXiv:0912.4892,https://arxiv.org/abs/0912.4892,"b'Abstract:  We demonstrate quantum control techniques for a single trapped ion in a\ncryogenic, surface-electrode trap. A narrow optical transition of Sr+ along\nwith the ground and first excited motional states of the harmonic trapping\npotential form a two-qubit system. The optical qubit transition is susceptible\nto magnetic field fluctuations, which we stabilize with a simple and compact\nmethod using superconducting rings. Decoherence of the motional qubit is\nsuppressed by the cryogenic environment. AC Stark shift correction is\naccomplished by controlling the laser phase in the pulse sequencer, eliminating\nthe need for an additional laser. Quantum process tomography is implemented on\natomic and motional states using conditional pulse sequences. With these\ntechniques we demonstrate a Cirac-Zoller Controlled-NOT gate in a single ion\nwith a mean fidelity of 91(1)%.'"
Isaac Chuang,Chuang_Isaac,arXiv:0910.4129,https://arxiv.org/abs/0910.4129,"b'Abstract:  Graphs are closely related to quantum error-correcting codes: every\nstabilizer code is locally equivalent to a graph code, and every codeword\nstabilized code can be described by a graph and a classical code. For the\nconstruction of good quantum codes of relatively large block length,\nconcatenated quantum codes and their generalizations play an important role. We\ndevelop a systematic method for constructing concatenated quantum codes based\non ""graph concatenation"", where graphs representing the inner and outer codes\nare concatenated via a simple graph operation called ""generalized local\ncomplementation."" Our method applies to both binary and non-binary concatenated\nquantum codes as well as their generalizations.'"
Isaac Chuang,Chuang_Isaac,arXiv:0905.0148,https://arxiv.org/abs/0905.0148,"b'Abstract:  We report a demonstration and quantitative characterization of\none-dimensional cavity cooling of a single trapped 88Sr+ ion in the resolved\nsideband regime. We measure the spectrum of cavity transitions, the rates of\ncavity heating and cooling, and the steady-state cooling limit. The cavity\ncooling dynamics and cooling limit of 22.5(3) motional quanta, limited by the\nmoderate coupling between the ion and the cavity, are consistent with a simple\nmodel [Phys. Rev. A 64, 033405] without any free parameters, validating the\nrate equation model for cavity cooling.'"
Isaac Chuang,Chuang_Isaac,arXiv:0812.4067,https://arxiv.org/abs/0812.4067,"b'Abstract:  Many-body entangled quantum states studied in condensed matter physics can be\nprimary resources for quantum information, allowing any quantum computation to\nbe realized using measurements alone, on the state. Such a universal state\nwould be remarkably valuable, if only it were thermodynamically stable and\nexperimentally accessible, by virtue of being the unique ground state of a\nphysically reasonable Hamiltonian made of two-body, nearest neighbor\ninteractions. We introduce such a state, composed of six-state particles on a\nhexagonal lattice, and describe a general method for analyzing its properties\nbased on its projected entangled pair state representation.'"
Isaac Chuang,Chuang_Isaac,arXiv:0811.2422,https://arxiv.org/abs/0811.2422,b'Abstract:  Dense array of ions in microfabricated traps represent one possible way to\nscale up ion trap quantum computing. The ability to address individual ions is\nan important component of such a scheme. We demonstrate individual addressing\nof trapped ions in a microfabricated surface-electrode trap using a magnetic\nfield gradient generated on-chip. A frequency splitting of 310(2) kHz for two\nions separated by 5 um is achieved. Selective single qubit operations are\nperformed on one of two trapped ions with an average of 2.2+/-1.0% crosstalk.\nCoherence time as measured by the spin-echo technique is unaffected by the\nfield gradient.'
Isaac Chuang,Chuang_Isaac,arXiv:0809.2824,https://arxiv.org/abs/0809.2824,"b'Abstract:  Quantum simulations of spin systems could enable the solution of problems\nwhich otherwise require infeasible classical resources. Such a simulation may\nbe implemented using a well-controlled system of effective spins, such as a\ntwo-dimensional lattice of locally interacting ions. We propose here a layered\nplanar rf trap design that can be used to create arbitrary two-dimensional\nlattices of ions. The design also leads naturally to ease of microfabrication.\nAs a first experimental demonstration, we confine strontium-88 ions in a\nmm-scale lattice trap and verify numerical models of the trap by measuring the\nmotional frequencies. We also confine 440 nm diameter charged microspheres and\nobserve ion-ion repulsion between ions in neighboring lattice sites. Our\ndesign, when scaled to smaller ion-ion distances, is appropriate for quantum\nsimulation schemes, e.g. that of Porras and Cirac (PRL 92 207901 (2004)). We\nnote, however, that in practical realizations of the trap, an increase in the\nsecular frequency with decreasing ion spacing may make a coupling rate that is\nlarge relative to the decoherence rate in such a trap difficult to achieve.'"
Isaac Chuang,Chuang_Isaac,arXiv:0808.3086,https://arxiv.org/abs/0808.3086,"b'Abstract:  The codeword stabilized (CWS) quantum codes formalism presents a unifying\napproach to both additive and nonadditive quantum error-correcting codes\n(arXiv:0708.1021 [quant-ph]), but only for binary states. Here we generalize\nthe CWS framework to the nonbinary case (of both prime and nonprime dimension)\nand map the search for nonbinary quantum codes to a corresponding search\nproblem for classical nonbinary codes with specific error patterns. We show\nthat while the additivity properties of nonbinary CWS codes are similar to the\nbinary case, the structural properties of the nonbinary codes differ\nsubstantially from the binary case, even for prime dimensions. In particular,\nwe identify specific structure patterns of stabilizer groups, based on which\nefficient constructions might be possible for codes that encode more dimensions\nthan any stabilizer codes of the same length and distance; similar methods\ncannot be applied in the binary case. Understanding of these structural\nproperties can help prune the search space and facilitate the identification of\ngood nonbinary CWS codes.'"
Isaac Chuang,Chuang_Isaac,arXiv:0804.2665,https://arxiv.org/abs/0804.2665,"b'Abstract:  Electric field noise from fluctuating patch potentials is a significant\nproblem for a broad range of precision experiments, including trapped ion\nquantum computation and single spin detection. Recent results demonstrated\nstrong suppression of this noise by cryogenic cooling, suggesting an underlying\nthermal process. We present measurements characterizing the temperature and\nfrequency dependence of the noise from 7 to 100 K, using a single Sr+ ion\ntrapped 75 um above the surface of a gold plated surface electrode ion trap.\nThe noise amplitude is observed to have an approximate 1/f spectrum around 1\nMHz, and grows rapidly with temperature as T^beta for beta from 2 to 4. The\ndata are consistent with microfabricated cantilever measurements of non-contact\nfriction but do not extrapolate to the DC measurements with neutral atoms or\ncontact potential probes.'"
Isaac Chuang,Chuang_Isaac,arXiv:0803.3232,https://arxiv.org/abs/0803.3232,"b'Abstract:  The codeword stabilized (""CWS"") quantum codes formalism presents a unifying\napproach to both additive and nonadditive quantum error-correcting codes\n(arXiv:0708.1021). This formalism reduces the problem of constructing such\nquantum codes to finding a binary classical code correcting an error pattern\ninduced by a graph state. Finding such a classical code can be very difficult.\nHere, we consider an algorithm which maps the search for CWS codes to a problem\nof identifying maximum cliques in a graph. While solving this problem is in\ngeneral very hard, we prove three structure theorems which reduce the search\nspace, specifying certain admissible and optimal ((n,K,d)) additive codes. In\nparticular, we find there does not exist any ((7,3,3)) CWS code though the\nlinear programming bound does not rule it out. The complexity of the CWS search\nalgorithm is compared with the contrasting method introduced by Aggarwal and\nCalderbank (arXiv:cs/0610159).'"
Isaac Chuang,Chuang_Isaac,arXiv:0801.2360,https://arxiv.org/abs/0801.2360,"b'Abstract:  A long-standing open problem in fault-tolerant quantum computation has been\nto find a universal set of transversal gates. As three of us proved in arXiv:\n0706.1382, such a set does not exist for binary stabilizer codes. Here we\ngeneralize our work to show that for subsystem stabilizer codes in $d$\ndimensional Hilbert space, such a universal set of transversal gates cannot\nexist for even one encoded qudit, for any dimension $d$, prime or nonprime.\nThis result strongly supports the idea that other primitives, such as quantum\nteleportation, are necessary for universal fault-tolerant quantum computation,\nand may be an important factor for fault tolerance noise thresholds.'"
Isaac Chuang,Chuang_Isaac,arXiv:0712.2084,https://arxiv.org/abs/0712.2084,"b""Abstract:  Teleportation is a crucial element in fault-tolerant quantum computation and\na complete understanding of its capacity is very important for the practical\nimplementation of optimal fault-tolerant architectures. It is known that\nstabilizer codes support a natural set of gates that can be more easily\nimplemented by teleportation than any other gates. These gates belong to the so\ncalled $\\mathcal{C}_k$ hierarchy introduced by Gottesman and Chuang (Nature\n\\textbf{402}, 390). Moreover, a subset of $\\mathcal{C}_k$ gates, called\nsemi-Clifford operations, can be implemented by an even simpler architecture\nthan the traditional teleportation setup (Phys. Rev. \\textbf{A62}, 052316).\nHowever, the precise set of gates in $\\mathcal{C}_k$ remains unknown, even for\na fixed number of qubits $n$, which prevents us from knowing exactly what\nteleportation is capable of. In this paper we study the structure of\n$\\mathcal{C}_k$ in terms of semi-Clifford operations, which send by conjugation\nat least one maximal abelian subgroup of the $n$-qubit Pauli group into another\none. We show that for $n=1,2$, all the $\\mathcal{C}_k$ gates are semi-Clifford,\nwhich is also true for $\\{n=3,k=3\\}$. However, this is no longer true for\n$\\{n>2,k>3\\}$. To measure the capability of this teleportation primitive, we\nintroduce a quantity called `teleportation depth', which characterizes how many\nteleportation steps are necessary, on average, to implement a given gate. We\ncalculate upper bounds for teleportation depth by decomposing gates into both\nsemi-Clifford $\\mathcal{C}_k$ gates and those $\\mathcal{C}_k$ gates beyond\nsemi-Clifford operations, and compare their efficiency."""
Isaac Chuang,Chuang_Isaac,arXiv:0706.3763,https://arxiv.org/abs/0706.3763,"b'Abstract:  Dense arrays of trapped ions provide one way of scaling up ion trap quantum\ninformation processing. However, miniaturization of ion traps is currently\nlimited by sharply increasing motional state decoherence at sub-100 um\nion-electrode distances. We characterize heating rates in cryogenically cooled\nsurface-electrode traps, with characteristic sizes in 75 um to 150 um range.\nUpon cooling to 6 K, the measured rates are suppressed by 7 orders of\nmagnitude, two orders of magnitude below previously published data of similarly\nsized traps operated at room temperature. The observed noise depends strongly\non fabrication process, which suggests further improvements are possible.'"
Isaac Chuang,Chuang_Isaac,arXiv:0706.3374,https://arxiv.org/abs/0706.3374,"b'Abstract:  We demonstrate loading by laser ablation of $^{88}$Sr$^+$ ions into a\nmm-scale surface-electrode ion trap. The laser used for ablation is a pulsed,\nfrequency-tripled Nd:YAG with pulse energies of 1-10 mJ and durations of 3-5\nns. An additional laser is not required to photoionize the ablated material.\nThe efficiency and lifetime of several candidate materials for the laser\nablation target are characterized by measuring the trapped ion fluorescence\nsignal for a number of consecutive loads. Additionally, laser ablation is used\nto load traps with a trap depth (40 meV) below where electron impact ionization\nloading is typically successful ($\\gtrsim$ 500 meV).'"
Isaac Chuang,Chuang_Isaac,arXiv:0706.1382,https://arxiv.org/abs/0706.1382,"b'Abstract:  Certain quantum codes allow logic operations to be performed on the encoded\ndata, such that a multitude of errors introduced by faulty gates can be\ncorrected. An important class of such operations are {\\em transversal}, acting\nbitwise between corresponding qubits in each code block, thus allowing error\npropagation to be carefully limited. If any quantum operation could be\nimplemented using a set of such gates, the set would be {\\em universal}; codes\nwith such a universal, transversal gate set have been widely desired for\nefficient fault-tolerant quantum computation. We study the structure of\nGF(4)-additive quantum codes and prove that no universal set of transversal\nlogic operations exists for these codes. This result strongly supports the idea\nthat additional primitive operations, based for example on quantum\nteleportation, are necessary to achieve universal fault-tolerant computation on\nadditive codes.'"
Isaac Chuang,Chuang_Isaac,arXiv:physics/0702025,https://arxiv.org/abs/physics/0702025,"b'Abstract:  We produce large numbers of low-energy ions by photoionization of\nlaser-cooled atoms inside a surface-electrode-based Paul trap. The\nisotope-selective trap loading rate of $4\\times10^{5}$ Yb$^{+}$ ions/s exceeds\nthat attained by photoionization (electron impact ionization) of an atomic beam\nby four (six) orders of magnitude. Traps as shallow as 0.13 eV are easily\nloaded with this technique. The ions are confined in the same spatial region as\nthe laser-cooled atoms, which will allow the experimental investigation of\ninteractions between cold ions and cold atoms or Bose-Einstein condensates.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0611214,https://arxiv.org/abs/quant-ph/0611214,"b""Abstract:  The equivalence of stabilizer states under local transformations is of\nfundamental interest in understanding properties and uses of entanglement. Two\nstabilizer states are equivalent under the usual stochastic local operations\nand classical communication criterion if and only if they are equivalent under\nlocal unitary (LU) operations. More surprisingly, under certain conditions, two\nLU equivalent stabilizer states are also equivalent under local Clifford (LC)\noperations, as was shown by Van den Nest et al. [Phys. Rev. \\textbf{A71},\n062323]. Here, we broaden the class of stabilizer states for which LU\nequivalence implies LC equivalence ($LU\\Leftrightarrow LC$) to include all\nstabilizer states represented by graphs with neither cycles of length 3 nor 4.\nTo compare our result with Van den Nest et al.'s, we show that any stabilizer\nstate of distance $\\delta=2$ is beyond their criterion. We then further prove\nthat $LU\\Leftrightarrow LC$ holds for a more general class of stabilizer states\nof $\\delta=2$. We also explicitly construct graphs representing $\\delta>2$\nstabilizer states which are beyond their criterion: we identify all 58 graphs\nwith up to 11 vertices and construct graphs with $2^m-1$ ($m\\geq 4$) vertices\nusing quantum error correcting codes which have non-Clifford transversal gates."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0604070,https://arxiv.org/abs/quant-ph/0604070,"b""Abstract:  The assumption of maximum parallelism support for the successful realization\nof scalable quantum computers has led to homogeneous, ``sea-of-qubits''\narchitectures. The resulting architectures overcome the primary challenges of\nreliability and scalability at the cost of physically unacceptable system area.\nWe find that by exploiting the natural serialization at both the application\nand the physical microarchitecture level of a quantum computer, we can reduce\nthe area requirement while improving performance. In particular we present a\nscalable quantum architecture design that employs specialization of the system\ninto memory and computational regions, each individually optimized to match\nhardware support to the available parallelism. Through careful application and\nsystem analysis, we find that our new architecture can yield up to a factor of\nthirteen savings in area due to specialization. In addition, by providing a\nmemory hierarchy design for quantum computers, we can increase time performance\nby a factor of eight. This result brings us closer to the realization of a\nquantum processor that can solve meaningful problems."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0603142,https://arxiv.org/abs/quant-ph/0603142,"b'Abstract:  We demonstrate a method for loading surface electrode ion traps by electron\nimpact ionization. The method relies on the property of surface electrode\ngeometries that the trap depth can be increased at the cost of more\nmicromotion. By introducing a buffer gas, we can counteract the rf heating\nassocated with the micromotion and benefit from the larger trap depth. After an\ninitial loading of the trap, standard compensation techniques can be used to\ncancel the stray fields resulting from charged dielectric and allow for the\nloading of the trap at ultra-high vacuum.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0601021,https://arxiv.org/abs/quant-ph/0601021,"b'Abstract:  Quantum simulation uses a well-known quantum system to predict the behavior\nof another quantum system. Certain limitations in this technique arise,\nhowever, when applied to specific problems, as we demonstrate with a\ntheoretical and experimental study of an algorithm to find the low-lying\nspectrum of a Hamiltonian. While the number of elementary quantum gates does\nscale polynomially with the size of the system, it increases inversely to the\ndesired error bound $\\epsilon$. Making such simulations robust to decoherence\nusing fault-tolerance constructs requires an additional factor of $1/ \\epsilon$\ngates. These constraints are illustrated by using a three qubit nuclear\nmagnetic resonance system to simulate a pairing Hamiltonian, following the\nalgorithm proposed by Wu, Byrd, and Lidar.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0601001,https://arxiv.org/abs/quant-ph/0601001,"b'Abstract:  We present an efficient family of quantum circuits for a fundamental\nprimitive in quantum information theory, the Schur transform. The Schur\ntransform on n d-dimensional quantum systems is a transform between a standard\ncomputational basis to a labelling related to the representation theory of the\nsymmetric and unitary groups. If we desire to implement the Schur transform to\nan accuracy of epsilon, then our circuit construction uses a number of gates\nwhich is polynomial in n, d and log(1/epsilon). The important insights we use\nto perform this construction are the selection of the appropriate subgroup\nadapted basis and the Wigner-Eckart theorem. Our efficient circuit construction\nrenders numerous protocols in quantum information theory computationally\ntractable and is an important new efficient quantum circuit family which goes\nsignificantly beyond the standard paradigm of the quantum Fourier transform.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0509051,https://arxiv.org/abs/quant-ph/0509051,"b'Abstract:  Recent experimental advances have demonstrated technologies capable of\nsupporting scalable quantum computation. A critical next step is how to put\nthose technologies together into a scalable, fault-tolerant system that is also\nfeasible. We propose a Quantum Logic Array (QLA) microarchitecture that forms\nthe foundation of such a system. The QLA focuses on the communication resources\nnecessary to efficiently support fault-tolerant computations. We leverage the\nextensive groundwork in quantum error correction theory and provide analysis\nthat shows that our system is both asymptotically and empirically fault\ntolerant. Specifically, we use the QLA to implement a hierarchical, array-based\ndesign and a logarithmic expense quantum-teleportation communication protocol.\nOur goal is to overcome the primary scalability challenges of reliability,\ncommunication, and quantum resource distribution that plague current proposals\nfor large-scale quantum computing.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0409170,https://arxiv.org/abs/quant-ph/0409170,"b'Abstract:  The role of mixed state entanglement in liquid-state nuclear magnetic\nresonance (NMR) quantum computation is not yet well-understood. In particular,\ndespite the success of quantum information processing with NMR, recent work has\nshown that quantum states used in most of those experiments were not entangled.\nThis is because these states, derived by unitary transforms from the thermal\nequilibrium state, were too close to the maximally mixed state. We are thus\nmotivated to determine whether a given NMR state is entanglable - that is, does\nthere exist a unitary transform that entangles the state? The boundary between\nentanglable and nonentanglable thermal states is a function of the spin system\nsize $N$ and its temperature $T$. We provide new bounds on the location of this\nboundary using analytical and numerical methods; our tightest bound scales as\n$N \\sim T$, giving a lower bound requiring at least $N \\sim 22,000$ proton\nspins to realize an entanglable thermal state at typical laboratory NMR\nmagnetic fields. These bounds are tighter than known bounds on the\nentanglability of effective pure states.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0408166,https://arxiv.org/abs/quant-ph/0408166,"b'Abstract:  Nuclear Magnetic Resonance (NMR) has provided a valuable experimental testbed\nfor quantum information processing (QIP). Here, we briefly review the use of\nnuclear spins as qubits, and discuss the current status of NMR-QIP. Advances in\nthe techniques available for control are described along with the various\nimplementations of quantum algorithms and quantum simulations that have been\nperformed using NMR. The recent application of NMR control techniques to other\nquantum computing systems are reviewed before concluding with a description of\nthe efforts currently underway to transition to solid state NMR systems that\nhold promise for scalable architectures.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0407082,https://arxiv.org/abs/quant-ph/0407082,"b'Abstract:  The Schur basis on n d-dimensional quantum systems is a generalization of the\ntotal angular momentum basis that is useful for exploiting symmetry under\npermutations or collective unitary rotations. We present efficient (size\npoly(n,d,log(1/\\epsilon)) for accuracy \\epsilon) quantum circuits for the Schur\ntransform, which is the change of basis between the computational and the Schur\nbases. These circuits are based on efficient circuits for the Clebsch-Gordan\ntransformation. We also present an efficient circuit for a limited version of\nthe Schur transform in which one needs only to project onto different Schur\nsubspaces. This second circuit is based on a generalization of phase estimation\nto any nonabelian finite group for which there exists a fast quantum Fourier\ntransform.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0407022,https://arxiv.org/abs/quant-ph/0407022,"b'Abstract:  Systematic errors in quantum operations can be the dominating source of\nimperfection in achieving control over quantum systems. This problem, which has\nbeen well studied in nuclear magnetic resonance, can be addressed by replacing\nsingle operations with composite sequences of pulsed operations, which cause\nerrors to cancel by symmetry. Remarkably, this can be achieved without\nknowledge of the amount of error epsilon. Independent of the initial state of\nthe system, current techniques allow the error to be reduced to O(epsilon^3).\nHere, we extend the composite pulse technique to cancel errors to O(epsilon^n),\nfor arbitrary n.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0406168,https://arxiv.org/abs/quant-ph/0406168,"b'Abstract:  We define a multi-partite entanglement measure for stabilizer states, which\ncan be computed efficiently from a set of generators of the stabilizer group.\nOur measure applies to qubits, qudits and continuous variables.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0404064,https://arxiv.org/abs/quant-ph/0404064,"b'Abstract:  Fifty years of developments in nuclear magnetic resonance (NMR) have resulted\nin an unrivaled degree of control of the dynamics of coupled two-level quantum\nsystems. This coherent control of nuclear spin dynamics has recently been taken\nto a new level, motivated by the interest in quantum information processing.\nNMR has been the workhorse for the experimental implementation of quantum\nprotocols, allowing exquisite control of systems up to seven qubits in size.\nHere, we survey and summarize a broad variety of pulse control and tomographic\ntechniques which have been developed for and used in NMR quantum computation.\nMany of these will be useful in other quantum systems now being considered for\nimplementation of quantum information processing tasks.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0302057,https://arxiv.org/abs/quant-ph/0302057,b'Abstract:  We report the realization of a nuclear magnetic resonance computer with three\nquantum bits that simulates an adiabatic quantum optimization algorithm.\nAdiabatic quantum algorithms offer new insight into how quantum resources can\nbe used to solve hard problems. This experiment uses a particularly well suited\nthree quantum bit molecule and was made possible by introducing a technique\nthat encodes general instances of the given optimization problem into an easily\napplicable Hamiltonian. Our results indicate an optimal run time of the\nadiabatic algorithm that agrees well with the prediction of a simple\ndecoherence model.'
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0112176,https://arxiv.org/abs/quant-ph/0112176,"b""Abstract:  The number of steps any classical computer requires in order to find the\nprime factors of an $l$-digit integer $N$ increases exponentially with $l$, at\nleast using algorithms known at present. Factoring large integers is therefore\nconjectured to be intractable classically, an observation underlying the\nsecurity of widely used cryptographic codes. Quantum computers, however, could\nfactor integers in only polynomial time, using Shor's quantum factoring\nalgorithm. Although important for the study of quantum computers, experimental\ndemonstration of this algorithm has proved elusive. Here we report an\nimplementation of the simplest instance of Shor's algorithm: factorization of\n${N=15}$ (whose prime factors are 3 and 5). We use seven spin-1/2 nuclei in a\nmolecule as quantum bits, which can be manipulated with room temperature liquid\nstate nuclear magnetic resonance techniques. This method of using nuclei to\nstore quantum information is in principle scalable to many quantum bit systems,\nbut such scalability is not implied by the present work. The significance of\nour work lies in the demonstration of experimental and theoretical techniques\nfor precise control and modelling of complex quantum computers. In particular,\nwe present a simple, parameter-free but predictive model of decoherence effects\nin our system."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0111031,https://arxiv.org/abs/quant-ph/0111031,"b'Abstract:  Quantum compiling addresses the problem of approximating an arbitrary quantum\ngate with a string of gates drawn from a particular finite set. It has been\nshown that this is possible for almost all choices of base sets and furthermore\nthat the number of gates required for precision epsilon is only polynomial in\nlog 1/epsilon. Here we prove that using certain sets of base gates quantum\ncompiling requires a string length that is linear in log 1/epsilon, a result\nwhich matches the lower bound from counting volume up to constant factor.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0105147,https://arxiv.org/abs/quant-ph/0105147,"b""Abstract:  Current experiments in liquid-state nuclear magnetic resonance quantum\ncomputing are limited by low initial polarization. To address this problem, we\nhave investigated the use of optical pumping techniques to enhance the\npolarization of a 2-qubit NMR quantum computer (13C and 1H in 13CHCl3). To\nefficiently use the increased polarization, we have generalized the procedure\nfor effective pure state preparation. With this new, more flexible scheme, an\neffective pure state was prepared with polarization-enhancement of a factor of\n10 compared to the thermal state. An implementation of Grover's quantum search\nalgorithm was demonstrated using this new technique."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0105032,https://arxiv.org/abs/quant-ph/0105032,"b'Abstract:  We present a quantum digital signature scheme whose security is based on\nfundamental principles of quantum physics. It allows a sender (Alice) to sign a\nmessage in such a way that the signature can be validated by a number of\ndifferent people, and all will agree either that the message came from Alice or\nthat it has been tampered with. To accomplish this task, each recipient of the\nmessage must have a copy of Alice\'s ""public key,"" which is a set of quantum\nstates whose exact identity is known only to Alice. Quantum public keys are\nmore difficult to deal with than classical public keys: for instance, only a\nlimited number of copies can be in circulation, or the scheme becomes insecure.\nHowever, in exchange for this price, we achieve unconditionally secure digital\nsignatures. Sending an m-bit message uses up O(m) quantum bits for each\nrecipient of the public key. We briefly discuss how to securely distribute\nquantum public keys, and show the signature scheme is absolutely secure using\none method of key distribution. The protocol provides a model for importing the\nideas of classical public key cryptography into the quantum world.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0012108,https://arxiv.org/abs/quant-ph/0012108,"b'Abstract:  This in an introduction on quantum computing and on the use of NMR to build\nquantum computers, geared towards an NMR audience.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0012032,https://arxiv.org/abs/quant-ph/0012032,"b'Abstract:  Quantum process tomography is a procedure by which the unknown dynamical\nevolution of an open quantum system can be fully experimentally characterized.\nWe demonstrate explicitly how this procedure can be implemented with a nuclear\nmagnetic resonance quantum computer. This allows us to measure the fidelity of\na controlled-not logic gate and to experimentally investigate the error model\nfor our computer. Based on the latter analysis, we test an important assumption\nunderlying nearly all models of quantum error correction, the independence of\nerrors on different qubits.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0008070,https://arxiv.org/abs/quant-ph/0008070,"b'Abstract:  Although the conditions for performing arbitrary unitary operations to\nsimulate the dynamics of a closed quantum system are well understood, the same\nis not true of the more general class of quantum operations (also known as\nsuperoperators) corresponding to the dynamics of open quantum systems. We\npropose a framework for the generation of Markovian quantum dynamics and study\nthe resources needed for universality. For the case of a single qubit, we show\nthat a single nonunitary process is necessary and sufficient to generate all\nunital Markovian quantum dynamics, whereas a set of processes parametrized by\none continuous parameter is needed in general. We also obtain preliminary\nresults for the unital case in higher dimensions.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0008065,https://arxiv.org/abs/quant-ph/0008065,"b'Abstract:  Although the initial proposal for ion trap quantum computation made use of an\nauxiliary internal level to perform logic between ions, this resource is not\nnecessary in principle. Instead, one may perform such operations directly using\nsideband laser pulses, operating with an arbitrary (sufficiently small)\nLamb-Dicke parameter. We explore the potential of this technique, showing how\nto perform logical operations between the internal state of an ion and the\ncollective motional state and giving explicit constructions for a\ncontrolled-not gate between ions.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0007017,https://arxiv.org/abs/quant-ph/0007017,"b""Abstract:  We report the realization of a nuclear magnetic resonance (NMR) quantum\ncomputer which combines the quantum Fourier transform (QFT) with exponentiated\npermutations, demonstrating a quantum algorithm for order-finding. This\nalgorithm has the same structure as Shor's algorithm and its speed-up over\nclassical algorithms scales exponentially. The implementation uses a\nparticularly well-suited five quantum bit molecule and was made possible by a\nnew state initialization procedure and several quantum control techniques."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0005092,https://arxiv.org/abs/quant-ph/0005092,"b'Abstract:  The clock synchronization problem is to determine the time difference\n$\\Delta$ between two spatially separated clocks. When message delivery times\nbetween the two clocks are uncertain, $O(2^{2n})$ classical messages must be\nexchanged between the clocks to determine $n$ digits of $\\Delta$. On the other\nhand, as we show, there exists a quantum algorithm to obtain $n$ digits of\n$\\Delta$ while communicating only O(n) quantum messages.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0002039,https://arxiv.org/abs/quant-ph/0002039,"b'Abstract:  We present a general method to construct fault-tolerant quantum logic gates\nwith a simple primitive, which is an analog of quantum teleportation. The\ntechnique extends previous results based on traditional quantum teleportation\n(Gottesman and Chuang, Nature {\\bf 402}, 390, 1999) and leads to\nstraightforward and systematic construction of many fault-tolerant encoded\noperations, including the $\\pi/8$ and Toffoli gates. The technique can also be\napplied to the construction of remote quantum operations that cannot be\ndirectly performed.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9910075,https://arxiv.org/abs/quant-ph/9910075,"b""Abstract:  We report the experimental implementation of Grover's quantum search\nalgorithm on a quantum computer with three quantum bits. The computer consists\nof molecules of $^{13}$C-labeled CHFBr$_2$, in which the three weakly coupled\nspin-1/2 nuclei behave as the bits and are initialized, manipulated, and read\nout using magnetic resonance techniques. This quantum computation is made\npossible by the introduction of two techniques which significantly reduce the\ncomplexity of the experiment and by the surprising degree of cancellation of\nsystematic errors which have previously limited the total possible number of\nquantum gates."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9908010,https://arxiv.org/abs/quant-ph/9908010,"b'Abstract:  We present a method to create a variety of interesting gates by teleporting\nquantum bits through special entangled states. This allows, for instance, the\nconstruction of a quantum computer based on just single qubit operations, Bell\nmeasurements, and GHZ states. We also present straightforward constructions of\na wide variety of fault-tolerant quantum gates.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9907063,https://arxiv.org/abs/quant-ph/9907063,"b'Abstract:  Liquid crystals offer several advantages as solvents for molecules used for\nnuclear magnetic resonance quantum computing (NMRQC). The dipolar coupling\nbetween nuclear spins manifest in the NMR spectra of molecules oriented by a\nliquid crystal permits a significant increase in clock frequency, while short\nspin-lattice relaxation times permit fast recycling of algorithms, and save\ntime in calibration and signal-enhancement experiments. Furthermore, the use of\nliquid crystal solvents offers scalability in the form of an expanded library\nof spin-bearing molecules suitable for NMRQC. These ideas are demonstrated with\nthe successful execution of a 2-qubit Grover search using a molecule\n($^{13}$C$^{1}$HCl$_3$) oriented in a liquid crystal and a clock speed eight\ntimes greater than in an isotropic solvent. Perhaps more importantly, five\ntimes as many logic operations can be executed within the coherence time using\nthe liquid crystal solvent.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9906112,https://arxiv.org/abs/quant-ph/9906112,"b""Abstract:  Realistic physical implementations of quantum computers can entail tradeoffs\nwhich depart from the ideal model of quantum computation. Although these\ntradeoffs have allowed successful demonstration of certain quantum algorithms,\na crucial question is whether they fundamentally limit the computational\ncapacity of such machines. We study the limitations of a quantum computation\nmodel in which only ensemble averages of measurement observables are\naccessible. Furthermore, we stipulate that input qubits may only be prepared in\nhighly random, ``hot'' mixed states. In general, these limitations are believed\nto dramatically detract from the computational power of the system. However, we\nconstruct a class of algorithms for this limited model, which, surprisingly,\nare polynomially equivalent to the ideal case. This class includes the well\nknown Deutsch-Jozsa algorithm."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9905041,https://arxiv.org/abs/quant-ph/9905041,"b'Abstract:  We report the first use of ""logical labeling"" to perform a quantum\ncomputation with a room-temperature bulk system. This method entails the\nselection of a subsystem which behaves as if it were at zero temperature -\nexcept for a decrease in signal strength - conditioned upon the state of the\nremaining system. No averaging over differently prepared molecules is required.\nIn order to test this concept, we execute a quantum search algorithm in a\nsubspace of two nuclear spins, labeled by a third spin, using solution nuclear\nmagnetic resonance (NMR), and employing a novel choice of reference frame to\nuncouple nuclei.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9904100,https://arxiv.org/abs/quant-ph/9904100,"b'Abstract:  We present an efficient scheme which couples any designated pair of spins in\nheteronuclear spin systems. The scheme is based on the existence of Hadamard\nmatrices. For a system of $n$ spins with pairwise coupling, the scheme\nconcatenates $cn$ intervals of system evolution and uses at most $c n^2$ pulses\nwhere $c \\approx 1$. Our results demonstrate that, in many systems, selective\nrecoupling is possible with linear overhead, contrary to common speculation\nthat exponential effort is always required.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9811068,https://arxiv.org/abs/quant-ph/9811068,"b'Abstract:  Using nuclear magnetic resonance techniques, we experimentally investigated\nthe effects of applying a two bit phase error detection code to preserve\nquantum information in nuclear spin systems. Input states were stored with and\nwithout coding, and the resulting output states were compared with the\noriginals and with each other. The theoretically expected result, net reduction\nof distortion and conditional error probabilities to second order, was indeed\nobserved, despite imperfect coding operations which increased the error\nprobabilities by approximately 5%. Systematic study of the deviations from the\nideal behavior provided quantitative measures of different sources of error,\nand good agreement was found with a numerical model. Theoretical questions in\nquantum error correction in bulk nuclear spin systems including fidelity\nmeasures, signal strength and syndrome measurements are discussed.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9801037,https://arxiv.org/abs/quant-ph/9801037,"b""Abstract:  Nuclear magnetic resonance techniques are used to realize a quantum algorithm\nexperimentally. The algorithm allows a simple NMR quantum computer to determine\nglobal properties of an unknown function requiring fewer function ``calls''\nthan is possible using a classical computer."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9706053,https://arxiv.org/abs/quant-ph/9706053,"b'Abstract:  In bulk quantum computation one can manipulate a large number of\nindistinguishable quantum computers by parallel unitary operations and measure\nexpectation values of certain observables with limited sensitivity. The initial\nstate of each computer in the ensemble is known but not pure. Methods for\nobtaining effective pure input states by a series of manipulations have been\ndescribed by Gershenfeld and Chuang (logical labeling) and Cory et al. (spatial\naveraging) for the case of quantum computation with nuclear magnetic resonance.\nWe give a different technique called temporal averaging. This method is based\non classical randomization, requires no ancilla qubits and can be implemented\nin nuclear magnetic resonance without using gradient fields. We introduce\nseveral temporal averaging algorithms suitable for both high temperature and\nlow temperature bulk quantum computing and analyze the signal to noise behavior\nof each.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9703032,https://arxiv.org/abs/quant-ph/9703032,"b'Abstract:  We show how to construct quantum gate arrays that can be programmed to\nperform different unitary operations on a data register, depending on the input\nto some program register. It is shown that a universal quantum gate array - a\ngate array which can be programmed to perform any unitary operation - exists\nonly if one allows the gate array to operate in a probabilistic fashion. The\nuniversal quantum gate array we construct requires an exponentially smaller\nnumber of gates than a classical universal gate array.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9610001,https://arxiv.org/abs/quant-ph/9610001,"b'Abstract:  We give an explicit prescription for experimentally determining the evolution\noperators which completely describe the dynamics of a quantum mechanical black\nbox -- an arbitrary open quantum system. We show necessary and sufficient\nconditions for this to be possible, and illustrate the general theory by\nconsidering specifically one and two quantum bit systems. These procedures may\nbe useful in the comparative evaluation of experimental quantum measurement,\ncommunication, and computation systems.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9604031,https://arxiv.org/abs/quant-ph/9604031,"b""Abstract:  Decoherence and loss will limit the practicality of quantum cryptography and\ncomputing unless successful error correction techniques are developed. To this\nend, we have discovered a new scheme for perfectly detecting and rejecting the\nerror caused by loss (amplitude damping to a reservoir at T=0), based on using\na dual-rail representation of a quantum bit. This is possible because (1)\nbalanced loss does not perform a ``which-path'' measurement in an\ninterferometer, and (2) balanced quantum nondemolition measurement of the\n``total'' photon number can be used to detect loss-induced quantum jumps\nwithout disturbing the quantum coherence essential to the quantum bit. Our\nresults are immediately applicable to optical quantum computers using single\nphotonics devices."""
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9604030,https://arxiv.org/abs/quant-ph/9604030,"b'Abstract:  The construction of large, coherent quantum systems necessary for quantum\ncomputation remains an entreating but elusive goal, due to the ubiquitous\nnature of decoherence. Recent progress in quantum error correction schemes have\ngiven new hope to this field, but thus far, the codes presented in the\nliterature assume a restricted number of errors and error free encoding,\ndecoding, and measurement. We investigate a specific scenario without these\nassumptions; in particular, we evaluate a scheme to preserve a single quantum\nbit against phase damping using a three-qubit encoding based on Shor. By\napplying a new formalism which gives simple operators for decoherence and noisy\nlogic gates, we find the fidelity of the stored qubit as a function of time,\nincluding decoherence which occurs not only during storage but also during\nprocessing. We generalize our results to include any source of error, and\nderive an upper limit on the allowable decoherence per timestep. Physically,\nour results suggest the feasibility of engineering artificial metastable states\nthrough repeated error correction.'"
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9602018,https://arxiv.org/abs/quant-ph/9602018,"b'Abstract:  We investigate the impact of loss (amplitude damping) and decoherence (phase\ndamping) on the performance of a simple quantum computer which solves the\none-bit Deutsch problem. The components of this machine are beamsplitters and\nnonlinear optical Kerr cells, but errors primarily originate from the latter.\nWe develop models to describe the effect of these errors on a quantum optical\nFredkin gate. The results are used to analyze possible error correction\nstrategies in a complete quantum computer. We find that errors due to loss can\nbe avoided perfectly by appropriate design techniques, while decoherence can be\npartially dealt with using projective error correction.'"
David Clark,Clark_David,arXiv:1806.10235,https://arxiv.org/abs/1806.10235,"b""Abstract:  Traditional program analysis analyses a program language, that is, all\nprograms that can be written in the language. There is a difference, however,\nbetween all possible programs that can be written and the corpus of actual\nprograms written in a language. We seek to exploit this difference: for a given\nprogram, we apply a bespoke program transformation Indexify to convert\nexpressions that current SMT solvers do not, in general, handle, such as\nconstraints on strings, into equisatisfiable expressions that they do handle.\nTo this end, Indexify replaces operators in hard-to-handle expressions with\nhomomorphic versions that behave the same on a finite subset of the domain of\nthe original operator, and return bottom denoting unknown outside of that\nsubset. By focusing on what literals and expressions are most useful for\nanalysing a given program, Indexify constructs a small, finite theory that\nextends the power of a solver on the expressions a target program builds.\nIndexify's bespoke nature necessarily means that its evaluation must be\nexperimental, resting on a demonstration of its effectiveness in practice. We\nhave developed Indexif}, a tool for Indexify. We demonstrate its utility and\neffectiveness by applying it to two real world benchmarks --- string\nexpressions in coreutils and floats in fdlibm53. Indexify reduces\ntime-to-completion on coreutils from Klee's 49.5m on average to 6.0m. It\nincreases branch coverage on coreutils from 30.10% for Klee and 14.79% for\nZesti to 66.83%. When indexifying floats in fdlibm53, Indexifyl increases\nbranch coverage from 34.45% to 71.56% over Klee. For a restricted class of\ninputs, Indexify permits the symbolic execution of program paths unreachable\nwith previous techniques: it covers more than twice as many branches in\ncoreutils as Klee."""
David Clark,Clark_David,arXiv:1805.08889,https://arxiv.org/abs/1805.08889,"b""Abstract:  Neuromorphic architectures achieve low-power operation by using many simple\nspiking neurons in lieu of traditional hardware. Here, we develop methods for\nprecise linear computations in spiking neural networks and use these methods to\nmap the evolution of a linear dynamical system (LDS) onto an existing\nneuromorphic chip: IBM's TrueNorth. We analytically characterize, and\nnumerically validate, the discrepancy between the spiking LDS state sequence\nand that of its non-spiking counterpart. These analytical results shed light on\nthe multiway tradeoff between time, space, energy, and accuracy in neuromorphic\ncomputation. To demonstrate the utility of our work, we implemented a\nneuromorphic Kalman filter (KF) and used it for offline decoding of human vocal\npitch from neural data. The neuromorphic KF could be used for low-power\nfiltering in domains beyond neuroscience, such as navigation or robotics."""
David Clark,Clark_David,arXiv:1711.06355,https://arxiv.org/abs/1711.06355,"b'Abstract:  We present a study of comet C/2017 K2 (PANSTARRS) using prediscovery archival\ndata taken from 2013 to 2017. Our measurements show that the comet has been\nmarginally increasing in activity since at least 2013 May (heliocentric\ndistance of $r_{\\mathrm{H}} = 23.7$ AU pre-perihelion). We estimate the\nmass-loss rate during the period 2013--2017 as $\\overline{\\dot{M}} \\approx\n\\left(2.4 \\pm 1.1 \\right) \\times 10^{2}$ kg s$^{-1}$, which requires a minimum\nactive surface area of $\\sim$10--10$^2$ km$^{2}$ for sublimation of\nsupervolatiles such as CO and CO$_2$, by assuming a nominal cometary albedo\n$p_V = 0.04 \\pm 0.02$. The corresponding lower limit to the nucleus radius is a\nfew kilometers. Our Monte Carlo dust simulations show that dust grains in the\ncoma are $\\gtrsim0.5$ mm in radius, with ejection speeds from $\\sim$1--3 m\ns$^{-1}$, and have been emitted in a protracted manner since 2013, confirming\nestimates by Jewitt et al. (2017). The current heliocentric orbit is\nhyperbolic. Our N-body backward dynamical integration of the orbit suggests\nthat the comet is most likely (with a probability of $\\sim$98\\%) from the Oort\nspike. The calculated median reciprocal of the semimajor axis 1 Myr ago was\n$a_{\\mathrm{med}}^{-1} = \\left( 3.61 \\pm 1.71 \\right) \\times 10^{-5}$ AU$^{-1}$\n(in a reference system of the solar-system barycentre).'"
David Clark,Clark_David,arXiv:1609.02404,https://arxiv.org/abs/1609.02404,"b'Abstract:  Malware creators have been getting their way for too long now. String-based\nsimilarity measures can leverage ground truth in a scalable way and can operate\nat a level of abstraction that is difficult to combat from the code level. We\nintroduce ITect, a scalable approach to malware similarity detection based on\ninformation theory. ITect targets file entropy patterns in different ways to\nachieve 100% precision with 90% accuracy but it could target 100% recall\ninstead. It outperforms VirusTotal for precision and accuracy on combined\nKaggle and VirusShare malware.'"
David Clark,Clark_David,arXiv:1602.03123,https://arxiv.org/abs/1602.03123,"b'Abstract:  Temporarily Captured Orbiters (TCOs) are Near-Earth Objects (NEOs) which make\na few orbits of Earth before returning to heliocentric orbits. Only one TCO has\nbeen observed to date, 2006 RH120, captured by Earth for one year before\nescaping. Detailed modeling predicts capture should occur from the NEO\npopulation predominantly through the Sun-Earth L1 and L2 points, with 1% of\nTCOs impacting Earth and approximately 0.1% of meteoroids being TCOs. Although\nthousands of meteoroid orbits have been measured, none until now have\nconclusively exhibited TCO behaviour, largely due to difficulties in measuring\ninitial meteoroid speed with sufficient precision. We report on a precise\nmeteor observation of January 13, 2014 by a new generation of all-sky fireball\ndigital camera systems operated in the Czech Republic as part of the European\nFireball Network, providing the lowest natural object entry speed observed in\ndecades long monitoring by networks world-wide. Modeling atmospheric\ndeceleration and fragmentation yields an initial mass of ~5 kg and diameter of\n15 cm, with a maximum Earth-relative velocity just over 11.0 km/s. Spectral\nobservations prove its natural origin. Back-integration across observational\nuncertainties yields a 92 - 98% probability of TCO behaviour, with close lunar\ndynamical interaction. The capture duration varies across observational\nuncertainties from 48 days to 5+ years. We also report on two low-speed impacts\nrecorded by US Government sensors, and we examine Prairie Network event PN39078\nfrom 1965 having an extremely low entry speed of 10.9 km/s. In these cases\nuncertainties in measurement and origin make TCO designation uncertain.'"
David Clark,Clark_David,arXiv:1509.07606,https://arxiv.org/abs/1509.07606,"b'Abstract:  We present studies of C/2015 D1 (SOHO), the first sunskirting comet ever seen\nfrom ground stations over the past half century. The Solar and Heliospheric\nObservatory (SOHO) witnessed its peculiar light curve with a huge dip followed\nby a flareup around perihelion: the dip was likely caused by sublimation of\nolivines, directly evidenced by a coincident temporary disappearance of the\ntail. The flareup likely reflects a disintegration event, which we suggest was\ntriggered by intense thermal stress established within the nucleus interior.\nPhotometric data reveal an increasingly dusty coma, indicative of volatile\ndepletion. A catastrophic mass loss rate of $\\sim$10$^{5}$ kg s$^{-1}$ around\nperihelion was seen. Ground-based Xingming Observatory spotted the\npost-perihelion debris cloud. Our morphological simulations of post-perihelion\nimages find newly released dust grains of size $a \\gtrsim 10$ $\\mu$m in radius,\nhowever, a temporal increase in $a_{\\min}$ was also witnessed, possibly due to\nswift dispersions of smaller grains swept away by radiation forces without\nreplenishment. Together with the fading profile of the light curve, a power law\ndust size distribution with index $\\gamma = 3.2 \\pm 0.1$ is derived. We\ndetected no active remaining cometary nuclei over $\\sim$0.1 km in radius in\npost-perihelion images acquired at Lowell Observatory. Applying radial\nnon-gravitational parameter, $\\mathcal{A}_{1} = \\left(1.209 \\pm 0.118 \\right)\n\\times 10^{-6}$ AU day$^{-2}$, from an isothermal water-ice sublimation model\nto the SOHO astrometry significantly reduces residuals and sinusoidal trends in\nthe orbit determination. The nucleus mass $\\sim$10$^{8}$--10$^{9}$ kg, and the\nradius $\\sim$50--150 m (bulk density $\\rho_{\\mathrm{d}} = 0.4$ g cm$^{-3}$\nassumed) before the disintegration are deduced from the photometric data;\nconsistent results were determined from the non-gravitational effects.'"
David Clark,Clark_David,arXiv:1506.03482,https://arxiv.org/abs/1506.03482,"b'Abstract:  A common and natural intuition among software testers is that test cases need\nto differ if a software system is to be tested properly and its quality\nensured. Consequently, much research has gone into formulating distance\nmeasures for how test cases, their inputs and/or their outputs differ. However,\ncommon to these proposals is that they are data type specific and/or calculate\nthe diversity only between pairs of test inputs, traces or outputs.\nWe propose a new metric to measure the diversity of sets of tests: the test\nset diameter (TSDm). It extends our earlier, pairwise test diversity metrics\nbased on recent advances in information theory regarding the calculation of the\nnormalized compression distance (NCD) for multisets. An advantage is that TSDm\ncan be applied regardless of data type and on any test-related information, not\nonly the test inputs. A downside is the increased computational time compared\nto competing approaches.\nOur experiments on four different systems show that the test set diameter can\nhelp select test sets with higher structural and fault coverage than random\nselection even when only applied to test inputs. This can enable early test\ndesign and selection, prior to even having a software system to test, and\ncomplement other types of test automation and analysis. We argue that this\nquantification of test set diversity creates a number of opportunities to\nbetter understand software quality and provides practical ways to increase it.'"
David Clark,Clark_David,arXiv:1502.07661,https://arxiv.org/abs/1502.07661,"b'Abstract:  This work focuses on a specific front of the malware detection arms-race,\nnamely the detection of persistent, disk-resident malware. We exploit\nnormalised compression distance (NCD), an information theoretic measure,\napplied directly to binaries. Given a zoo of labelled malware and benign-ware,\nwe ask whether a suspect program is more similar to our malware or to our\nbenign-ware. Our approach classifies malware with 97.1% accuracy and a false\npositive rate of 3%. We achieve our results with off-the-shelf compressors and\na standard machine learning classifier and without any specialised knowledge.\nAn end-user need only collect a zoo of malware and benign-ware and then can\nimmediately apply our techniques.\nWe apply statistical rigour to our experiments and our selection of data. We\ndemonstrate that accuracy can be optimised by combining NCD with the\ncompressibility rates of the executables. We demonstrate that malware reported\nwithin a more narrow time frame of a few days is more homogenous than malware\nreported over a longer one of two years but that our method still classifies\nthe latter with 95.2% accuracy and a 5% false positive rate. Due to the use of\ncompression, the time and computation cost of our method is non-trivial. We\nshow that simple approximation techniques can improve the time complexity of\nour approach by up to 63%.\nWe compare our results to the results of applying the 59 anti-malware\nprograms used on the VirusTotal web site to our malware. Our approach does\nbetter than any single one of them as well as the 59 used collectively.'"
David Clark,Clark_David,arXiv:1306.5339,https://arxiv.org/abs/1306.5339,"b'Abstract:  We give a new solution to the famous Gion shrine geometry problem from\neighteenth-century Japan. Like the classical Japanese solution, ours is given\nin the form of a degree ten equation. However, our polynomial has the advantage\nof being much easier to write down. We also provide some additional analysis,\nincluding a discussion of existence and uniqueness.'"
David Clark,Clark_David,arXiv:1008.4747,https://arxiv.org/abs/1008.4747,"b'Abstract:  This paper develops a general method for constructing entanglement-assisted\nquantum low-density parity-check (LDPC) codes, which is based on combinatorial\ndesign theory. Explicit constructions are given for entanglement-assisted\nquantum error-correcting codes (EAQECCs) with many desirable properties. These\nproperties include the requirement of only one initial entanglement bit, high\nerror correction performance, high rates, and low decoding complexity. The\nproposed method produces infinitely many new codes with a wide variety of\nparameters and entanglement requirements. Our framework encompasses various\ncodes including the previously known entanglement-assisted quantum LDPC codes\nhaving the best error correction performance and many new codes with better\nblock error rates in simulations over the depolarizing channel. We also\ndetermine important parameters of several well-known classes of quantum and\nclassical LDPC codes for previously unsettled cases.'"
David Clark,Clark_David,arXiv:0911.0665,https://arxiv.org/abs/0911.0665,"b'Abstract:  We present high sensitivity H91$\\alpha$ and 3.5 cm radio continuum\nobservations toward the planetary nebula NGC 3242. The electron temperature\ndetermined assuming local thermodynamic equilibrium is consistent within\n$\\sim$10% with that derived from optical lines and the Balmer discontinuity.\nThe line emission and the continuum emission have very similar spatial\ndistribution, suggesting that at this wavelength there is no other continuum\nprocess present in a significant manner. In particular, we conclude that\nemission from spinning dust is not important at this wavelength. In this radio\nrecombination line the nebula presents a radial velocity structure consistent\nwith that obtained from observations of optical lines.'"
David Clark,Clark_David,arXiv:0806.0601,https://arxiv.org/abs/0806.0601,"b""Abstract:  We prove that Morrison and Nieh's categorification of the su(3) quantum knot\ninvariant is functorial with respect to tangle cobordisms. This is in contrast\nto the categorified su(2) theory, which was not functorial as originally\ndefined. We use methods of Bar-Natan to construct explicit chain maps for each\nvariation of the third Reidemeister move. Then, to show functoriality, we\nmodify arguments used by Clark, Morrison, and Walker to show that induced chain\nmaps are invariant under Carter and Saito's movie moves."""
David Clark,Clark_David,arXiv:0707.0040,https://arxiv.org/abs/0707.0040,"b'Abstract:  Neutron resonance spectrometry (NRS) has been used to measure the temperature\ninside Mo samples during shock loading. The temperatures obtained were\nsignificantly higher than predicted assuming ideal hydrodynamic loading. The\neffect of plastic flow and non-ideal projectile behavior were assessed. Plastic\nflow was calculated self-consistently with the shock jump conditions: this is\nnecessary for a rigorous estimate of the locus of shock states accessible.\nPlastic flow was estimated to contribute a temperature rise of 53K compared\nwith hydrodynamic flow. Simulations were performed of the operation of the\nexplosively-driven projectile system used to induce the shock in the Mo sample.\nThe simulations predicted that the projectile was significantly curved on\nimpact, and still accelerating. The resulting spatial variations in load,\nincluding radial components of velocity, were predicted to increase the\napparent temperature that would be deduced from the width of the neutron\nresonance by 160K. These corrections are sufficient to reconcile the apparent\ntemperatures deduced using NRS with the accepted properties of Mo, in\nparticular its equation of state.'"
David Clark,Clark_David,arXiv:0704.1850,https://arxiv.org/abs/0704.1850,"b'Abstract:  Shock and release temperatures in Mo were calculated, taking account of\nheating from plastic flow predicted using the Steinberg-Guinan model. Plastic\nflow was calculated self-consistently with the shock jump conditions: this is\nnecessary for a rigorous estimate of the locus of shock states accessible. The\ntemperatures obtained were significantly higher than predicted assuming ideal\nhydrodynamic loading. The temperatures were compared with surface emission\nspectrometry measurements for Mo shocked to around 60GPa and then released into\nvacuum or into a LiF window. Shock loading was induced by the impact of a\nplanar projectile, accelerated by high explosive or in a gas gun. Surface\nvelocimetry showed an elastic wave at the start of release from the shocked\nstate; the amplitude of the elastic wave matched the prediction to around 10%,\nindicating that the predicted flow stress in the shocked state was reasonable.\nThe measured temperatures were consistent with the simulations, indicating that\nthe fraction of plastic work converted to heat was in the range 70-100% for\nthese loading conditions.'"
David Clark,Clark_David,arXiv:cond-mat/0702693,https://arxiv.org/abs/cond-mat/0702693,"b""Abstract:  The hydrodynamic operation of the `Forest Flyer' type of explosive launching\nsystem for shock physics projectiles was investigated in detail using one- and\ntwo-dimensional continuum dynamics simulations. The simulations were\ninsensitive to uncertainties in the material properties, and reproduced\nmeasurements of the projectile. The most commonly-used variant, with an Al\nalloy case, was predicted to produce a slightly curved projectile, subjected to\nsome shock heating, and likely exhibiting some porosity from tensile damage.\nThe flatness can be improved by using a case of lower shock impedance, such as\npolymethyl methacrylate. High-impedance cases, including Al alloys but with\ndenser materials improving the launching efficiency, can be used if designed\naccording to the physics of oblique shock reflection. The tensile stress\ninduced in the projectile depends on the relative thickness of the explosive,\nexpansion gap, and projectile. The thinner the projectile with respect to the\nexplosive, the smaller the tensile stress. If the explosive is initiated with a\nplane wave lens, the tensile stress is lower than for initiation with multiple\ndetonators over a plane. The previous plane wave lens designs did however\ninduce a tensile stress close to the spall strength of the projectile. The\ntensile stress can be reduced by changes in the component thicknesses.\nExperiments to verify the operation of explosively-launched projectiles should\nattempt to measure porosity induced in the projectile: arrival time\nmeasurements may be insensitive to porous regions caused by damaged or\nrecollected material."""
David Clark,Clark_David,arXiv:math/0701339,https://arxiv.org/abs/math/0701339,"b""Abstract:  We describe a modification of Khovanov homology (math.QA/9908171), in the\nspirit of Bar-Natan (math.GT/0410495), which makes the theory properly\nfunctorial with respect to link cobordisms.\nThis requires introducing `disorientations' in the category of smoothings and\nabstract cobordisms between them used in Bar-Natan's definition.\nDisorientations have `seams' separating oppositely oriented regions, coming\nwith a preferred normal direction. The seams satisfy certain relations (just as\nthe underlying cobordisms satisfy relations such as the neck cutting relation).\nWe construct explicit chain maps for the various Reidemeister moves, then\nprove that the compositions of chain maps associated to each side of each of\nCarter and Saito's movie moves (MR1238875, MR1445361) always agree. These\ncalculations are greatly simplified by following arguments due to Bar-Natan and\nKhovanov, which ensure that the two compositions must agree, up to a sign. We\nset up this argument in our context by proving a result about duality in\nKhovanov homology, generalising previous results about mirror images of knots\nto a `local' result about tangles. Along the way, we reproduce Jacobsson's sign\ntable (math.GT/0206303) for the original `unoriented theory', with a few\ndisagreements."""
David Clark,Clark_David,arXiv:cs/9901011,https://arxiv.org/abs/cs/9901011,"b'Abstract:  The Internet has revolutionized the computer and communications world like\nnothing before. The invention of the telegraph, telephone, radio, and computer\nset the stage for this unprecedented integration of capabilities. The Internet\nis at once a world-wide broadcasting capability, a mechanism for information\ndissemination, and a medium for collaboration and interaction between\nindividuals and their computers without regard for geographic location.\nIn this paper, several of us involved in the development and evolution of the\nInternet share our views of its origins and history. This is intended to be a\nbrief, necessarily cursory and incomplete history. This history revolves around\nfour distinct aspects. There is the technological evolution that began with\nearly research on packet switching and the ARPANET (and related technologies),\nand where current research continues to expand the horizons of the\ninfrastructure along several dimensions, such as scale, performance, and higher\nlevel functionality. There is the operations and management aspect of a global\nand complex operational infrastructure. There is the social aspect, which\nresulted in a broad community of Internauts working together to create and\nevolve the technology. And there is the commercialization aspect, resulting in\nan extremely effective transition of research results into a broadly deployed\nand available information infrastructure.'"
Munther Dahleh,Dahleh_Munther,arXiv:1902.01848,https://arxiv.org/abs/1902.01848,"b'Abstract:  We address the problem of learning the parameters of a stable linear time\ninvariant (LTI) system with unknown latent space dimension, or \\textit{order},\nfrom its noisy input-output data. In particular, we focus on learning the\nparameters of the best lower order approximation allowed by the finite data.\nThis is achieved by constructing a Hankel-like representation of the underlying\nsystem using ordinary least squares. Such a representation circumvents the\nnon-convexities that typically arise in system identification, and it allows\naccurate estimation of the underlying LTI system. Our results rely on a careful\nanalysis of a self-normalized martingale difference term that helps bound\nidentification error up to logarithmic factors of the lower bound. We provide a\ndata-dependent scheme for order selection and find a realization of system\nparameters, corresponding to that order, by an approach that is closely related\nto the celebrated Kalman-Ho subspace algorithm. We show that this realization\nis a good approximation of the underlying LTI system with high probability.\nFinally, we demonstrate that the proposed model order selection procedure is\nminimax optimal, i.e., for the given data length it is not always possible to\nestimate higher order models or find higher order approximations with\nreasonable accuracy.'"
Munther Dahleh,Dahleh_Munther,arXiv:1810.10513,https://arxiv.org/abs/1810.10513,"b'Abstract:  Spurred by the growth of transportation network companies and increasing data\ncapabilities, vehicle routing and ride-matching algorithms can improve the\nefficiency of private transportation services. However, existing routing\nsolutions do not address where drivers should travel after dropping off a\npassenger and before receiving the next passenger ride request, i.e., during\nthe between-ride period. We address this problem by developing an efficient\nalgorithm to find the optimal policy for drivers between rides in order to\nmaximize driver profits. We model the road network as a graph, and we show that\nthe between-ride routing problem is equivalent to a stochastic shortest path\nproblem, an infinite dynamic program with no discounting. We prove under\nreasonable assumptions that an optimal routing policy exists that avoids\ncycles; policies of this type can be efficiently found. We present an iterative\napproach to find an optimal routing policy. Our approach can account for\nvarious factors, including the frequency of passenger ride requests at\ndifferent locations, traffic conditions, and surge pricing. We demonstrate the\neffectiveness of the approach by implementing it on road network data from\nBoston and New York City.'"
Munther Dahleh,Dahleh_Munther,arXiv:1809.05948,https://arxiv.org/abs/1809.05948,"b'Abstract:  This paper addresses two fundamental problems in the context of jump linear\nsystems (JLS). The first problem is concerned with characterizing the minimal\nstate space dimension solely from input-output pairs and without any knowledge\nof the number of mode switches. The second problem is concerned with\ncharacterizing the number of discrete modes of the JLS. For the first problem,\nwe develop a linear system theory based approach and construct an appropriate\nHankel-like matrix. The rank of this matrix gives us the state space dimension.\nFor the second problem we show that minimal number of modes corresponds to the\nminimal rank of a positive semi-definite matrix obtained via a non--convex\nformulation.'"
Munther Dahleh,Dahleh_Munther,arXiv:1805.08125,https://arxiv.org/abs/1805.08125,"b""Abstract:  In this work, we aim to design a data marketplace; a robust real-time\nmatching mechanism to efficiently buy and sell training data for Machine\nLearning tasks. While the monetization of data and pre-trained models is an\nessential focus of industry today, there does not exist a market mechanism to\nprice training data and match buyers to sellers while still addressing the\nassociated (computational and other) complexity. The challenge in creating such\na market stems from the very nature of data as an asset: (i) it is freely\nreplicable; (ii) its value is inherently combinatorial due to correlation with\nsignal in other data; (iii) prediction tasks and the value of accuracy vary\nwidely; (iv) usefulness of training data is difficult to verify a priori\nwithout first applying it to a prediction task. As our main contributions we:\n(i) propose a mathematical model for a two-sided data market and formally\ndefine the key associated challenges; (ii) construct algorithms for such a\nmarket to function and analyze how they meet the challenges defined. We\nhighlight two technical contributions: (i) a new notion of 'fairness' required\nfor cooperative games with freely replicable goods; (ii) a truthful, zero\nregret mechanism to auction a class of combinatorial goods based on utilizing\nMyerson's payment function and the Multiplicative Weights algorithm. These\nmight be of independent interest."""
Munther Dahleh,Dahleh_Munther,arXiv:1802.07321,https://arxiv.org/abs/1802.07321,"b'Abstract:  We consider the problem of robustness in large consensus networks that occur\nin many areas such as distributed optimization. Robustness, in this context, is\nthe scaling of performance measures, e.g. H2-norm, as a function of network\ndimension. We provide a formal framework to quantify the relation between such\nperformance scaling and the convergence speed of the network. Specifically, we\nprovide upper and lower bounds for the convergence speed in terms of robustness\nand discuss how these bounds scale with the network topology. The main\ncontribution of this work is that we obtain tight bounds, that hold regardless\nof network topology. The work here also encompasses some results in convergence\ntime analysis in previous literature.'"
Munther Dahleh,Dahleh_Munther,arXiv:1712.05273,https://arxiv.org/abs/1712.05273,"b'Abstract:  This paper examines the dependence of network performance measures on network\nsize and considers scaling results for large networks. We connect two\nperformance measures that are well studied, but appear to be unrelated. The\nfirst measure is concerned with energy metrics, namely the $\\Hcal_2$--norm of a\nnetwork, which arises in control theory applications. The second measure is\nconcerned with the notion of ""tail risk"" which arises in economic and financial\nnetworks. We study the question of why such performance measures may\ndeteriorate at a faster rate than the growth rate of the network. We first\nfocus on the energy metric and its well known connection to controllability\nGramian of the underlying dynamical system. We show that undirected networks\nexhibit the most graceful energy growth rates as network size grows. This rate\nis quantified completely by the proximity of spectral radius to unity or\ndistance to instability. In contrast, we show that the simple characterization\nof energy in terms of network spectrum does not exist for directed networks. We\ndemonstrate that, for any fixed distance to instability, energy of a directed\nnetwork can grow at an exponentially faster rate. We provide general methods\nfor manipulating networks to reduce energy. In particular, we prove that\ncertain operations that increase the symmetry in a network cannot increase\nenergy (in an order sense). Secondly, we focus on tail risk in economic and\nfinancial networks. In contrast to $\\Hcal_2$--norm which arises from computing\nthe expectation of energy in the network, tail risk focuses on tail probability\nbehavior of network variables. Although the two measures differ substantially\nwe show that they are precisely connected through the system Gramian. This\nsurprising result explains why topology considerations rather than specific\nperformance measures dictate the large scale behavior of networks.'"
Munther Dahleh,Dahleh_Munther,arXiv:1709.01432,https://arxiv.org/abs/1709.01432,"b'Abstract:  In coalitional games, traditional coalitional game theory does not apply if\ndifferent participants hold different opinions about the payoff function that\ncorresponds to each subset of the coalition. In this paper, we propose a\nframework in which players can exchange opinions about their views of payoff\nfunctions and then decide the distribution of the value of the grand coalition.\nWhen all players are truth-telling, the problem of opinion consensus is\ndecoupled from the coalitional game, but interesting dynamics will arise when\nplayers are strategic in the consensus phase. Assuming that all players are\nrational, the model implies that, if influential players are risk-averse, an\nefficient fusion of the distributed data is achieved at pure strategy Nash\nequilibrium, meaning that the average opinion will not drift. Also, without the\nassumption that all players are rational, each player can use an algorithmic\nR-learning process, which gives the same result as the pure strategy Nash\nequilibrium with rational players.'"
Munther Dahleh,Dahleh_Munther,arXiv:1703.00980,https://arxiv.org/abs/1703.00980,"b""Abstract:  This paper analyzes the impact of peer effects on electricity consumption of\na network of rational, utility-maximizing users. Users derive utility from\nconsuming electricity as well as consuming less energy than their neighbors.\nHowever, a disutility is incurred for consuming more than their neighbors. To\nmaximize the profit of the load-serving entity that provides electricity to\nsuch users, we develop a two-stage game-theoretic model, where the entity sets\nthe prices in the first stage. In the second stage, consumers decide on their\ndemand in response to the observed price set in the first stage so as to\nmaximize their utility. To this end, we derive theoretical statements under\nwhich such peer effects reduce aggregate user consumption. Further, we obtain\nexpressions for the resulting electricity consumption and profit of the load\nserving entity for the case of perfect price discrimination and a single price\nunder complete information, and approximations under incomplete information.\nSimulations suggest that exposing only a selected subset of all users to peer\neffects maximizes the entity's profit."""
Munther Dahleh,Dahleh_Munther,arXiv:1703.00976,https://arxiv.org/abs/1703.00976,"b'Abstract:  Load-serving entities which procure electricity from the wholesale\nelectricity market to service end-users face significant quantity and price\nrisks due to the volatile nature of electricity demand and quasi-fixed\nresidential tariffs at which electricity is sold. This paper investigates\nstrategies for load serving entities to hedge against such price risks.\nSpecifically, we compute profit-maximizing portfolios of forward contract and\ncall options as a function of the uncertain aggregate user demand. We compare\nthe profit to the case of Demand Response, where users are offered monetary\nincentives to temporarily reduce their consumption during periods of supply\nshortages. Using smart meter data of residential customers in California, we\nsimulate optimal portfolios and derive conditions under which Demand Response\noutperforms call options and forward contracts.'"
Munther Dahleh,Dahleh_Munther,arXiv:1703.00972,https://arxiv.org/abs/1703.00972,"b'Abstract:  Residential Demand Response has emerged as a viable tool to alleviate supply\nand demand imbalances of electricity, particularly during times when the\nelectric grid is strained due a shortage of supply. Demand Response providers\nbid reduction capacity into the wholesale electricity market by asking their\ncustomers under contract to temporarily reduce their consumption in exchange\nfor a monetary incentive. To contribute to the analysis of consumer behavior in\nresponse to such incentives, this paper formulates Demand Response as a\nMechanism Design problem, where a Demand Response Provider elicits private\ninformation of its rational, profit-maximizing customers who derive positive\nexpected utility by participating in reduction events. By designing an\nincentive compatible and individually rational mechanism to collect users\'\nprice elasticities of demand, the Demand Response provider can target the most\nsusceptible users to incentives. We measure reductions by comparing the\nmaterialized consumption to the projected consumption, which we model as the\n""10-in-10""-baseline, the regulatory standard set by the California Independent\nSystem Operator. Due to the suboptimal performance of this baseline, we show,\nusing consumption data of residential customers in California, that Demand\nResponse Providers receive payments for ""virtual reductions"", which exist due\nto the inaccuracies of the baseline rather than actual reductions. Improving\nthe accuracy of the baseline diminishes the contribution of these virtual\nreductions.'"
Munther Dahleh,Dahleh_Munther,arXiv:1611.03765,https://arxiv.org/abs/1611.03765,"b'Abstract:  We investigate the ability of a homogeneous collection of deferrable energy\nloads to behave as a battery; that is, to absorb and release energy in a\ncontrollable fashion up to fixed and predetermined limits on volume, charge\nrate and discharge rate. We derive explicit bounds on the battery capacity that\ncan be offered, and show that there is a fundamental trade-off between the\nabilities of collective load to absorb and release energy at high aggregate\nrates. Finally, we introduce a new class of dynamic priority-driven feedback\npolicies that balance these abilities, and characterize the batteries that they\ncan emulate.'"
Munther Dahleh,Dahleh_Munther,arXiv:1610.01973,https://arxiv.org/abs/1610.01973,"b'Abstract:  We investigate the ability of a homogeneous collection of deferrable energy\nloads to behave as a battery; that is, to absorb and release energy in a\ncontrollable fashion up to fixed and predetermined limits on volume, charge\nrate and discharge rate. We derive bounds on the battery capacity that can be\nrealized and show that there are fundamental trade-offs between battery\nparameters. By characterizing the state trajectories under scheduling policies\nthat emulate two illustrative batteries, we show that the trade-offs occur\nbecause the states that allow the loads to absorb and release energy at high\naggregate rates are conflicting.'"
Munther Dahleh,Dahleh_Munther,arXiv:1609.06700,https://arxiv.org/abs/1609.06700,"b'Abstract:  In this paper, we investigate the use of variable speed limits for resilient\noperation of transportation networks, which are modeled as dynamical flow\nnetworks under local routing decisions. In such systems, some external inflow\nis injected to the so-called origin nodes of the network. The total inflow\narriving at each node is routed to its operational outgoing links based on\ntheir current particle densities. The density on each link has first order\ndynamics driven by the difference of its incoming and outgoing flows. A link\nirreversibly fails if it reaches its jam density. Such failures may propagate\nin the network and cause a systemic failure. We show that larger link\ncapacities do not necessarily help in preventing systemic failures under local\nrouting. Accordingly, we propose the use of variable speed limits to operate\nthe links below their capacities, when necessary, to compensate for the lack of\nglobal information and coordination in routing decisions. Our main result shows\nthat systemic failures under feasible external inflows can always be averted\nthrough a proper selection of speed limits if the routing decisions are\nsufficiently responsive to local congestion and the network is initially\nuncongested. This is an attractive feature as it is much easier in practice to\nadjust the speed limits than to build more physical capacity or to alter\nrouting decisions that are determined by social behavior.'"
Munther Dahleh,Dahleh_Munther,arXiv:1609.06193,https://arxiv.org/abs/1609.06193,"b""Abstract:  This paper analyzes stability conditions for wholesale electricity markets\nunder real-time retail pricing and realistic consumption models with memory,\nwhich explicitly take into account previous electricity prices and consumption\nlevels. By passing on the current retail price of electricity from supplier to\nconsumer and feeding the observed consumption back to the supplier, a\nclosed-loop dynamical system for electricity prices and consumption arises\nwhose stability is to be investigated. Under mild assumptions on the generation\ncost of electricity and consumers' backlog disutility functions, we show that,\nfor consumer models with price memory only, market stability is achieved if the\nratio between the consumers' marginal backlog disutility and the suppliers'\nmarginal cost of supply remains below a fixed threshold. Further, consumer\nmodels with price and consumption memory can result in greater stability\nregions and faster convergence to the equilibrium compared to models with price\nmemory alone, if consumption deviations from nominal demand are adequately\npenalized."""
Munther Dahleh,Dahleh_Munther,arXiv:1608.04155,https://arxiv.org/abs/1608.04155,"b'Abstract:  In this paper, we are concerned with the resilience of locally routed network\nflows with finite link capacities. In this setting, an external inflow is\ninjected to the so-called origin nodes. The total inflow arriving at each node\nis routed locally such that none of the outgoing links are overloaded unless\nthe node receives an inflow greater than its total outgoing capacity. A link\nirreversibly fails if it is overloaded or if there is no operational link in\nits immediate downstream to carry its flow. For such systems, resilience is\ndefined as the minimum amount of reduction in the link capacities that would\nresult in the failure of all the outgoing links of an origin node. We show that\nsuch networks do not necessarily become more resilient as additional capacity\nis built in the network. Moreover, when the external inflow does not exceed the\nnetwork capacity, selective reductions of capacity at certain links can\nactually help averting the cascading failures, without requiring any change in\nthe local routing policies. This is an attractive feature as it is often easier\nin practice to reduce the available capacity of some critical links than to add\nphysical capacity or to alter routing policies, e.g., when such policies are\ndetermined by social behavior, as in the case of road traffic networks. The\nresults can thus be used for real-time monitoring of distance-to-failure in\nsuch networks and devising a feasible course of actions to avert systemic\nfailures.'"
Munther Dahleh,Dahleh_Munther,arXiv:1606.08101,https://arxiv.org/abs/1606.08101,"b'Abstract:  Voltage control plays an important role in the operation of electricity\ndistribution networks, especially when there is a large penetration of\nrenewable energy resources. In this paper, we focus on voltage control through\nreactive power compensation and study how different information structures\naffect the control performance. In particular, we first show that using only\nvoltage measurements to determine reactive power compensation is insufficient\nto maintain voltage in the acceptable range. Then we propose two fully\ndecentralized and robust algorithms by adding additional information, which can\nstabilize the voltage in the acceptable range. The one with higher complexity\ncan further minimize a cost of reactive power compensation in a particular\nform. Both of the two algorithms use only local measurements and local\nvariables and require no communication. In addition, the two algorithms are\nrobust against heterogeneous update rates and delays.'"
Munther Dahleh,Dahleh_Munther,arXiv:1506.06394,https://arxiv.org/abs/1506.06394,"b'Abstract:  We introduce a new class of (dynamical) systems that inherently capture\ncascading effects (viewed as consequential effects) and are naturally amenable\nto combinations. We develop an axiomatic general theory around those systems,\nand guide the endeavor towards an understanding of cascading failure. The\ntheory evolves as an interplay of lattices and fixed points, and its results\nmay be instantiated to commonly studied models of cascade effects.\nWe characterize the systems through their fixed points, and equip them with\ntwo operators. We uncover properties of the operators, and express global\nsystems through combinations of local systems. We enhance the theory with a\nnotion of failure, and understand the class of shocks inducing a system to\nfailure. We develop a notion of mu-rank to capture the energy of a system, and\nunderstand the minimal amount of effort required to fail a system, termed\nresilience. We deduce a dual notion of fragility and show that the combination\nof systems sets a limit on the amount of fragility inherited.'"
Munther Dahleh,Dahleh_Munther,arXiv:1411.3698,https://arxiv.org/abs/1411.3698,"b'Abstract:  Consider a stationary discrete random process with alphabet size d, which is\nassumed to be the output process of an unknown stationary Hidden Markov Model\n(HMM). Given the joint probabilities of finite length strings of the process,\nwe are interested in finding a finite state generative model to describe the\nentire process. In particular, we focus on two classes of models: HMMs and\nquasi-HMMs, which is a strictly larger class of models containing HMMs. In the\nmain theorem, we show that if the random process is generated by an HMM of\norder less or equal than k, and whose transition and observation probability\nmatrix are in general position, namely almost everywhere on the parameter\nspace, both the minimal quasi-HMM realization and the minimal HMM realization\ncan be efficiently computed based on the joint probabilities of all the length\nN strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to\ncompare and connect the two lines of literature: realization theory of HMMs,\nand the recent development in learning latent variable models with tensor\ndecomposition techniques.'"
Munther Dahleh,Dahleh_Munther,arXiv:1407.3518,https://arxiv.org/abs/1407.3518,"b'Abstract:  We propose a dynamical model for cascading failures in single-commodity\nnetwork flows. In the proposed model, the network state consists of flows and\nactivation status of the links. Network dynamics is determined by a, possibly\nstate-dependent and adversarial, disturbance process that reduces flow capacity\non the links, and routing policies at the nodes that have access to the network\nstate, but are oblivious to the presence of disturbance. Under the proposed\ndynamics, a link becomes irreversibly inactive either due to overload condition\non itself or on all of its immediate downstream links. The coupling between\nlink activation and flow dynamics implies that links to become inactive\nsuccessively are not necessarily adjacent to each other, and hence the pattern\nof cascading failure under our model is qualitatively different than standard\ncascade models. The magnitude of a disturbance process is defined as the sum of\ncumulative capacity reductions across time and links of the network, and the\nmargin of resilience of the network is defined as the infimum over the\nmagnitude of all disturbance processes under which the links at the origin node\nbecome inactive. We propose an algorithm to compute an upper bound on the\nmargin of resilience for the setting where the routing policy only has access\nto information about the local state of the network. For the limiting case when\nthe routing policies update their action as fast as network dynamics, we\nidentify sufficient conditions on network parameters under which the upper\nbound is tight under an appropriate routing policy. Our analysis relies on\nmaking connections between network parameters and monotonicity in network state\nevolution under proposed dynamics.'"
Munther Dahleh,Dahleh_Munther,arXiv:1211.1696,https://arxiv.org/abs/1211.1696,"b'Abstract:  The primary concerns of this paper are twofold: to understand the economic\nvalue of storage in the presence of ramp constraints and exogenous electricity\nprices, and to understand the implications of the associated optimal storage\nmanagement policy on qualitative and quantitative characteristics of storage\nresponse to real-time prices. We present an analytic characterization of the\noptimal policy, along with the associated finite-horizon time-averaged value of\nstorage. We also derive an analytical upperbound on the infinite-horizon\ntime-averaged value of storage. This bound is valid for any achievable\nrealization of prices when the support of the distribution is fixed, and\nhighlights the dependence of the value of storage on ramp constraints and\nstorage capacity. While the value of storage is a non-decreasing function of\nprice volatility, due to the finite ramp rate, the value of storage saturates\nquickly as the capacity increases, regardless of volatility. To study the\nimplications of the optimal policy, we first present computational experiments\nthat suggest that optimal utilization of storage can, in expectation, induce a\nconsiderable amount of price elasticity near the average price, but little or\nno elasticity far from it. We then present a computational framework for\nunderstanding the behavior of storage as a function of price and the amount of\nstored energy, and for characterization of the buy/sell phase transition region\nin the price-state plane. Finally, we study the impact of market-based\noperation of storage on the required reserves, and show that the reserves may\nneed to be expanded to accommodate market-based storage.'"
Munther Dahleh,Dahleh_Munther,arXiv:1211.0654,https://arxiv.org/abs/1211.0654,"b""Abstract:  We study a model for cascade effects over finite networks based on a\ndeterministic binary linear threshold model. Our starting point is a networked\ncoordination game where each agent's payoff is the sum of the payoffs coming\nfrom pairwise interactions with each of the neighbors. We first establish that\nthe best response dynamics in this networked game is equivalent to the linear\nthreshold dynamics with heterogeneous thresholds over the agents. While the\nprevious literature has studied such linear threshold models under the\nassumption that each agent may change actions at most once, a study of best\nresponse dynamics in such networked games necessitates an analysis that allows\nfor multiple switches in actions. In this paper, we develop such an analysis\nand construct a combinatorial framework to understand the behavior of the\nmodel. To this end, we establish that the agents behavior cycles among\ndifferent actions in the limit and provide three sets of results.\nWe first characterize the limiting behavioral properties of the dynamics. We\ndetermine the length of the limit cycles and reveal bounds on the time steps\nrequired to reach such cycles for different network structures. We then study\nthe complexity of decision/counting problems that arise within the context.\nSpecifically, we consider the tractability of counting the number of limit\ncycles and fixed-points, and deciding the reachability of action profiles. We\nfinally propose a measure of network resilience that captures the nature of the\ninvolved dynamics. We prove bounds and investigate the resilience of different\nnetwork structures under this measure."""
Munther Dahleh,Dahleh_Munther,arXiv:1209.0229,https://arxiv.org/abs/1209.0229,"b'Abstract:  In this paper, we examine in an abstract framework, how a tradeoff between\nefficiency and robustness arises in different dynamic oligopolistic market\narchitectures. We consider a market in which there is a monopolistic resource\nprovider and agents that enter and exit the market following a random process.\nSelf-interested and fully rational agents dynamically update their resource\nconsumption decisions over a finite time horizon, under the constraint that the\ntotal resource consumption requirements are met before each individual\'s\ndeadline. We then compare the statistics of the stationary aggregate demand\nprocesses induced by the non-cooperative and cooperative load scheduling\nschemes. We show that although the non-cooperative load scheduling scheme leads\nto an efficiency loss - widely known as the ""price of anarchy"" - the stationary\ndistribution of the corresponding aggregate demand process has a smaller tail.\nThis tail, which corresponds to rare and undesirable demand spikes, is\nimportant in many applications of interest. On the other hand, when the agents\ncan cooperate with each other in optimizing their total cost, a higher market\nefficiency is achieved at the cost of a higher probability of demand spikes. We\nthus posit that the origins of endogenous risk in such systems may lie in the\nmarket architecture, which is an inherent characteristic of the system.'"
Munther Dahleh,Dahleh_Munther,arXiv:1205.0076,https://arxiv.org/abs/1205.0076,"b'Abstract:  Robustness of routing policies for networks is a central problem which is\ngaining increased attention with a growing awareness to safeguard critical\ninfrastructure networks against natural and man-induced disruptions. Routing\nunder limited information and the possibility of cascades through the network\nadds serious challenges to this problem. This abstract considers the framework\nof dynamical networks introduced in our earlier work [1,2], where the network\nis modeled by a system of ordinary differential equations derived from mass\nconservation laws on directed acyclic graphs with a single origin-destination\npair and a constant inflow at the origin. The rate of change of the particle\ndensity on each link of the network equals the difference between the inflow\nand the outflow on that link. The latter is modeled to depend on the current\nparticle density on that link through a flow function. The novel modeling\nelement in this paper is that every link is assumed to have finite capacity for\nparticle density and that the flow function is modeled to be strictly\nincreasing as density increases from zero up to the maximum density capacity,\nand is discontinuous at the maximum density capacity, with the flow function\nvalue being zero at that point. This feature, in particular, allows for the\npossibility of spill-backs in our model. In this paper, we present our results\non resilience of such networks under distributed routing, towards perturbations\nthat reduce link-wise flow functions.'"
Munther Dahleh,Dahleh_Munther,arXiv:1109.6505,https://arxiv.org/abs/1109.6505,"b""Abstract:  This paper examines the value of storage in securing reliability of a system\nwith uncertain supply and demand, and supply friction. The storage is\nfrictionless as a supply source, but once used, it cannot be filled up\ninstantaneously. The focus application is a power supply network in which the\nbase supply and demand are assumed to match perfectly, while deviations from\nthe base are modeled as random shocks with stochastic arrivals. Due to\nfriction, the random surge shocks cannot be tracked by the main supply sources.\nStorage, when available, can be used to compensate, fully or partially, for the\nsurge in demand or loss of supply. The problem of optimal utilization of\nstorage with the objective of maximizing system reliability is formulated as\nminimization of the expected discounted cost of blackouts over an infinite\nhorizon. It is shown that when the stage cost is linear in the size of the\nblackout, the optimal policy is myopic in the sense that all shocks are\ncompensated by storage up to the available level of storage. However, when the\nstage cost is strictly convex, it may be optimal to curtail some of the demand\nand allow a small current blackout in the interest of maintaining a higher\nlevel of reserve to avoid a large blackout in the future. The value of storage\ncapacity in improving system's reliability, as well as the effects of the\nassociated optimal policies under different stage costs on the probability\ndistribution of blackouts are examined."""
Munther Dahleh,Dahleh_Munther,arXiv:1109.4564,https://arxiv.org/abs/1109.4564,"b""Abstract:  We propose a general methodology for performing statistical inference within\na `rare-events regime' that was recently suggested by Wagner, Viswanath and\nKulkarni. Our approach allows one to easily establish consistent estimators for\na very large class of canonical estimation problems, in a large alphabet\nsetting. These include the problems studied in the original paper, such as\nentropy and probability estimation, in addition to many other interesting ones.\nWe particularly illustrate this approach by consistently estimating the size of\nthe alphabet and the range of the probabilities. We start by proposing an\nabstract methodology based on constructing a probability measure with the\ndesired asymptotic properties. We then demonstrate two concrete constructions\nby casting the Good-Turing estimator as a pseudo-empirical measure, and by\nusing the theory of mixture model estimation."""
Munther Dahleh,Dahleh_Munther,arXiv:1106.1401,https://arxiv.org/abs/1106.1401,"b""Abstract:  The paper proposes a framework for modeling and analysis of the dynamics of\nsupply, demand, and clearing prices in power system with real-time retail\npricing and information asymmetry. Real-time retail pricing is characterized by\npassing on the real-time wholesale electricity prices to the end consumers, and\nis shown to create a closed-loop feedback system between the physical layer and\nthe market layer of the power system. In the absence of a carefully designed\ncontrol law, such direct feedback between the two layers could increase\nvolatility and lower the system's robustness to uncertainty in demand and\ngeneration. A new notion of generalized price-elasticity is introduced, and it\nis shown that price volatility can be characterized in terms of the system's\nmaximal relative price elasticity, defined as the maximal ratio of the\ngeneralized price-elasticity of consumers to that of the producers. As this\nratio increases, the system becomes more volatile, and eventually, unstable. As\nnew demand response technologies and distributed storage increase the\nprice-elasticity of demand, the architecture under examination is likely to\nlead to increased volatility and possibly instability. This highlights the need\nfor assessing architecture systematically and in advance, in order to optimally\nstrike the trade-offs between volatility, economic efficiency, and system\nreliability."""
Munther Dahleh,Dahleh_Munther,arXiv:1103.4893,https://arxiv.org/abs/1103.4893,"b'Abstract:  Strong resilience properties of dynamical flow networks are analyzed for\ndistributed routing policies. The latter are characterized by the property that\nthe way the inflow at a non-destination node gets split among its outgoing\nlinks is allowed to depend only on local information about the current particle\ndensities on the outgoing links. The strong resilience of the network is\ndefined as the infimum sum of link-wise flow capacity reductions under which\nthe network cannot maintain the asymptotic total inflow to the destination node\nto be equal to the inflow at the origin. A class of distributed routing\npolicies that are locally responsive to local information is shown to yield the\nmaximum possible strong resilience under such local information constraints for\nan acyclic dynamical flow network with a single origin-destination pair. The\nmaximal strong resilience achievable is shown to be equal to the minimum node\nresidual capacity of the network. The latter depends on the limit flow of the\nunperturbed network and is defined as the minimum, among all the\nnon-destination nodes, of the sum, over all the links outgoing from the node,\nof the differences between the maximum flow capacity and the limit flow of the\nunperturbed network. We propose a simple convex optimization problem to solve\nfor equilibrium limit flows of the unperturbed network that minimize average\ndelay subject to strong resilience guarantees, and discuss the use of tolls to\ninduce such an equilibrium limit flow in transportation networks. Finally, we\npresent illustrative simulations to discuss the connection between cascaded\nfailures and the resilience properties of the network.'"
Munther Dahleh,Dahleh_Munther,arXiv:1102.1107,https://arxiv.org/abs/1102.1107,"b""Abstract:  Robustness of distributed routing policies is studied for dynamical flow\nnetworks, with respect to adversarial disturbances that reduce the link flow\ncapacities. A dynamical flow network is modeled as a system of ordinary\ndifferential equations derived from mass conservation laws on a directed\nacyclic graph with a single origin-destination pair and a constant inflow at\nthe origin. Routing policies regulate the way the inflow at a non-destination\nnode gets split among its outgoing links as a function of the current particle\ndensity, while the outflow of a link is modeled to depend on the current\nparticle density on that link through a flow function. The dynamical flow\nnetwork is called partially transferring if the total inflow at the destination\nnode is asymptotically bounded away from zero, and its weak resilience is\nmeasured as the minimum sum of the link-wise magnitude of all disturbances that\nmake it not partially transferring. The weak resilience of a dynamical flow\nnetwork with arbitrary routing policy is shown to be upper-bounded by the\nnetwork's min-cut capacity, independently of the initial flow conditions.\nMoreover, a class of distributed routing policies that rely exclusively on\nlocal information on the particle densities, and are locally responsive to\nthat, is shown to yield such maximal weak resilience. These results imply that\nlocality constraints on the information available to the routing policies do\nnot cause loss of weak resilience. Some fundamental properties of dynamical\nflow networks driven by locally responsive distributed policies are analyzed in\ndetail, including global convergence to a unique limit flow."""
Munther Dahleh,Dahleh_Munther,arXiv:1101.2220,https://arxiv.org/abs/1101.2220,"b""Abstract:  Stability of Wardrop equilibria is analyzed for dynamical transportation\nnetworks in which the drivers' route choices are influenced by information at\nmultiple temporal and spatial scales. The considered model involves a continuum\nof indistinguishable drivers commuting between a common origin/destination pair\nin an acyclic transportation network. The drivers' route choices are affected\nby their, relatively infrequent, perturbed best responses to global information\nabout the current network congestion levels, as well as their instantaneous\nlocal observation of the immediate surroundings as they transit through the\nnetwork. A novel model is proposed for the drivers' route choice behavior,\nexhibiting local consistency with their preference toward globally less\ncongested paths as well as myopic decisions in favor of locally less congested\npaths. The simultaneous evolution of the traffic congestion on the network and\nof the aggregate path preference is modeled by a system of coupled ordinary\ndifferential equations. The main result shows that, if the frequency of updates\nof path preferences is sufficiently small as compared to the frequency of the\ntraffic flow dynamics, then the state of the transportation network ultimately\napproaches a neighborhood of the Wardrop equilibrium. The presented results may\nbe read as a further evidence in support of Wardrop's postulate of equilibrium,\nshowing robustness of it with respect to non-persistent perturbations. The\nproposed analysis combines techniques from singular perturbation theory,\nevolutionary game theory, and cooperative dynamical systems."""
Munther Dahleh,Dahleh_Munther,arXiv:0903.5535,https://arxiv.org/abs/0903.5535,"b'Abstract:  We demonstrate the use of a new, control-oriented notion of finite state\napproximation for a particular class of hybrid systems. Specifically, we\nconsider the problem of designing a stabilizing binary output feedback\nswitching controller for a pair of unstable homogeneous second order systems.\nThe constructive approach presented in this note, in addition to yielding an\nexplicit construction of a deterministic finite state approximate model of the\nhybrid plant, allows us to efficiently establish a useable upper bound on the\nquality of approximation, and leads to a discrete optimization problem whose\nsolution immediately provides a certifiably correct-by-design controller for\nthe original system. The resulting controller consists of a finite state\nobserver for the plant and a corresponding full state feedback switching\ncontrol law.'"
Munther Dahleh,Dahleh_Munther,arXiv:0810.5148,https://arxiv.org/abs/0810.5148,"b'Abstract:  A set of N independent Gaussian linear time invariant systems is observed by\nM sensors whose task is to provide the best possible steady-state causal\nminimum mean square estimate of the state of the systems, in addition to\nminimizing a steady-state measurement cost. The sensors can switch between\nsystems instantaneously, and there are additional resource constraints, for\nexample on the number of sensors which can observe a given system\nsimultaneously. We first derive a tractable relaxation of the problem, which\nprovides a bound on the achievable performance. This bound can be computed by\nsolving a convex program involving linear matrix inequalities. Exploiting the\nadditional structure of the sites evolving independently, we can decompose this\nprogram into coupled smaller dimensional problems. In the scalar case with\nidentical sensors, we give an analytical expression of an index policy proposed\nin a more general context by Whittle. In the general case, we develop open-loop\nperiodic switching policies whose performance matches the bound arbitrarily\nclosely.'"
Munther Dahleh,Dahleh_Munther,arXiv:0805.1563,https://arxiv.org/abs/0805.1563,b'Abstract:  We extend a relaxation technique due to Bertsimas and Nino-Mora for the\nrestless bandit problem to the case where arbitrary costs penalize switching\nbetween the bandits. We also construct a one-step lookahead policy using the\nsolution of the relaxation. Computational experiments and a bound for\napproximate dynamic programming provide some empirical support for the\nheuristic.'
Luca Daniel,Daniel_Luca,arXiv:1812.08329,https://arxiv.org/abs/1812.08329,"b""Abstract:  With deep neural networks providing state-of-the-art machine learning models\nfor numerous machine learning tasks, quantifying the robustness of these models\nhas become an important area of research. However, most of the research\nliterature merely focuses on the \\textit{worst-case} setting where the input of\nthe neural network is perturbed with noises that are constrained within an\n$\\ell_p$ ball; and several algorithms have been proposed to compute certified\nlower bounds of minimum adversarial distortion based on such worst-case\nanalysis. In this paper, we address these limitations and extend the approach\nto a \\textit{probabilistic} setting where the additive noises can follow a\ngiven distributional characterization. We propose a novel probabilistic\nframework PROVEN to PRObabilistically VErify Neural networks with statistical\nguarantees -- i.e., PROVEN certifies the probability that the classifier's\ntop-1 prediction cannot be altered under any constrained $\\ell_p$ norm\nperturbation to a given input. Importantly, we show that it is possible to\nderive closed-form probabilistic certificates based on current state-of-the-art\nneural network robustness verification frameworks. Hence, the probabilistic\ncertificates provided by PROVEN come naturally and with almost no overhead when\nobtaining the worst-case certified lower bounds from existing methods such as\nFast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR\nneural network models demonstrate our probabilistic approach can achieve up to\naround $75\\%$ improvement in the robustness certification with at least a\n$99.99\\%$ confidence compared with the worst-case robustness certificate\ndelivered by CROWN."""
Luca Daniel,Daniel_Luca,arXiv:1811.12395,https://arxiv.org/abs/1811.12395,"b'Abstract:  Verifying robustness of neural network classifiers has attracted great\ninterests and attention due to the success of deep neural networks and their\nunexpected vulnerability to adversarial perturbations. Although finding minimum\nadversarial distortion of neural networks (with ReLU activations) has been\nshown to be an NP-complete problem, obtaining a non-trivial lower bound of\nminimum distortion as a provable robustness guarantee is possible. However,\nmost previous works only focused on simple fully-connected layers (multilayer\nperceptrons) and were limited to ReLU activations. This motivates us to propose\na general and efficient framework, CNN-Cert, that is capable of certifying\nrobustness on general convolutional neural networks. Our framework is general\n-- we can handle various architectures including convolutional layers,\nmax-pooling layers, batch normalization layer, residual blocks, as well as\ngeneral activation functions; our approach is efficient -- by exploiting the\nspecial structure of convolutional layers, we achieve up to 17 and 11 times of\nspeed-up compared to the state-of-the-art certification algorithms (e.g.\nFast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach\nwhile our algorithm obtains similar or even better verification bounds. In\naddition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and\nCROWN. We demonstrate by extensive experiments that our method outperforms\nstate-of-the-art lower-bound-based certification algorithms in terms of both\nbound quality and speed.'"
Luca Daniel,Daniel_Luca,arXiv:1811.00866,https://arxiv.org/abs/1811.00866,"b'Abstract:  Finding minimum distortion of adversarial examples and thus certifying\nrobustness in neural network classifiers for given data points is known to be a\nchallenging problem. Nevertheless, recently it has been shown to be possible to\ngive a non-trivial certified lower bound of minimum adversarial distortion, and\nsome recent progress has been made towards this direction by exploiting the\npiece-wise linear nature of ReLU activations. However, a generic robustness\ncertification for general activation functions still remains largely\nunexplored. To address this issue, in this paper we introduce CROWN, a general\nframework to certify robustness of neural networks with general activation\nfunctions for given input data points. The novelty in our algorithm consists of\nbounding a given activation function with linear and quadratic functions, hence\nallowing it to tackle general activation functions including but not limited to\nfour popular choices: ReLU, tanh, sigmoid and arctan. In addition, we\nfacilitate the search for a tighter certified lower bound by adaptively\nselecting appropriate surrogates for each neuron activation. Experimental\nresults show that CROWN on ReLU networks can notably improve the certified\nlower bounds compared to the current state-of-the-art algorithm Fast-Lin, while\nhaving comparable computational efficiency. Furthermore, CROWN also\ndemonstrates its effectiveness and flexibility on networks with general\nactivation functions, including tanh, sigmoid and arctan.'"
Luca Daniel,Daniel_Luca,arXiv:1810.08640,https://arxiv.org/abs/1810.08640,"b'Abstract:  CLEVER (Cross-Lipschitz Extreme Value for nEtwork Robustness) is an Extreme\nValue Theory (EVT) based robustness score for large-scale deep neural networks\n(DNNs). In this paper, we propose two extensions on this robustness score.\nFirst, we provide a new formal robustness guarantee for classifier functions\nthat are twice differentiable. We apply extreme value theory on the new formal\nrobustness guarantee and the estimated robustness is called second-order CLEVER\nscore. Second, we discuss how to handle gradient masking, a common defensive\ntechnique, using CLEVER with Backward Pass Differentiable Approximation (BPDA).\nWith BPDA applied, CLEVER can evaluate the intrinsic robustness of neural\nnetworks of a broader class -- networks with non-differentiable input\ntransformations. We demonstrate the effectiveness of CLEVER with BPDA in\nexperiments on a 121-layer Densenet model trained on the ImageNet dataset.'"
Luca Daniel,Daniel_Luca,arXiv:1804.09699,https://arxiv.org/abs/1804.09699,"b'Abstract:  Verifying the robustness property of a general Rectified Linear Unit (ReLU)\nnetwork is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer\nCAV17]. Although finding the exact minimum adversarial distortion is hard,\ngiving a certified lower bound of the minimum distortion is possible. Current\navailable methods of computing such a bound are either time-consuming or\ndelivering low quality bounds that are too loose to be useful. In this paper,\nwe exploit the special structure of ReLU networks and provide two\ncomputationally efficient algorithms Fast-Lin and Fast-Lip that are able to\ncertify non-trivial lower bounds of minimum distortions, by bounding the ReLU\nunits with appropriate linear functions Fast-Lin, or by bounding the local\nLipschitz constant Fast-Lip. Experiments show that (1) our proposed methods\ndeliver bounds close to (the gap is 2-3X) exact minimum distortion found by\nReluplex in small MNIST networks while our algorithms are more than 10,000\ntimes faster; (2) our methods deliver similar quality of bounds (the gap is\nwithin 35% and usually around 10%; sometimes our bounds are even better) for\nlarger networks compared to the methods based on solving linear programming\nproblems but our algorithms are 33-14,000 times faster; (3) our method is\ncapable of solving large MNIST and CIFAR networks up to 7 layers with more than\n10,000 neurons within tens of seconds on a single CPU core.\nIn addition, we show that, in fact, there is no polynomial time algorithm\nthat can approximately find the minimum $\\ell_1$ adversarial distortion of a\nReLU network with a $0.99\\ln n$ approximation ratio unless\n$\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network.'"
Luca Daniel,Daniel_Luca,arXiv:1801.10578,https://arxiv.org/abs/1801.10578,"b'Abstract:  The robustness of neural networks to adversarial examples has received great\nattention due to security implications. Despite various attack approaches to\ncrafting visually imperceptible adversarial examples, little has been developed\ntowards a comprehensive measure of robustness. In this paper, we provide a\ntheoretical justification for converting robustness analysis into a local\nLipschitz constant estimation problem, and propose to use the Extreme Value\nTheory for efficient evaluation. Our analysis yields a novel robustness metric\ncalled CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork\nRobustness. The proposed CLEVER score is attack-agnostic and computationally\nfeasible for large neural networks. Experimental results on various networks,\nincluding ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned\nwith the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms\nof adversarial examples from powerful attacks, and (ii) defended networks using\ndefensive distillation or bounded ReLU indeed achieve better CLEVER scores. To\nthe best of our knowledge, CLEVER is the first attack-independent robustness\nmetric that can be applied to any neural network classifier.'"
Luca Daniel,Daniel_Luca,arXiv:1707.07803,https://arxiv.org/abs/1707.07803,"b'Abstract:  We propose a new algorithm for the computation of a singular value\ndecomposition (SVD) low-rank approximation of a matrix in the Matrix Product\nOperator (MPO) format, also called the Tensor Train Matrix format. Our tensor\nnetwork randomized SVD (TNrSVD) algorithm is an MPO implementation of the\nrandomized SVD algorithm that is able to compute dominant singular values and\ntheir corresponding singular vectors. In contrast to the state-of-the-art\ntensor-based alternating least squares SVD (ALS-SVD) and modified alternating\nleast squares SVD (MALS-SVD) matrix approximation methods, TNrSVD can be up to\n17 times faster while achieving the same accuracy. In addition, our TNrSVD\nalgorithm also produces accurate approximations in particular cases where both\nALS-SVD and MALS-SVD fail to converge. We also propose a new algorithm for the\nfast conversion of a sparse matrix into its corresponding MPO form, which is up\nto 509 times faster than the standard Tensor Train SVD (TT-SVD) method while\nachieving machine precision accuracy. The efficiency and accuracy of both\nalgorithms are demonstrated in numerical experiments.'"
Luca Daniel,Daniel_Luca,arXiv:1611.02256,https://arxiv.org/abs/1611.02256,"b'Abstract:  Fabrication process variations are a major source of yield degradation in the\nnano-scale design of integrated circuits (IC), microelectromechanical systems\n(MEMS) and photonic circuits. Stochastic spectral methods are a promising\ntechnique to quantify the uncertainties caused by process variations. Despite\ntheir superior efficiency over Monte Carlo for many design cases, these\nalgorithms suffer from the curse of dimensionality; i.e., their computational\ncost grows very fast as the number of random parameters increases. In order to\nsolve this challenging problem, this paper presents a high-dimensional\nuncertainty quantification algorithm from a big-data perspective. Specifically,\nwe show that the huge number of (e.g., $1.5 \\times 10^{27}$) simulation samples\nin standard stochastic collocation can be reduced to a very small one (e.g.,\n$500$) by exploiting some hidden structures of a high-dimensional data array.\nThis idea is formulated as a tensor recovery problem with sparse and low-rank\nconstraints; and it is solved with an alternating minimization approach.\nNumerical results show that our approach can simulate efficiently some ICs, as\nwell as MEMS and photonic problems with over 50 independent random parameters,\nwhereas the traditional algorithm can only handle several random parameters.'"
Luca Daniel,Daniel_Luca,arXiv:1610.04272,https://arxiv.org/abs/1610.04272,"b'Abstract:  Many critical EDA problems suffer from the curse of dimensionality, i.e. the\nvery fast-scaling computational burden produced by large number of parameters\nand/or unknown variables. This phenomenon may be caused by multiple spatial or\ntemporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit\nsimulation), nonlinearity of devices and circuits, large number of design or\noptimization parameters (e.g. full-chip routing/placement and circuit sizing),\nor extensive process variations (e.g. variability/reliability analysis and\ndesign for manufacturability). The computational challenges generated by such\nhigh dimensional problems are generally hard to handle efficiently with\ntraditional EDA core algorithms that are based on matrix and vector\ncomputation. This paper presents ""tensor computation"" as an alternative general\nframework for the development of efficient EDA algorithms and tools. A tensor\nis a high-dimensional generalization of a matrix and a vector, and is a natural\nchoice for both storing and solving efficiently high-dimensional EDA problems.\nThis paper gives a basic tutorial on tensors, demonstrates some recent examples\nof EDA applications (e.g., nonlinear circuit modeling and high-dimensional\nuncertainty quantification), and suggests further open EDA problems where the\nuse of tensor computation could be of advantage.'"
Luca Daniel,Daniel_Luca,arXiv:1603.06119,https://arxiv.org/abs/1603.06119,"b'Abstract:  Stochastic spectral methods have become a popular technique to quantify the\nuncertainties of nano-scale devices and circuits. They are much more efficient\nthan Monte Carlo for certain design cases with a small number of random\nparameters. However, their computational cost significantly increases as the\nnumber of random parameters increases. This paper presents a big-data approach\nto solve high-dimensional uncertainty quantification problems. Specifically, we\nsimulate integrated circuits and MEMS at only a small number of quadrature\nsamples, then, a huge number of (e.g., $1.5 \\times 10^{27}$) solution samples\nare estimated from the available small-size (e.g., $500$) solution samples via\na low-rank and tensor-recovery method. Numerical results show that our\nalgorithm can easily extend the applicability of tensor-product stochastic\ncollocation to IC and MEMS problems with over 50 random parameters, whereas the\ntraditional algorithm can only handle several random parameters.'"
Luca Daniel,Daniel_Luca,arXiv:1508.02489,https://arxiv.org/abs/1508.02489,"b'Abstract:  This paper presents a tensor-recovery method to solve probabilistic power\nflow problems. Our approach generates a high-dimensional and sparse generalized\npolynomial-chaos expansion that provides useful statistical information. The\nresult can also speed up other essential routines in power systems (e.g.,\nstochastic planning, operations and controls).\nInstead of simulating a power flow equation at all quadrature points, our\napproach only simulates an extremely small subset of samples. We suggest a\nmodel to exploit the underlying low-rank and sparse structure of\nhigh-dimensional simulation data arrays, making our technique applicable to\npower systems with many random parameters. We also present a numerical method\nto solve the resulting nonlinear optimization problem.\nOur algorithm is implemented in MATLAB and is verified by several benchmarks\nin MATPOWER $5.1$. Accurate results are obtained for power systems with up to\n$50$ independent random parameters, with a speedup factor up to $9\\times\n10^{20}$.'"
Luca Daniel,Daniel_Luca,arXiv:1409.4831,https://arxiv.org/abs/1409.4831,"b'Abstract:  Uncertainties have become a major concern in integrated circuit design. In\norder to avoid the huge number of repeated simulations in conventional Monte\nCarlo flows, this paper presents an intrusive spectral simulator for\nstatistical circuit analysis. Our simulator employs the recently developed\ngeneralized polynomial chaos expansion to perform uncertainty quantification of\nnonlinear transistor circuits with both Gaussian and non-Gaussian random\nparameters. We modify the nonintrusive stochastic collocation (SC) method and\ndevelop an intrusive variant called stochastic testing (ST) method to\naccelerate the numerical simulation. Compared with the stochastic Galerkin (SG)\nmethod, the resulting coupled deterministic equations from our proposed ST\nmethod can be solved in a decoupled manner at each time point. At the same\ntime, ST uses fewer samples and allows more flexible time step size controls\nthan directly using a nonintrusive SC solver. These two properties make ST more\nefficient than SG and than existing SC methods, and more suitable for\ntime-domain circuit simulation. Simulation results of several digital, analog\nand RF circuits are reported. Since our algorithm is based on generic\nmathematical models, the proposed ST algorithm can be applied to many other\nengineering problems.'"
Luca Daniel,Daniel_Luca,arXiv:1409.4829,https://arxiv.org/abs/1409.4829,"b'Abstract:  Stochastic spectral methods are efficient techniques for uncertainty\nquantification. Recently they have shown excellent performance in the\nstatistical analysis of integrated circuits. In stochastic spectral methods,\none needs to determine a set of orthonormal polynomials and a proper numerical\nquadrature rule. The former are used as the basis functions in a generalized\npolynomial chaos expansion. The latter is used to compute the integrals\ninvolved in stochastic spectral methods. Obtaining such information requires\nknowing the density function of the random input {\\it a-priori}. However,\nindividual system components are often described by surrogate models rather\nthan density functions. In order to apply stochastic spectral methods in\nhierarchical uncertainty quantification, we first propose to construct\nphysically consistent closed-form density functions by two monotone\ninterpolation schemes. Then, by exploiting the special forms of the obtained\ndensity functions, we determine the generalized polynomial-chaos basis\nfunctions and the Gauss quadrature rules that are required by a stochastic\nspectral simulator. The effectiveness of our proposed algorithm is verified by\nboth synthetic and practical circuit examples.'"
Luca Daniel,Daniel_Luca,arXiv:1409.4826,https://arxiv.org/abs/1409.4826,b'Abstract:  This brief paper proposes an uncertainty quantification method for the\nperiodic steady-state (PSS) analysis with both Gaussian and non-Gaussian\nvariations. Our stochastic testing formulation for the PSS problem provides\nsuperior efficiency over both Monte Carlo methods and existing spectral\nmethods. The numerical implementation of a stochastic shooting Newton solver is\npresented for both forced and autonomous circuits. Simulation results on some\nanalog/RF circuits are reported to show the effectiveness of our proposed\nalgorithms.'
Luca Daniel,Daniel_Luca,arXiv:1409.4824,https://arxiv.org/abs/1409.4824,"b'Abstract:  Due to significant manufacturing process variations, the performance of\nintegrated circuits (ICs) has become increasingly uncertain. Such uncertainties\nmust be carefully quantified with efficient stochastic circuit simulators. This\npaper discusses the recent advances of stochastic spectral circuit simulators\nbased on generalized polynomial chaos (gPC). Such techniques can handle both\nGaussian and non-Gaussian random parameters, showing remarkable speedup over\nMonte Carlo for circuits with a small or medium number of parameters. We focus\non the recently developed stochastic testing and the application of\nconventional stochastic Galerkin and stochastic collocation schemes to\nnonlinear circuit problems. The uncertainty quantification algorithms for\nstatic, transient and periodic steady-state simulations are presented along\nwith some practical simulation results. Some open problems in this field are\ndiscussed.'"
Luca Daniel,Daniel_Luca,arXiv:1409.4822,https://arxiv.org/abs/1409.4822,"b""Abstract:  Process variations are a major concern in today's chip design since they can\nsignificantly degrade chip performance. To predict such degradation, existing\ncircuit and MEMS simulators rely on Monte Carlo algorithms, which are typically\ntoo slow. Therefore, novel fast stochastic simulators are highly desired. This\npaper first reviews our recently developed stochastic testing simulator that\ncan achieve speedup factors of hundreds to thousands over Monte Carlo. Then, we\ndevelop a fast hierarchical stochastic spectral simulator to simulate a complex\ncircuit or system consisting of several blocks. We further present a fast\nsimulation approach based on anchored ANOVA (analysis of variance) for some\ndesign problems with many process variations. This approach can reduce the\nsimulation cost and can identify which variation sources have strong impacts on\nthe circuit's performance. The simulation results of some circuit and MEMS\nexamples are reported to show the effectiveness of our simulator"""
Luca Daniel,Daniel_Luca,arXiv:1407.3023,https://arxiv.org/abs/1407.3023,"b'Abstract:  Hierarchical uncertainty quantification can reduce the computational cost of\nstochastic circuit simulation by employing spectral methods at different\nlevels. This paper presents an efficient framework to simulate hierarchically\nsome challenging stochastic circuits/systems that include high-dimensional\nsubsystems. Due to the high parameter dimensionality, it is challenging to both\nextract surrogate models at the low level of the design hierarchy and to handle\nthem in the high-level simulation. In this paper, we develop an efficient\nANOVA-based stochastic circuit/MEMS simulator to extract efficiently the\nsurrogate models at the low level. In order to avoid the curse of\ndimensionality, we employ tensor-train decomposition at the high level to\nconstruct the basis functions and Gauss quadrature points. As a demonstration,\nwe verify our algorithm on a stochastic oscillator with four MEMS capacitors\nand 184 random parameters. This challenging example is simulated efficiently by\nour simulator at the cost of only 10 minutes in MATLAB on a regular personal\ncomputer.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1811.10581,https://arxiv.org/abs/1811.10581,"b""Abstract:  Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an\naccurate method for estimating probabilities of events on a small number of\nvariables of a graphical model satisfying Dobrushin's\ncondition~\\cite{DeSaOR16}. We investigate whether it can be used to accurately\nestimate expectations of functions of {\\em all the variables} of the model.\nUnder the same condition, we show that the synchronous (sequential) and\nasynchronous Gibbs samplers can be coupled so that the expected Hamming\ndistance between their (multivariate) samples remains bounded by $O(\\tau \\log\nn),$ where $n$ is the number of variables in the graphical model, and $\\tau$ is\na measure of the asynchronicity. A similar bound holds for any constant power\nof the Hamming distance. Hence, the expectation of any function that is\nLipschitz with respect to a power of the Hamming distance, can be estimated\nwith a bias that grows logarithmically in $n$. Going beyond Lipschitz\nfunctions, we consider the bias arising from asynchronicity in estimating the\nexpectation of polynomial functions of all variables in the model. Using recent\nconcentration of measure results, we show that the bias introduced by the\nasynchronicity is of smaller order than the standard deviation of the function\nvalue already present in the true model. We perform experiments on a\nmulti-processor machine to empirically illustrate our theoretical findings."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1810.11896,https://arxiv.org/abs/1810.11896,"b""Abstract:  We analyze linear independence of rank one tensors produced by tensor powers\nof randomly perturbed vectors. This enables efficient decomposition of sums of\nhigh-order tensors. Our analysis builds upon [BCMV14] but allows for a wider\nrange of perturbation models, including discrete ones. We give an application\nto recovering assemblies of neurons.\nAssemblies are large sets of neurons representing specific memories or\nconcepts. The size of the intersection of two assemblies has been shown in\nexperiments to represent the extent to which these memories co-occur or these\nconcepts are related; the phenomenon is called association of assemblies. This\nsuggests that an animal's memory is a complex web of associations, and poses\nthe problem of recovering this representation from cognitive data. Motivated by\nthis problem, we study the following more general question: Can we reconstruct\nthe Venn diagram of a family of sets, given the sizes of their $\\ell$-wise\nintersections? We show that as long as the family of sets is randomly\nperturbed, it is enough for the number of measurements to be polynomially\nlarger than the number of nonempty regions of the Venn diagram to fully\nreconstruct the diagram."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1809.03986,https://arxiv.org/abs/1809.03986,"b'Abstract:  We provide an efficient algorithm for the classical problem, going back to\nGalton, Pearson, and Fisher, of estimating, with arbitrary accuracy the\nparameters of a multivariate normal distribution from truncated samples.\nTruncated samples from a $d$-variate normal ${\\cal\nN}(\\mathbf{\\mu},\\mathbf{\\Sigma})$ means a samples is only revealed if it falls\nin some subset $S \\subseteq \\mathbb{R}^d$; otherwise the samples are hidden and\ntheir count in proportion to the revealed samples is also hidden. We show that\nthe mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$ can be\nestimated with arbitrary accuracy in polynomial-time, as long as we have oracle\naccess to $S$, and $S$ has non-trivial measure under the unknown $d$-variate\nnormal distribution. Additionally we show that without oracle access to $S$,\nany non-trivial estimation is impossible.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1807.04252,https://arxiv.org/abs/1807.04252,"b'Abstract:  Motivated by applications in Game Theory, Optimization, and Generative\nAdversarial Networks, recent work of Daskalakis et al~\\cite{DISZ17} and\nfollow-up work of Liang and Stokes~\\cite{LiangS18} have established that a\nvariant of the widely used Gradient Descent/Ascent procedure, called\n""Optimistic Gradient Descent/Ascent (OGDA)"", exhibits last-iterate convergence\nto saddle points in {\\em unconstrained} convex-concave min-max optimization\nproblems. We show that the same holds true in the more general problem of {\\em\nconstrained} min-max optimization under a variant of the no-regret\nMultiplicative-Weights-Update method called ""Optimistic Multiplicative-Weights\nUpdate (OMWU)"". This answers an open question of Syrgkanis et al~\\cite{SALS15}.\nThe proof of our result requires fundamentally different techniques from\nthose that exist in no-regret learning literature and the aforementioned\npapers. We show that OMWU monotonically improves the Kullback-Leibler\ndivergence of the current iterate to the (appropriately normalized) min-max\nsolution until it enters a neighborhood of the solution. Inside that\nneighborhood we show that OMWU becomes a contracting map converging to the\nexact solution. We believe that our techniques will be useful in the analysis\nof the last iterate of other learning algorithms.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1807.03907,https://arxiv.org/abs/1807.03907,"b'Abstract:  Motivated by applications in Optimization, Game Theory, and the training of\nGenerative Adversarial Networks, the convergence properties of first order\nmethods in min-max problems have received extensive study. It has been\nrecognized that they may cycle, and there is no good understanding of their\nlimit points when they do not. When they converge, do they converge to local\nmin-max solutions? We characterize the limit points of two basic first order\nmethods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent\nAscent (OGDA). We show that both dynamics avoid unstable critical points for\nalmost all initializations. Moreover, for small step sizes and under mild\nassumptions, the set of \\{OGDA\\}-stable critical points is a superset of\n\\{GDA\\}-stable critical points, which is a superset of local min-max solutions\n(strict in some cases). The connecting thread is that the behavior of these\ndynamics can be studied from a dynamical systems perspective.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1805.09697,https://arxiv.org/abs/1805.09697,"b""Abstract:  We consider testing and learning problems on causal Bayesian networks as\ndefined by Pearl (Pearl, 2009). Given a causal Bayesian network $\\mathcal{M}$\non a graph with $n$ discrete variables and bounded in-degree and bounded\n`confounded components', we show that $O(\\log n)$ interventions on an unknown\ncausal Bayesian network $\\mathcal{X}$ on the same graph, and\n$\\tilde{O}(n/\\epsilon^2)$ samples per intervention, suffice to efficiently\ndistinguish whether $\\mathcal{X}=\\mathcal{M}$ or whether there exists some\nintervention under which $\\mathcal{X}$ and $\\mathcal{M}$ are farther than\n$\\epsilon$ in total variation distance. We also obtain sample/time/intervention\nefficient algorithms for: (i) testing the identity of two unknown causal\nBayesian networks on the same graph; and (ii) learning a causal Bayesian\nnetwork on a given graph. Although our algorithms are non-adaptive, we show\nthat adaptivity does not help in general: $\\Omega(\\log n)$ interventions are\nnecessary for testing the identity of two unknown causal Bayesian networks on\nthe same graph, even adaptively. Our algorithms are enabled by a new\nsubadditivity inequality for the squared Hellinger distance between two causal\nBayesian networks."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1803.00494,https://arxiv.org/abs/1803.00494,"b'Abstract:  We study revenue optimization in a repeated auction between a single seller\nand a single buyer. Traditionally, the design of repeated auctions requires\nstrong modeling assumptions about the bidder behavior, such as it being myopic,\ninfinite lookahead, or some specific form of learning behavior. Is it possible\nto design mechanisms which are simultaneously optimal against a multitude of\npossible buyer behaviors? We answer this question by designing a simple\nstate-based mechanism that is simultaneously approximately optimal against a\n$k$-lookahead buyer for all $k$, a buyer who is a no-regret learner, and a\nbuyer who is a policy-regret learner. Against each type of buyer our mechanism\nattains a constant fraction of the optimal revenue attainable against that type\nof buyer. We complement our positive results with almost tight impossibility\nresults, showing that the revenue approximation tradeoffs achieved by our\nmechanism for different lookahead attitudes are near-optimal.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1712.09196,https://arxiv.org/abs/1712.09196,"b'Abstract:  Deep neural networks are demonstrating excellent performance on several\nclassical vision problems. However, these networks are vulnerable to\nadversarial examples, minutely modified images that induce arbitrary\nattacker-chosen output from the network. We propose a mechanism to protect\nagainst these adversarial inputs based on a generative model of the data. We\nintroduce a pre-processing step that projects on the range of a generative\nmodel using gradient descent before feeding an input into a classifier. We show\nthat this step provides the classifier with robustness against first-order,\nsubstitute model, and combined adversarial attacks. Using a min-max\nformulation, we show that there may exist adversarial examples even in the\nrange of the generator, natural-looking images extremely close to the decision\nboundary for which the classifier has unjustifiedly high confidence. We show\nthat adversarial training on the generative manifold can be used to make a\nclassifier that is robust to these attacks.\nFinally, we show how our method can be applied even without a pre-trained\ngenerative model using a recent method called the deep image prior. We evaluate\nour method on MNIST, CelebA and Imagenet and show robustness against the\ncurrent state of the art attacks.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1711.00141,https://arxiv.org/abs/1711.00141,"b'Abstract:  We address the issue of limit cycling behavior in training Generative\nAdversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for\ntraining Wasserstein GANs. Recent theoretical results have shown that\noptimistic mirror decent (OMD) can enjoy faster regret rates in the context of\nzero-sum games. WGANs is exactly a context of solving a zero-sum game with\nsimultaneous no-regret dynamics. Moreover, we show that optimistic mirror\ndecent addresses the limit cycling problem in training WGANs. We formally show\nthat in the case of bi-linear zero-sum games the last iterate of OMD dynamics\nconverges to an equilibrium, in contrast to GD dynamics which are bound to\ncycle. We also portray the huge qualitative difference between GD and OMD\ndynamics with toy examples, even when GD is modified with many adaptations\nproposed in the recent literature, such as gradient penalty or momentum. We\napply OMD WGAN training to a bioinformatics problem of generating DNA\nsequences. We observe that models trained with OMD achieve consistently smaller\nKL divergence with respect to the true underlying distribution, than models\ntrained with GD variants. Finally, we introduce a new algorithm, Optimistic\nAdam, which is an optimistic variant of Adam. We apply it to WGAN training on\nCIFAR10 and observe improved performance in terms of inception score as\ncompared to Adam.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1710.04170,https://arxiv.org/abs/1710.04170,"b'Abstract:  We prove near-tight concentration of measure for polynomial functions of the\nIsing model under high temperature. For any degree $d$, we show that a\ndegree-$d$ polynomial of a $n$-spin Ising model exhibits exponential tails that\nscale as $\\exp(-r^{2/d})$ at radius $r=\\tilde{\\Omega}_d(n^{d/2})$. Our\nconcentration radius is optimal up to logarithmic factors for constant $d$,\nimproving known results by polynomial factors in the number of spins. We\ndemonstrate the efficacy of polynomial functions as statistics for testing the\nstrength of interactions in social networks in both synthetic and real world\ndata.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1709.00228,https://arxiv.org/abs/1709.00228,"b'Abstract:  We provide algorithms that learn simple auctions whose revenue is\napproximately optimal in multi-item multi-bidder settings, for a wide range of\nvaluations including unit-demand, additive, constrained additive, XOS, and\nsubadditive. We obtain our learning results in two settings. The first is the\ncommonly studied setting where sample access to the bidders\' distributions over\nvaluations is given, for both regular distributions and arbitrary distributions\nwith bounded support. Our algorithms require polynomially many samples in the\nnumber of items and bidders. The second is a more general max-min learning\nsetting that we introduce, where we are given ""approximate distributions,"" and\nwe seek to compute an auction whose revenue is approximately optimal\nsimultaneously for all ""true distributions"" that are close to the given ones.\nThese results are more general in that they imply the sample-based results, and\nare also applicable in settings where we have no sample access to the\nunderlying distributions but have estimated them indirectly via market research\nor by observation of previously run, potentially non-truthful auctions.\nOur results hold for valuation distributions satisfying the standard (and\nnecessary) independence-across-items property. They also generalize and improve\nupon recent works, which have provided algorithms that learn approximately\noptimal auctions in more restricted settings with additive, subadditive and\nunit-demand valuations using sample access to distributions. We generalize\nthese results to the complete unit-demand, additive, and XOS setting, to i.i.d.\nsubadditive bidders, and to the max-min setting.\nOur results are enabled by new uniform convergence bounds for hypotheses\nclasses under product measures. Our bounds result in exponential savings in\nsample complexity compared to bounds derived by bounding the VC dimension, and\nare of independent interest.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1708.00002,https://arxiv.org/abs/1708.00002,"b'Abstract:  Given samples from an unknown distribution $p$ and a description of a\ndistribution $q$, are $p$ and $q$ close or far? This question of ""identity\ntesting"" has received significant attention in the case of testing whether $p$\nand $q$ are equal or far in total variation distance. However, in recent work,\nthe following questions have been been critical to solving problems at the\nfrontiers of distribution testing:\n-Alternative Distances: Can we test whether $p$ and $q$ are far in other\ndistances, say Hellinger?\n-Tolerance: Can we test when $p$ and $q$ are close, rather than equal? And if\nso, close in which distances?\nMotivated by these questions, we characterize the complexity of distribution\ntesting under a variety of distances, including total variation, $\\ell_2$,\nHellinger, Kullback-Leibler, and $\\chi^2$. For each pair of distances $d_1$ and\n$d_2$, we study the complexity of testing if $p$ and $q$ are close in $d_1$\nversus far in $d_2$, with a focus on identifying which problems allow strongly\nsublinear testers (i.e., those with complexity $O(n^{1 - \\gamma})$ for some\n$\\gamma > 0$ where $n$ is the size of the support of the distributions $p$ and\n$q$). We provide matching upper and lower bounds for each case. We also study\nthese questions in the case where we only have samples from $q$ (equivalence\ntesting), showing qualitative differences from identity testing in terms of\nwhen tolerance can be achieved. Our algorithms fall into the classical paradigm\nof $\\chi^2$-statistics, but require crucial changes to handle the challenges\nintroduced by each distance we consider. Finally, we survey other recent\nresults in an attempt to serve as a reference for the complexity of various\ndistribution testing problems.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1704.06850,https://arxiv.org/abs/1704.06850,"b'Abstract:  Classical distribution testing assumes access to i.i.d. samples from the\ndistribution that is being tested. We initiate the study of Markov chain\ntesting, assuming access to a single trajectory of a Markov Chain. In\nparticular, we observe a single trajectory X0,...,Xt,... of an unknown,\nsymmetric, and finite state Markov Chain M. We do not control the starting\nstate X0, and we cannot restart the chain. Given our single trajectory, the\ngoal is to test whether M is identical to a model Markov Chain M0 , or far from\nit under an appropriate notion of difference. We propose a measure of\ndifference between two Markov chains, motivated by the early work of Kazakos\n[Kaz78], which captures the scaling behavior of the total variation distance\nbetween trajectories sampled from the Markov chains as the length of these\ntrajectories grows. We provide efficient testers and information-theoretic\nlower bounds for testing identity of symmetric Markov chains under our proposed\nmeasure of difference, which are tight up to logarithmic factors if the hitting\ntimes of the model chain M0 is O(n) in the size of the state space n.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1703.10127,https://arxiv.org/abs/1703.10127,"b'Abstract:  We develop differentially private hypothesis testing methods for the small\nsample regime. Given a sample $\\cal D$ from a categorical distribution $p$ over\nsome domain $\\Sigma$, an explicitly described distribution $q$ over $\\Sigma$,\nsome privacy parameter $\\varepsilon$, accuracy parameter $\\alpha$, and\nrequirements $\\beta_{\\rm I}$ and $\\beta_{\\rm II}$ for the type I and type II\nerrors of our test, the goal is to distinguish between $p=q$ and\n$d_{\\rm{TV}}(p,q) \\geq \\alpha$.\nWe provide theoretical bounds for the sample size $|{\\cal D}|$ so that our\nmethod both satisfies $(\\varepsilon,0)$-differential privacy, and guarantees\n$\\beta_{\\rm I}$ and $\\beta_{\\rm II}$ type I and type II errors. We show that\ndifferential privacy may come for free in some regimes of parameters, and we\nalways beat the sample complexity resulting from running the $\\chi^2$-test with\nnoisy counts, or standard approaches such as repetition for endowing\nnon-private $\\chi^2$-style statistics with differential privacy guarantees. We\nexperimentally compare the sample complexity of our method to that of recently\nproposed methods for private hypothesis testing.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1702.07339,https://arxiv.org/abs/1702.07339,"b""Abstract:  Banach's fixed point theorem for contraction maps has been widely used to\nanalyze the convergence of iterative methods in non-convex problems. It is a\ncommon experience, however, that iterative maps fail to be globally contracting\nunder the natural metric in their domain, making the applicability of Banach's\ntheorem limited. We explore how generally we can apply Banach's fixed point\ntheorem to establish the convergence of iterative methods when pairing it with\ncarefully designed metrics.\nOur first result is a strong converse of Banach's theorem, showing that it is\na universal analysis tool for establishing global convergence of iterative\nmethods to unique fixed points, and for bounding their convergence rate. In\nother words, we show that, whenever an iterative map globally converges to a\nunique fixed point, there exists a metric under which the iterative map is\ncontracting and which can be used to bound the number of iterations until\nconvergence. We illustrate our approach in the widely used power method,\nproviding a new way of bounding its convergence rate through contraction\narguments.\nWe next consider the computational complexity of Banach's fixed point\ntheorem. Making the proof of our converse theorem constructive, we show that\ncomputing a fixed point whose existence is guaranteed by Banach's fixed point\ntheorem is CLS-complete. We thus provide the first natural complete problem for\nthe class CLS, which was defined in [Daskalakis, Papadimitriou 2011] to capture\nthe complexity of problems such as P-matrix LCP, computing KKT-points, and\nfinding mixed Nash equilibria in congestion and network coordination games."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1612.03164,https://arxiv.org/abs/1612.03164,"b'Abstract:  We show that the square Hellinger distance between two Bayesian networks on\nthe same directed graph, $G$, is subadditive with respect to the neighborhoods\nof $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two\nBayesian networks on the same DAG, our inequality states that the square\nHellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the\nsum, $\\sum_v H^2(P_{\\{v\\} \\cup \\Pi_v}, Q_{\\{v\\} \\cup \\Pi_v})$, of the square\nHellinger distances between the marginals of $P$ and $Q$ on every node $v$ and\nits parents $\\Pi_v$ in the DAG. Importantly, our bound does not involve the\nconditionals but the marginals of $P$ and $Q$. We derive a similar inequality\nfor more general Markov Random Fields.\nAs an application of our inequality, we show that distinguishing whether two\nBayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy\n$P=Q$ vs $d_{\\rm TV}(P,Q)>\\epsilon$ can be performed from\n$\\tilde{O}(|\\Sigma|^{3/4(d+1)} \\cdot n/\\epsilon^2)$ samples, where $d$ is the\nmaximum in-degree of the DAG and $\\Sigma$ the domain of each variable of the\nBayesian networks. If $P$ and $Q$ are defined on potentially different and\npotentially unknown trees, the sample complexity becomes\n$\\tilde{O}(|\\Sigma|^{4.5} n/\\epsilon^2)$, whose dependence on $n, \\epsilon$ is\noptimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product\ndistributions over $\\{0,1\\}^n$ and $Q$ is known, the sample complexity becomes\n$O(\\sqrt{n}/\\epsilon^2)$, which is optimal up to constant factors.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1612.03147,https://arxiv.org/abs/1612.03147,"b'Abstract:  Given samples from an unknown multivariate distribution $p$, is it possible\nto distinguish whether $p$ is the product of its marginals versus $p$ being far\nfrom every product distribution? Similarly, is it possible to distinguish\nwhether $p$ equals a given distribution $q$ versus $p$ and $q$ being far from\neach other? These problems of testing independence and goodness-of-fit have\nreceived enormous attention in statistics, information theory, and theoretical\ncomputer science, with sample-optimal algorithms known in several interesting\nregimes of parameters. Unfortunately, it has also been understood that these\nproblems become intractable in large dimensions, necessitating exponential\nsample complexity.\nMotivated by the exponential lower bounds for general distributions as well\nas the ubiquity of Markov Random Fields (MRFs) in the modeling of\nhigh-dimensional distributions, we initiate the study of distribution testing\non structured multivariate distributions, and in particular the prototypical\nexample of MRFs: the Ising Model. We demonstrate that, in this structured\nsetting, we can avoid the curse of dimensionality, obtaining sample and time\nefficient testers for independence and goodness-of-fit. One of the key\ntechnical challenges we face along the way is bounding the variance of\nfunctions of the Ising model.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1609.00368,https://arxiv.org/abs/1609.00368,"b'Abstract:  The Expectation-Maximization (EM) algorithm is a widely used method for\nmaximum likelihood estimation in models with latent variables. For estimating\nmixtures of Gaussians, its iteration can be viewed as a soft version of the\nk-means clustering algorithm. Despite its wide use and applications, there are\nessentially no known convergence guarantees for this method. We provide global\nconvergence guarantees for mixtures of two Gaussians with known covariance\nmatrices. We show that the population version of EM, where the algorithm is\ngiven access to infinitely many samples from the mixture, converges\ngeometrically to the correct mean vectors, and provide simple, closed-form\nexpressions for the convergence rate. As a simple illustration, we show that,\nin one dimension, ten steps of the EM algorithm initialized at infinity result\nin less than 1\\% error estimation of the means. In the finite sample regime, we\nshow that, under a random initialization, $\\tilde{O}(d/\\epsilon^2)$ samples\nsuffice to compute the unknown vectors to within $\\epsilon$ in Mahalanobis\ndistance, where $d$ is the dimension. In particular, the error rate of the EM\nbased estimator is $\\tilde{O}\\left(\\sqrt{d \\over n}\\right)$ where $n$ is the\nnumber of samples, which is optimal up to logarithmic factors.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1605.02054,https://arxiv.org/abs/1605.02054,"b'Abstract:  We consider the problem of a revenue-maximizing seller with m items for sale\nto n additive bidders with hard budget constraints, assuming that the seller\nhas some prior distribution over bidder values and budgets. The prior may be\ncorrelated across items and budgets of the same bidder, but is assumed\nindependent across bidders. We target mechanisms that are Bayesian Incentive\nCompatible, but that are ex-post Individually Rational and ex-post budget\nrespecting. Virtually no such mechanisms are known that satisfy all these\nconditions and guarantee any revenue approximation, even with just a single\nitem. We provide a computationally efficient mechanism that is a\n$3$-approximation with respect to all BIC, ex-post IR, and ex-post budget\nrespecting mechanisms. Note that the problem is NP-hard to approximate better\nthan a factor of 16/15, even in the case where the prior is a point mass\n\\cite{ChakrabartyGoel}. We further characterize the optimal mechanism in this\nsetting, showing that it can be interpreted as a distribution over virtual\nwelfare maximizers.\nWe prove our results by making use of a black-box reduction from mechanism to\nalgorithm design developed by \\cite{CaiDW13b}. Our main technical contribution\nis a computationally efficient $3$-approximation algorithm for the algorithmic\nproblem that results by an application of their framework to this problem. The\nalgorithmic problem has a mixed-sign objective and is NP-hard to optimize\nexactly, so it is surprising that a computationally efficient approximation is\npossible at all. In the case of a single item ($m=1$), the algorithmic problem\ncan be solved exactly via exhaustive search, leading to a computationally\nefficient exact algorithm and a stronger characterization of the optimal\nmechanism as a distribution over virtual value maximizers.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1603.07229,https://arxiv.org/abs/1603.07229,"b'Abstract:  We provide a characterization of revenue-optimal dynamic mechanisms in\nsettings where a monopolist sells k items over k periods to a buyer who\nrealizes his value for item i in the beginning of period i. We require that the\nmechanism satisfies a strong individual rationality constraint, requiring that\nthe stage utility of each agent be positive during each period. We show that\nthe optimum mechanism can be computed by solving a nested sequence of static\n(single-period) mechanisms that optimize a tradeoff between the surplus of the\nallocation and the buyer\'s utility. We also provide a simple dynamic mechanism\nthat obtains at least half of the optimal revenue. The mechanism either ignores\nhistory and posts the optimal monopoly price in each period, or allocates with\na probability that is independent of the current report of the agent and is\nbased only on previous reports. Our characterization extends to multi-agent\nauctions. We also formulate a discounted infinite horizon version of the\nproblem, where we study the performance of ""Markov mechanisms.""'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1511.03641,https://arxiv.org/abs/1511.03641,"b'Abstract:  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the\nsum of $n$ independent random vectors supported on the set ${\\cal\nB}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We show\nthat any $(n,k)$-PMD is ${\\rm poly}\\left({k\\over \\sigma}\\right)$-close in total\nvariation distance to the (appropriately discretized) multi-dimensional\nGaussian with the same first two moments, removing the dependence on $n$ from\nthe Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT is\nobtained by bootstrapping the Valiant-Valiant CLT itself through the structural\ncharacterization of PMDs shown in recent work by Daskalakis, Kamath, and\nTzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTAS\nfor approximate Nash equilibria in anonymous games, significantly improving the\nstate of the art, and matching qualitatively the running time dependence on $n$\nand $1/\\varepsilon$ of the best known algorithm for two-strategy anonymous\ngames. Our new CLT also enables the construction of covers for the set of\n$(n,k)$-PMDs, which are proper and whose size is shown to be essentially\noptimal. Our cover construction combines our CLT with the Shapley-Folkman\ntheorem and recent sparsification results for Laplacian matrices by Batson,\nSpielman, and Srivastava. Our cover size lower bound is based on an algebraic\ngeometric construction. Finally, leveraging the structural properties of the\nFourier spectrum of PMDs we show that these distributions can be learned from\n$O_k(1/\\varepsilon^2)$ samples in ${\\rm poly}_k(1/\\varepsilon)$-time, removing\nthe quasi-polynomial dependence of the running time on $1/\\varepsilon$ from the\nalgorithm of Daskalakis, Kamath, and Tzamos.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1511.01411,https://arxiv.org/abs/1511.01411,"b'Abstract:  A line of recent work provides welfare guarantees of simple combinatorial\nauction formats, such as selling m items via simultaneous second price auctions\n(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et\nal. 2013). These guarantees hold even when the auctions are repeatedly executed\nand players use no-regret learning algorithms. Unfortunately, off-the-shelf\nno-regret algorithms for these auctions are computationally inefficient as the\nnumber of actions is exponential. We show that this obstacle is insurmountable:\nthere are no polynomial-time no-regret algorithms for SiSPAs, unless\nRP$\\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises\nthe question of how good outcomes polynomially-bounded bidders may discover in\nsuch auctions.\nTo answer this question, we propose a novel concept of learning in auctions,\ntermed ""no-envy learning."" This notion is founded upon Walrasian equilibrium,\nand we show that it is both efficiently implementable and results in\napproximately optimal welfare, even when the bidders have fractionally\nsubadditive (XOS) valuations (assuming demand oracles) or coverage valuations\n(without demand oracles). No-envy learning outcomes are a relaxation of\nno-regret outcomes, which maintain their approximate welfare optimality while\nendowing them with computational tractability. Our results extend to other\nauction formats that have been studied in the literature via the smoothness\nparadigm.\nOur results for XOS valuations are enabled by a novel\nFollow-The-Perturbed-Leader algorithm for settings where the number of experts\nis infinite, and the payoff function of the learner is non-linear. This\nalgorithm has applications outside of auction settings, such as in security\ngames. Our result for coverage valuations is based on a novel use of convex\nrounding schemes and a reduction to online convex optimization.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1508.01962,https://arxiv.org/abs/1508.01962,"b'Abstract:  Reconstructing the tree of life from molecular sequences is a fundamental\nproblem in computational biology. Modern data sets often contain a large number\nof genes, which can complicate the reconstruction problem due to the fact that\ndifferent genes may undergo different evolutionary histories. This is the case\nin particular in the presence of horizontal genetic transfer (HGT), where a\ngene is inherited from a distant species rather than an immediate ancestor.\nSuch an event produces a gene tree which is distinct from, but related to, the\nspecies phylogeny.\nIn previous work, a natural stochastic models of HGT was introduced and\nstudied. It was shown, both in simulation and theoretical studies, that a\nspecies phylogeny can be reconstructed from gene trees despite surprisingly\nhigh rates of HGT under this model. Rigorous lower and upper bounds on this\nachievable rate were also obtained, but a large gap remained. Here we close\nthis gap, up to a constant. Specifically we show that a species phylogeny can\nbe reconstructed correctly from gene trees even when, on each gene, each edge\nof the species tree has a constant probability of being the location of an HGT\nevent. Our new reconstruction algorithm, which relies only on unrooted gene\ntree topologies, builds the tree recursively from the leaves and runs in\npolynomial time.\nWe also provide a matching bound in the negative direction (up to a constant)\nand extend our results to some cases where gene trees are not perfectly known.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1507.05952,https://arxiv.org/abs/1507.05952,"b'Abstract:  Given samples from an unknown distribution $p$, is it possible to distinguish\nwhether $p$ belongs to some class of distributions $\\mathcal{C}$ versus $p$\nbeing far from every distribution in $\\mathcal{C}$? This fundamental question\nhas received tremendous attention in statistics, focusing primarily on\nasymptotic analysis, and more recently in information theory and theoretical\ncomputer science, where the emphasis has been on small sample size and\ncomputational complexity. Nevertheless, even for basic properties of\ndistributions such as monotonicity, log-concavity, unimodality, independence,\nand monotone-hazard rate, the optimal sample complexity is unknown.\nWe provide a general approach via which we obtain sample-optimal and\ncomputationally efficient testers for all these distribution families. At the\ncore of our approach is an algorithm which solves the following problem: Given\nsamples from an unknown distribution $p$, and a known distribution $q$, are $p$\nand $q$ close in $\\chi^2$-distance, or far in total variation distance?\nThe optimality of our testers is established by providing matching lower\nbounds with respect to both $n$ and $\\varepsilon$. Finally, a necessary\nbuilding block for our testers and an important byproduct of our work are the\nfirst known computationally efficient proper learners for discrete log-concave\nand monotone hazard rate distributions.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1504.08363,https://arxiv.org/abs/1504.08363,"b""Abstract:  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the\nsum of $n$ independent random vectors supported on the set ${\\cal\nB}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We prove\na structural characterization of these distributions, showing that, for all\n$\\varepsilon >0$, any $(n, k)$-Poisson multinomial random vector is\n$\\varepsilon$-close, in total variation distance, to the sum of a discretized\nmultidimensional Gaussian and an independent $(\\text{poly}(k/\\varepsilon),\nk)$-Poisson multinomial random vector. Our structural characterization extends\nthe multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to\nall approximation requirements $\\varepsilon$. In particular, it overcomes\nfactors depending on $\\log n$ and, importantly, the minimum eigenvalue of the\nPMD's covariance matrix from the distance to a multidimensional Gaussian random\nvariable.\nWe use our structural characterization to obtain an $\\varepsilon$-cover, in\ntotal variation distance, of the set of all $(n, k)$-PMDs, significantly\nimproving the cover size of Daskalakis and Papadimitriou, and obtaining the\nsame qualitative dependence of the cover size on $n$ and $\\varepsilon$ as the\n$k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structure\nto show that $(n,k)$-PMDs can be learned to within $\\varepsilon$ in total\nvariation distance from $\\tilde{O}_k(1/\\varepsilon^2)$ samples, which is\nnear-optimal in terms of dependence on $\\varepsilon$ and independent of $n$. In\nparticular, our result generalizes the single-dimensional result of Daskalakis,\nDiakonikolas, and Servedio for Poisson Binomials to arbitrary dimension."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1503.02516,https://arxiv.org/abs/1503.02516,"b""Abstract:  We show that computing the revenue-optimal deterministic auction in\nunit-demand single-buyer Bayesian settings, i.e. the optimal item-pricing, is\ncomputationally hard even in single-item settings where the buyer's value\ndistribution is a sum of independently distributed attributes, or multi-item\nsettings where the buyer's values for the items are independent. We also show\nthat it is intractable to optimally price the grand bundle of multiple items\nfor an additive bidder whose values for the items are independent. These\ndifficulties stem from implicit definitions of a value distribution. We provide\nthree instances of how different properties of implicit distributions can lead\nto intractability: the first is a #P-hardness proof, while the remaining two\nare reductions from the SQRT-SUM problem of Garey, Graham, and Johnson. While\nsimple pricing schemes can oftentimes approximate the best scheme in revenue,\nthey can have drastically different underlying structure. We argue therefore\nthat either the specification of the input distribution must be highly\nrestricted in format, or it is necessary for the goal to be mere approximation\nto the optimal scheme's revenue instead of computing properties of the scheme\nitself."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1503.01958,https://arxiv.org/abs/1503.01958,"b""Abstract:  Optimal mechanisms have been provided in quite general multi-item settings,\nas long as each bidder's type distribution is given explicitly by listing every\ntype in the support along with its associated probability. In the implicit\nsetting, e.g. when the bidders have additive valuations with independent and/or\ncontinuous values for the items, these results do not apply, and it was\nrecently shown that exact revenue optimization is intractable, even when there\nis only one bidder. Even for item distributions with special structure, optimal\nmechanisms have been surprisingly rare and the problem is challenging even in\nthe two-item case. In this paper, we provide a framework for designing optimal\nmechanisms using optimal transport theory and duality theory. We instantiate\nour framework to obtain conditions under which only pricing the grand bundle is\noptimal in multi-item settings (complementing the work of [Manelli and Vincent\n2006], as well as to characterize optimal two-item mechanisms. We use our\nresults to derive closed-form descriptions of the optimal mechanism in several\ntwo-item settings, exhibiting also a setting where a continuum of lotteries is\nnecessary for revenue optimization but a closed-form representation of the\nmechanism can still be found efficiently using our framework."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1412.4840,https://arxiv.org/abs/1412.4840,"b'Abstract:  Fictitious play is a natural dynamic for equilibrium play in zero-sum games,\nproposed by [Brown 1949], and shown to converge by [Robinson 1951]. Samuel\nKarlin conjectured in 1959 that fictitious play converges at rate\n$O(1/\\sqrt{t})$ with the number of steps $t$. We disprove this conjecture\nshowing that, when the payoff matrix of the row player is the $n \\times n$\nidentity matrix, fictitious play may converge with rate as slow as\n$\\Omega(t^{-1/n})$.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1410.3386,https://arxiv.org/abs/1410.3386,"b'Abstract:  A Poisson Binomial distribution over $n$ variables is the distribution of the\nsum of $n$ independent Bernoullis. We provide a sample near-optimal algorithm\nfor testing whether a distribution $P$ supported on $\\{0,...,n\\}$ to which we\nhave sample access is a Poisson Binomial distribution, or far from all Poisson\nBinomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$\nto which we provide a matching lower bound. We note that our sample complexity\nimproves quadratically upon that of the naive ""learn followed by tolerant-test""\napproach, while instance optimal identity testing [VV14] is not applicable\nsince we are looking to simultaneously test against a whole family of\ndistributions.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1409.4150,https://arxiv.org/abs/1409.4150,"b""Abstract:  We characterize optimal mechanisms for the multiple-good monopoly problem and\nprovide a framework to find them. We show that a mechanism is optimal if and\nonly if a measure $\\mu$ derived from the buyer's type distribution satisfies\ncertain stochastic dominance conditions. This measure expresses the marginal\nchange in the seller's revenue under marginal changes in the rent paid to\nsubsets of buyer types. As a corollary, we characterize the optimality of\ngrand-bundling mechanisms, strengthening several results in the literature,\nwhere only sufficient optimality conditions have been derived. As an\napplication, we show that the optimal mechanism for $n$ independent uniform\nitems each supported on $[c,c+1]$ is a grand-bundling mechanism, as long as $c$\nis sufficiently large, extending Pavlov's result for $2$ items [Pavlov'11]. At\nthe same time, our characterization also implies that, for all $c$ and for all\nsufficiently large $n$, the optimal mechanism for $n$ independent uniform items\nsupported on $[c,c+1]$ is not a grand bundling mechanism."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1408.2539,https://arxiv.org/abs/1408.2539,"b'Abstract:  We propose an optimum mechanism for providing monetary incentives to the data\nsources of a statistical estimator such as linear regression, so that high\nquality data is provided at low cost, in the sense that the sum of payments and\nestimation error is minimized. The mechanism applies to a broad range of\nestimators, including linear and polynomial regression, kernel regression, and,\nunder some additional assumptions, ridge regression. It also generalizes to\nseveral objectives, including minimizing estimation error subject to budget\nconstraints. Besides our concrete results for regression problems, we\ncontribute a mechanism design framework through which to design and analyze\nstatistical estimators whose examples are supplied by workers with cost for\nlabeling said examples.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1405.5940,https://arxiv.org/abs/1405.5940,"b'Abstract:  We provide polynomial-time approximately optimal Bayesian mechanisms for\nmakespan minimization on unrelated machines as well as for max-min fair\nallocations of indivisible goods, with approximation factors of $2$ and\n$\\min\\{m-k+1, \\tilde{O}(\\sqrt{k})\\}$ respectively, matching the approximation\nratios of best known polynomial-time \\emph{algorithms} (for max-min fairness,\nthe latter claim is true for certain ratios of the number of goods $m$ to\npeople $k$). Our mechanisms are obtained by establishing a polynomial-time\napproximation-sensitive reduction from the problem of designing approximately\noptimal {\\em mechanisms} for some arbitrary objective ${\\cal O}$ to that of\ndesigning bi-criterion approximation {\\em algorithms} for the same objective\n${\\cal O}$ plus a linear allocation cost term. Our reduction is itself enabled\nby extending the celebrated ""equivalence of separation and\noptimization""[GLSS81,KP80] to also accommodate bi-criterion approximations.\nMoreover, to apply the reduction to the specific problems of makespan and\nmax-min fairness we develop polynomial-time bi-criterion approximation\nalgorithms for makespan minimization with costs and max-min fairness with\ncosts, adapting the algorithms of [ST93], [BD05] and [AS07] to the type of\nbi-criterion approximation that is required by the reduction.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1312.1054,https://arxiv.org/abs/1312.1054,"b'Abstract:  We provide an algorithm for properly learning mixtures of two\nsingle-dimensional Gaussians without any separability assumptions. Given\n$\\tilde{O}(1/\\varepsilon^2)$ samples from an unknown mixture, our algorithm\noutputs a mixture that is $\\varepsilon$-close in total variation distance, in\ntime $\\tilde{O}(1/\\varepsilon^5)$. Our sample complexity is optimal up to\nlogarithmic factors, and significantly improves upon both Kalai et al., whose\nalgorithm has a prohibitive dependence on $1/\\varepsilon$, and Feldman et al.,\nwhose algorithm requires bounds on the mixture parameters and depends\npseudo-polynomially in these parameters.\nOne of our main contributions is an improved and generalized algorithm for\nselecting a good candidate distribution from among competing hypotheses.\nNamely, given a collection of $N$ hypotheses containing at least one candidate\nthat is $\\varepsilon$-close to an unknown distribution, our algorithm outputs a\ncandidate which is $O(\\varepsilon)$-close to the distribution. The algorithm\nrequires ${O}(\\log{N}/\\varepsilon^2)$ samples from the unknown distribution and\n${O}(N \\log N/\\varepsilon^2)$ time, which improves previous such results (such\nas the Scheff\xc3\xa9 estimator) from a quadratic dependence of the running time on\n$N$ to quasilinear. Given the wide use of such results for the purpose of\nhypothesis selection, our improved algorithm implies immediate improvements to\nany such use.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1309.7084,https://arxiv.org/abs/1309.7084,"b'Abstract:  The Chord algorithm is a popular, simple method for the succinct\napproximation of curves, which is widely used, under different names, in a\nvariety of areas, such as, multiobjective and parametric optimization,\ncomputational geometry, and graphics. We analyze the performance of the Chord\nalgorithm, as compared to the optimal approximation that achieves a desired\naccuracy with the minimum number of points. We prove sharp upper and lower\nbounds, both in the worst case and average case setting.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1307.3621,https://arxiv.org/abs/1307.3621,"b'Abstract:  We consider a problem which has received considerable attention in systems\nliterature because of its applications to routing in delay tolerant networks\nand replica placement in distributed storage systems. In abstract terms the\nproblem can be stated as follows: Given a random variable $X$ generated by a\nknown product distribution over $\\{0,1\\}^n$ and a target value $0 \\leq \\theta\n\\leq 1$, output a non-negative vector $w$, with $\\|w\\|_1 \\le 1$, which\nmaximizes the probability of the event $w \\cdot X \\ge \\theta$. This is a\nchallenging non-convex optimization problem for which even computing the value\n$\\Pr[w \\cdot X \\ge \\theta]$ of a proposed solution vector $w$ is #P-hard.\nWe provide an additive EPTAS for this problem which, for constant-bounded\nproduct distributions, runs in $ \\poly(n) \\cdot 2^{\\poly(1/\\eps)}$ time and\noutputs an $\\eps$-approximately optimal solution vector $w$ for this problem.\nOur approach is inspired by, and extends, recent structural results from the\ncomplexity-theoretic study of linear threshold functions. Furthermore, in spite\nof the objective function being non-smooth, we give a \\emph{unicriterion} PTAS\nwhile previous work for such objective functions has typically led to a\n\\emph{bicriterion} PTAS. We believe our techniques may be applicable to get\nunicriterion PTAS for other non-smooth objective functions.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1306.1265,https://arxiv.org/abs/1306.1265,"b'Abstract:  For all $n, \\epsilon >0$, we show that the set of Poisson Binomial\ndistributions on $n$ variables admits a proper $\\epsilon$-cover in total\nvariation distance of size $n^2+n \\cdot (1/\\epsilon)^{O(\\log^2 (1/\\epsilon))}$,\nwhich can also be computed in polynomial time. We discuss the implications of\nour construction for approximation algorithms and the computation of\napproximate Nash equilibria in anonymous games.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1305.4002,https://arxiv.org/abs/1305.4002,"b""Abstract:  We provide a computationally efficient black-box reduction from mechanism\ndesign to algorithm design in very general settings. Specifically, we give an\napproximation-preserving reduction from truthfully maximizing \\emph{any}\nobjective under \\emph{arbitrary} feasibility constraints with \\emph{arbitrary}\nbidder types to (not necessarily truthfully) maximizing the same objective plus\nvirtual welfare (under the same feasibility constraints). Our reduction is\nbased on a fundamentally new approach: we describe a mechanism's behavior\nindirectly only in terms of the expected value it awards bidders for certain\nbehavior, and never directly access the allocation rule at all.\nApplying our new approach to revenue, we exhibit settings where our reduction\nholds \\emph{both ways}. That is, we also provide an approximation-sensitive\nreduction from (non-truthfully) maximizing virtual welfare to (truthfully)\nmaximizing revenue, and therefore the two problems are computationally\nequivalent. With this equivalence in hand, we show that both problems are\nNP-hard to approximate within any polynomial factor, even for a single monotone\nsubmodular bidder.\nWe further demonstrate the applicability of our reduction by providing a\ntruthful mechanism maximizing fractional max-min fairness. This is the first\ninstance of a truthful mechanism that optimizes a non-linear objective."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1305.4000,https://arxiv.org/abs/1305.4000,"b'Abstract:  It was recently shown in [this http URL] that revenue\noptimization can be computationally efficiently reduced to welfare optimization\nin all multi-dimensional Bayesian auction problems with arbitrary (possibly\ncombinatorial) feasibility constraints and independent additive bidders with\narbitrary (possibly combinatorial) demand constraints. This reduction provides\na poly-time solution to the optimal mechanism design problem in all auction\nsettings where welfare optimization can be solved efficiently, but it is\nfragile to approximation and cannot provide solutions to settings where welfare\nmaximization can only be tractably approximated. In this paper, we extend the\nreduction to accommodate approximation algorithms, providing an approximation\npreserving reduction from (truthful) revenue maximization to (not necessarily\ntruthful) welfare maximization. The mechanisms output by our reduction choose\nallocations via black-box calls to welfare approximation on randomly selected\ninputs, thereby generalizing also our earlier structural results on optimal\nmulti-dimensional mechanisms to approximately optimal mechanisms. Unlike\n[this http URL], our results here are obtained through novel\nuses of the Ellipsoid algorithm and other optimization techniques over {\\em\nnon-convex regions}.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1211.1703,https://arxiv.org/abs/1211.1703,"b""Abstract:  Myerson's seminal work provides a computationally efficient revenue-optimal\nauction for selling one item to multiple bidders. Generalizing this work to\nselling multiple items at once has been a central question in economics and\nalgorithmic game theory, but its complexity has remained poorly understood. We\nanswer this question by showing that a revenue-optimal auction in multi-item\nsettings cannot be found and implemented computationally efficiently, unless\nZPP contains P^#P. This is true even for a single additive bidder whose values\nfor the items are independently distributed on two rational numbers with\nrational probabilities. Our result is very general: we show that it is hard to\ncompute any encoding of an optimal auction of any format (direct or indirect,\ntruthful or non-truthful) that can be implemented in expected polynomial time.\nIn particular, under well-believed complexity-theoretic assumptions,\nrevenue-optimization in very simple multi-item settings can only be tractably\napproximated.\nWe note that our hardness result applies to randomized mechanisms in a very\nsimple setting, and is not an artifact of introducing combinatorial structure\nto the problem by allowing correlation among item values, introducing\ncombinatorial valuations, or requiring the mechanism to be deterministic (whose\nstructure is readily combinatorial). Our proof is enabled by a\nflow-interpretation of the solutions of an exponential-size linear program for\nrevenue maximization with an additional supermodularity constraint."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1207.5518,https://arxiv.org/abs/1207.5518,"b""Abstract:  We provide a reduction from revenue maximization to welfare maximization in\nmulti-dimensional Bayesian auctions with arbitrary (possibly combinatorial)\nfeasibility constraints and independent bidders with arbitrary (possibly\ncombinatorial) demand constraints, appropriately extending Myerson's result to\nthis setting. We also show that every feasible Bayesian auction can be\nimplemented as a distribution over virtual VCG allocation rules. A virtual VCG\nallocation rule has the following simple form: Every bidder's type t_i is\ntransformed into a virtual type f_i(t_i), via a bidder-specific function. Then,\nthe allocation maximizing virtual welfare is chosen. Using this\ncharacterization, we show how to find and run the revenue-optimal auction given\nonly black box access to an implementation of the VCG allocation rule. We\ngeneralize this result to arbitrarily correlated bidders, introducing the\nnotion of a second-order VCG allocation rule.\nWe obtain our reduction from revenue to welfare optimization via two\nalgorithmic results on reduced forms in settings with arbitrary feasibility and\ndemand constraints. First, we provide a separation oracle for determining\nfeasibility of a reduced form. Second, we provide a geometric algorithm to\ndecompose any feasible reduced form into a distribution over virtual VCG\nallocation rules. In addition, we show how to execute both algorithms given\nonly black box access to an implementation of the VCG allocation rule.\nOur results are computationally efficient for all multi-dimensional settings\nwhere the bidders are additive. In this case, our mechanisms run in time\npolynomial in the total number of bidder types, but not type profiles. For\ngeneric correlated distributions, this is the natural description complexity of\nthe problem. The runtime can be further improved to poly(#items, #bidders) in\nitem-symmetric settings by making use of recent techniques."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1112.5659,https://arxiv.org/abs/1112.5659,"b'Abstract:  We give highly efficient algorithms, and almost matching lower bounds, for a\nrange of basic statistical problems that involve testing and estimating the L_1\ndistance between two k-modal distributions $p$ and $q$ over the discrete domain\n$\\{1,\\dots,n\\}$. More precisely, we consider the following four problems: given\nsample access to an unknown k-modal distribution $p$,\nTesting identity to a known or unknown distribution:\n1. Determine whether $p = q$ (for an explicitly given k-modal distribution\n$q$) versus $p$ is $\\eps$-far from $q$;\n2. Determine whether $p=q$ (where $q$ is available via sample access) versus\n$p$ is $\\eps$-far from $q$;\nEstimating $L_1$ distance (""tolerant testing\'\') against a known or unknown\ndistribution:\n3. Approximate $d_{TV}(p,q)$ to within additive $\\eps$ where $q$ is an\nexplicitly given k-modal distribution $q$;\n4. Approximate $d_{TV}(p,q)$ to within additive $\\eps$ where $q$ is available\nvia sample access.\nFor each of these four problems we give sub-logarithmic sample algorithms,\nthat we show are tight up to additive $\\poly(k)$ and multiplicative\n$\\polylog\\log n+\\polylog k$ factors. Thus our bounds significantly improve the\nprevious results of \\cite{BKR:04}, which were for testing identity of\ndistributions (items (1) and (2) above) in the special cases k=0 (monotone\ndistributions) and k=1 (unimodal distributions) and required $O((\\log n)^3)$\nsamples.\nAs our main conceptual contribution, we introduce a new reduction-based\napproach for distribution-testing problems that lets us obtain all the above\nresults in a unified way. Roughly speaking, this approach enables us to\ntransform various distribution testing problems for k-modal distributions over\n$\\{1,\\dots,n\\}$ to the corresponding distribution testing problems for\nunrestricted distributions over a much smaller domain $\\{1,\\dots,\\ell\\}$ where\n$\\ell = O(k \\log n).$'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1112.4572,https://arxiv.org/abs/1112.4572,"b""Abstract:  We provide a constructive proof of Border's theorem [Bor91, HR15a] and its\ngeneralization to reduced-form auctions with asymmetric bidders [Bor07, MV10,\nCKM13]. Given a reduced form, we identify a subset of Border constraints that\nare necessary and sufficient to determine its feasibility. Importantly, the\nnumber of these constraints is linear in the total number of bidder types. In\naddition, we provide a characterization result showing that every feasible\nreduced form can be induced by an ex-post allocation rule that is a\ndistribution over ironings of the same total ordering of the union of all\nbidders' types.\nWe show how to leverage our results for single-item reduced forms to design\nauctions with heterogeneous items and asymmetric bidders with valuations that\nare additive over items. Appealing to our constructive Border's theorem, we\nobtain polynomial-time algorithms for computing the revenue-optimal auction.\nAppealing to our characterization of feasible reduced forms, we characterize\nfeasible multi-item allocation rules."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1112.4006,https://arxiv.org/abs/1112.4006,"b""Abstract:  We efficiently solve the optimal multi-dimensional mechanism design problem\nfor independent bidders with arbitrary demand constraints when either the\nnumber of bidders is a constant or the number of items is a constant. In the\nfirst setting, we need that each bidder's values for the items are sampled from\na possibly correlated, item-symmetric distribution, allowing different\ndistributions for each bidder. In the second setting, we allow the values of\neach bidder for the items to be arbitrarily correlated, but assume that the\ndistribution of bidder types is bidder-symmetric.\nFor all eps>0, we obtain an additive eps-approximation, when the value\ndistributions are bounded, or a multiplicative (1-eps)-approximation when the\nvalue distributions are unbounded, but satisfy the Monotone Hazard Rate\ncondition, covering a widely studied class of distributions in Economics. Our\nruntime is polynomial in max{#items,#bidders}, and not the size of the support\nof the joint distribution of all bidders' values for all items, which is\ntypically exponential in both the number of items and the number of bidders.\nOur mechanisms are randomized, explicitly price bundles, and can sometimes\naccommodate budget constraints.\nOur results are enabled by establishing several new tools and structural\nproperties of Bayesian mechanisms. We provide a symmetrization technique\nturning any truthful mechanism into one that has the same revenue and respects\nall symmetries in the underlying value distributions. We also prove that\nitem-symmetric mechanisms satisfy a natural monotonicity property which, unlike\ncyclic-monotonicity, can be harnessed algorithmically. Finally, we provide a\ntechnique that turns any given eps-BIC mechanism (i.e. one where incentive\nconstraints are violated by eps) into a truly-BIC mechanism at the cost of\nO(sqrt{eps}) revenue. We expect our tools to be used beyond the settings we\nconsider here."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1109.5002,https://arxiv.org/abs/1109.5002,"b'Abstract:  We present an efficient phylogenetic reconstruction algorithm allowing\ninsertions and deletions which provably achieves a sequence-length requirement\n(or sample complexity) growing polynomially in the number of taxa. Our\nalgorithm is distance-based, that is, it relies on pairwise sequence\ncomparisons. More importantly, our approach largely bypasses the difficult\nproblem of multiple sequence alignment.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1107.2702,https://arxiv.org/abs/1107.2702,"b'Abstract:  We consider a basic problem in unsupervised learning: learning an unknown\n\\emph{Poisson Binomial Distribution}. A Poisson Binomial Distribution (PBD)\nover $\\{0,1,\\dots,n\\}$ is the distribution of a sum of $n$ independent\nBernoulli random variables which may have arbitrary, potentially non-equal,\nexpectations. These distributions were first studied by S. Poisson in 1837\n\\cite{Poisson:37} and are a natural $n$-parameter generalization of the\nfamiliar Binomial Distribution. Surprisingly, prior to our work this basic\nlearning problem was poorly understood, and known results for it were far from\noptimal.\nWe essentially settle the complexity of the learning problem for this basic\nclass of distributions. As our first main result we give a highly efficient\nalgorithm which learns to $\\eps$-accuracy (with respect to the total variation\ndistance) using $\\tilde{O}(1/\\eps^3)$ samples \\emph{independent of $n$}. The\nrunning time of the algorithm is \\emph{quasilinear} in the size of its input\ndata, i.e., $\\tilde{O}(\\log(n)/\\eps^3)$ bit-operations. (Observe that each draw\nfrom the distribution is a $\\log(n)$-bit string.) Our second main result is a\n{\\em proper} learning algorithm that learns to $\\eps$-accuracy using\n$\\tilde{O}(1/\\eps^2)$ samples, and runs in time $(1/\\eps)^{\\poly (\\log\n(1/\\eps))} \\cdot \\log n$. This is nearly optimal, since any algorithm {for this\nproblem} must use $\\Omega(1/\\eps^2)$ samples. We also give positive and\nnegative results for some extensions of this learning problem to weighted sums\nof independent Bernoulli random variables.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1107.2700,https://arxiv.org/abs/1107.2700,"b'Abstract:  A $k$-modal probability distribution over the discrete domain $\\{1,...,n\\}$\nis one whose histogram has at most $k$ ""peaks"" and ""valleys."" Such\ndistributions are natural generalizations of monotone ($k=0$) and unimodal\n($k=1$) probability distributions, which have been intensively studied in\nprobability theory and statistics.\nIn this paper we consider the problem of \\emph{learning} (i.e., performing\ndensity estimation of) an unknown $k$-modal distribution with respect to the\n$L_1$ distance. The learning algorithm is given access to independent samples\ndrawn from an unknown $k$-modal distribution $p$, and it must output a\nhypothesis distribution $\\widehat{p}$ such that with high probability the total\nvariation distance between $p$ and $\\widehat{p}$ is at most $\\epsilon.$ Our\nmain goal is to obtain \\emph{computationally efficient} algorithms for this\nproblem that use (close to) an information-theoretically optimal number of\nsamples.\nWe give an efficient algorithm for this problem that runs in time\n$\\mathrm{poly}(k,\\log(n),1/\\epsilon)$. For $k \\leq \\tilde{O}(\\log n)$, the\nnumber of samples used by our algorithm is very close (within an\n$\\tilde{O}(\\log(1/\\epsilon))$ factor) to being information-theoretically\noptimal. Prior to this work computationally efficient algorithms were known\nonly for the cases $k=0,1$ \\cite{Birge:87b,Birge:97}.\nA novel feature of our approach is that our learning algorithm crucially uses\na new algorithm for \\emph{property testing of probability distributions} as a\nkey subroutine. The learning algorithm uses the property tester to efficiently\ndecompose the $k$-modal distribution into $k$ (near-)monotone distributions,\nwhich are easier to learn.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1106.0519,https://arxiv.org/abs/1106.0519,"b'Abstract:  We provide a near-optimal, computationally efficient algorithm for the\nunit-demand pricing problem, where a seller wants to price n items to optimize\nrevenue against a unit-demand buyer whose values for the items are\nindependently drawn from known distributions. For any chosen accuracy eps>0 and\nitem values bounded in [0,1], our algorithm achieves revenue that is optimal up\nto an additive error of at most eps, in polynomial time. For values sampled\nfrom Monotone Hazard Rate (MHR) distributions, we achieve a (1-eps)-fraction of\nthe optimal revenue in polynomial time, while for values sampled from regular\ndistributions the same revenue guarantees are achieved in quasi-polynomial\ntime.\nOur algorithm for bounded distributions applies probabilistic techniques to\nunderstand the statistical properties of revenue distributions, obtaining a\nreduction in the search space of the algorithm via dynamic programming.\nAdapting this approach to MHR and regular distributions requires the proof of\nnovel extreme value theorems for such distributions.\nAs a byproduct, our techniques establish structural properties of\napproximately-optimal and near-optimal solutions. We show that, for values\nindependently distributed according to MHR distributions, pricing all items at\nthe same price achieves a constant fraction of the optimal revenue. Moreover,\nfor all eps >0, g(1/eps) distinct prices suffice to obtain a (1-eps)-fraction\nof the optimal revenue, where g(1/eps) is quadratic in 1/eps and independent of\nn. Similarly, for all eps>0 and n>0, at most g(1/(eps log n)) distinct prices\nsuffice if the values are independently distributed according to regular\ndistributions, where g() is a polynomial function. Finally, when the values are\ni.i.d. from some MHR distribution, we show that, if n is a sufficiently large\nfunction of 1/eps, a single price suffices to achieve a (1-eps)-fraction of the\noptimal revenue.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1103.0598,https://arxiv.org/abs/1103.0598,"b'Abstract:  We consider the problem of learning an unknown product distribution $X$ over\n$\\{0,1\\}^n$ using samples $f(X)$ where $f$ is a \\emph{known} transformation\nfunction. Each choice of a transformation function $f$ specifies a learning\nproblem in this framework.\nInformation-theoretic arguments show that for every transformation function\n$f$ the corresponding learning problem can be solved to accuracy $\\eps$, using\n$\\tilde{O}(n/\\eps^2)$ examples, by a generic algorithm whose running time may\nbe exponential in $n.$ We show that this learning problem can be\ncomputationally intractable even for constant $\\eps$ and rather simple\ntransformation functions. Moreover, the above sample complexity bound is nearly\noptimal for the general problem, as we give a simple explicit linear\ntransformation function $f(x)=w \\cdot x$ with integer weights $w_i \\leq n$ and\nprove that the corresponding learning problem requires $\\Omega(n)$ samples.\nAs our main positive result we give a highly efficient algorithm for learning\na sum of independent unknown Bernoulli random variables, corresponding to the\ntransformation function $f(x)= \\sum_{i=1}^n x_i$. Our algorithm learns to\n$\\eps$-accuracy in poly$(n)$ time, using a surprising poly$(1/\\eps)$ number of\nsamples that is independent of $n.$ We also give an efficient algorithm that\nuses $\\log n \\cdot \\poly(1/\\eps)$ samples but has running time that is only\n$\\poly(\\log n, 1/\\eps).$'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1102.2280,https://arxiv.org/abs/1102.2280,"b'Abstract:  If a game has a Nash equilibrium with probability values that are either zero\nor Omega(1) then this equilibrium can be found exhaustively in polynomial time.\nSomewhat surprisingly, we show that there is a PTAS for the games whose\nequilibria are guaranteed to have small-O(1/n)-values, and therefore\nlarge-Omega(n)-supports. We also point out that there is a PTAS for games with\nsparse payoff matrices, which are known to be PPAD-complete to solve exactly.\nBoth algorithms are of a special kind that we call oblivious: The algorithm\njust samples a fixed distribution on pairs of mixed strategies, and the game is\nonly used to determine whether the sampled strategies comprise an eps-Nash\nequilibrium; the answer is yes with inverse polynomial probability. These\nresults bring about the question: Is there an oblivious PTAS for Nash\nequilibrium in general games? We answer this question in the negative; our\nlower bound comes close to the quasi-polynomial upper bound of [Lipton,\nMarkakis, Mehta 2003].\nAnother recent PTAS for anonymous games is also oblivious in a weaker sense\nappropriate for this class of games (it samples from a fixed distribution on\nunordered collections of mixed strategies), but its runtime is exponential in\n1/eps. We prove that any oblivious PTAS for anonymous games with two strategies\nand three player types must have 1/eps^c in the exponent of the running time\nfor some c>1/3, rendering the algorithm in [Daskalakis 2008] essentially\noptimal within oblivious algorithms. In contrast, we devise a poly(n)\n(1/eps)^O(log^2(1/eps)) non-oblivious PTAS for anonymous games with 2\nstrategies and any bounded number of player types.\nOur algorithm is based on the construction of a sparse (and efficiently\ncomputable) eps-cover of the set of all possible sums of n independent\nindicators, under the total variation distance. The size of the cover is\npoly(n) (1/ eps^{O(log^2 (1/eps))}.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0912.2577,https://arxiv.org/abs/0912.2577,"b'Abstract:  Molecular phylogenetic techniques do not generally account for such common\nevolutionary events as site insertions and deletions (known as indels). Instead\ntree building algorithms and ancestral state inference procedures typically\nrely on substitution-only models of sequence evolution. In practice these\nmethods are extended beyond this simplified setting with the use of heuristics\nthat produce global alignments of the input sequences--an important problem\nwhich has no rigorous model-based solution. In this paper we consider a new\nversion of the multiple sequence alignment in the context of stochastic indel\nmodels. More precisely, we introduce the following {\\em trace reconstruction\nproblem on a tree} (TRPT): a binary sequence is broadcast through a tree\nchannel where we allow substitutions, deletions, and insertions; we seek to\nreconstruct the original sequence from the sequences received at the leaves of\nthe tree. We give a recursive procedure for this problem with strong\nreconstruction guarantees at low mutation rates, providing also an alignment of\nthe sequences at the leaves of the tree. The TRPT problem without indels has\nbeen studied in previous work (Mossel 2004, Daskalakis et al. 2006) as a\nbootstrapping step towards obtaining optimal phylogenetic reconstruction\nmethods. The present work sets up a framework for extending these works to\nevolutionary models with indels.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0812.2277,https://arxiv.org/abs/0812.2277,"b""Abstract:  We present a novel polynomial time approximation scheme for two-strategy\nanonymous games, in which the players' utility functions, although potentially\ndifferent, do not differentiate among the identities of the other players. Our\nalgorithm computes an $eps$-approximate Nash equilibrium of an $n$-player\n2-strategy anonymous game in time $poly(n) (1/eps)^{O(1/eps^2)}$, which\nsignificantly improves upon the running time $n^{O(1/eps^2)}$ required by the\nalgorithm of Daskalakis & Papadimitriou, 2007. The improved running time is\nbased on a new structural understanding of approximate Nash equilibria: We show\nthat, for any $eps$, there exists an $eps$-approximate Nash equilibrium in\nwhich either only $O(1/eps^3)$ players randomize, or all players who randomize\nuse the same mixed strategy. To show this result we employ tools from the\nliterature on Stein's Method."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0808.2801,https://arxiv.org/abs/0808.2801,"b'Abstract:  We show that there is a polynomial-time approximation scheme for computing\nNash equilibria in anonymous games with any fixed number of strategies (a very\nbroad and important class of games), extending the two-strategy result of\nDaskalakis and Papadimitriou 2007. The approximation guarantee follows from a\nprobabilistic result of more general interest: The distribution of the sum of n\nindependent unit vectors with values ranging over {e1, e2, ...,ek}, where ei is\nthe unit vector along dimension i of the k-dimensional Euclidean space, can be\napproximated by the distribution of the sum of another set of independent unit\nvectors whose probabilities of obtaining each value are multiples of 1/z for\nsome integer z, and so that the variational distance of the two distributions\nis at most eps, where eps is bounded by an inverse polynomial in z and a\nfunction of k, but with no dependence on n. Our probabilistic result specifies\nthe construction of a surprisingly sparse eps-cover -- under the total\nvariation distance -- of the set of distributions of sums of independent unit\nvectors, which is of interest on its own right.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0802.1604,https://arxiv.org/abs/0802.1604,"b'Abstract:  We consider the problem of computing Nash Equilibria of action-graph games\n(AGGs). AGGs, introduced by Bhat and Leyton-Brown, is a succinct representation\nof games that encapsulates both ""local"" dependencies as in graphical games, and\npartial indifference to other agents\' identities as in anonymous games, which\noccur in many natural settings. This is achieved by specifying a graph on the\nset of actions, so that the payoff of an agent for selecting a strategy depends\nonly on the number of agents playing each of the neighboring strategies in the\naction graph. We present a Polynomial Time Approximation Scheme for computing\nmixed Nash equilibria of AGGs with constant treewidth and a constant number of\nagent types (and an arbitrary number of strategies), together with hardness\nresults for the cases when either the treewidth or the number of agent types is\nunconstrained. In particular, we show that even if the action graph is a tree,\nbut the number of agent-types is unconstrained, it is NP-complete to decide the\nexistence of a pure-strategy Nash equilibrium and PPAD-complete to compute a\nmixed Nash equilibrium (even an approximate one); similarly for symmetric AGGs\n(all agents belong to a single type), if we allow arbitrary treewidth. These\nhardness results suggest that, in some sense, our PTAS is as strong of a\npositive result as one can expect.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0801.4190,https://arxiv.org/abs/0801.4190,"b'Abstract:  We introduce a new phylogenetic reconstruction algorithm which, unlike most\nprevious rigorous inference techniques, does not rely on assumptions regarding\nthe branch lengths or the depth of the tree. The algorithm returns a forest\nwhich is guaranteed to contain all edges that are: 1) sufficiently long and 2)\nsufficiently close to the leaves. How much of the true tree is recovered\ndepends on the sequence length provided. The algorithm is distance-based and\nruns in polynomial time.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0710.5582,https://arxiv.org/abs/0710.5582,"b'Abstract:  We present efficient approximation algorithms for finding Nash equilibria in\nanonymous games, that is, games in which the players utilities, though\ndifferent, do not differentiate between other players. Our results pertain to\nsuch games with many players but few strategies. We show that any such game has\nan approximate pure Nash equilibrium, computable in polynomial time, with\napproximation O(s^2 L), where s is the number of strategies and L is the\nLipschitz constant of the utilities. Finally, we show that there is a PTAS for\nfinding an epsilon'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0710.4982,https://arxiv.org/abs/0710.4982,"b'Abstract:  In this paper, we provide a rigorous analysis of preferential attachment with\nfitness, a random graph model introduced by Bianconi and Barabasi. Depending on\nthe shape of the fitness distribution, we observe three distinct phases: a\nfirst-mover-advantage phase, a fit-get-richer phase and an innovation-pays-off\nphase.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0707.1532,https://arxiv.org/abs/0707.1532,"b'Abstract:  Classical problems of sorting and searching assume an underlying linear\nordering of the objects being compared. In this paper, we study a more general\nsetting, in which some pairs of objects are incomparable. This generalization\nis relevant in applications related to rankings in sports, college admissions,\nor conference submissions. It also has potential applications in biology, such\nas comparing the evolutionary fitness of different strains of bacteria, or\nunderstanding input-output relations among a set of metabolic reactions or the\ncausal influences among a set of interacting genes or proteins. Our results\nimprove and extend results from two decades ago of Faigle and Tur\xc3\xa1n.\nA measure of complexity of a partially ordered set (poset) is its width. Our\nalgorithms obtain information about a poset by queries that compare two\nelements. We present an algorithm that sorts, i.e. completely identifies, a\nwidth w poset of size n and has query complexity O(wn + nlog(n)), which is\nwithin a constant factor of the information-theoretic lower bound. We also show\nthat a variant of Mergesort has query complexity O(wn(log(n/w))) and total\ncomplexity O((w^2)nlog(n/w)). Faigle and Tur\xc3\xa1n have shown that the sorting\nproblem has query complexity O(wn(log(n/w))) but did not address its total\ncomplexity.\nFor the related problem of determining the minimal elements of a poset, we\ngive efficient deterministic and randomized algorithms with O(wn) query and\ntotal complexity, along with matching lower bounds for the query complexity up\nto a factor of 2. We generalize these results to the k-selection problem of\ndetermining the elements of height at most k. We also derive upper bounds on\nthe total complexity of some other problems of a similar flavor.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:math/0703902,https://arxiv.org/abs/math/0703902,"b'Abstract:  We study how the structure of the interaction graph of a game affects the\nexistence of pure Nash equilibria. In particular, for a fixed interaction\ngraph, we are interested in whether there are pure Nash equilibria arising when\nrandom utility tables are assigned to the players. We provide conditions for\nthe structure of the graph under which equilibria are likely to exist and\ncomplementary conditions which make the existence of equilibria highly\nunlikely. Our results have immediate implications for many deterministic graphs\nand generalize known results for random games on the complete graph. In\nparticular, our results imply that the probability that bounded degree graphs\nhave pure Nash equilibria is exponentially small in the size of the graph and\nyield a simple algorithm that finds small nonexistence certificates for a large\nfamily of graphs. Then we show that in any strongly connected graph of n\nvertices with expansion $(1+\\Omega(1))\\log_2(n)$ the distribution of the number\nof equilibria approaches the Poisson distribution with parameter 1,\nasymptotically as $n \\to +\\infty$.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:cs/0702014,https://arxiv.org/abs/cs/0702014,"b""Abstract:  We initiate the probabilistic analysis of linear programming (LP) decoding of\nlow-density parity-check (LDPC) codes. Specifically, we show that for a random\nLDPC code ensemble, the linear programming decoder of Feldman et al. succeeds\nin correcting a constant fraction of errors with high probability. The fraction\nof correctable errors guaranteed by our analysis surpasses previous\nnon-asymptotic results for LDPC codes, and in particular exceeds the best\nprevious finite-length result on LP decoding by a factor greater than ten. This\nimprovement stems in part from our analysis of probabilistic bit-flipping\nchannels, as opposed to adversarial channels. At the core of our analysis is a\nnovel combinatorial characterization of LP decoding success, based on the\nnotion of a generalized matching. An interesting by-product of our analysis is\nto establish the existence of ``probabilistic expansion'' in random bipartite\ngraphs, in which one requires only that almost every (as opposed to every) set\nof a certain size expands, for sets much larger than in the classical\nworst-case setting."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:cs/0510031,https://arxiv.org/abs/cs/0510031,"b'Abstract:  In this paper we present a novel generic mapping between Graphical Games and\nMarkov Random Fields so that pure Nash equilibria in the former can be found by\nstatistical inference on the latter. Thus, the problem of deciding whether a\ngraphical game has a pure Nash equilibrium, a well-known intractable problem,\ncan be attacked by well-established algorithms such as Belief Propagation,\nJunction Trees, Markov Chain Monte Carlo and Simulated Annealing. Large classes\nof graphical games become thus tractable, including all classes already known,\nbut also new classes such as the games with O(log n) treewidth.'"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:math/0509575,https://arxiv.org/abs/math/0509575,"b'Abstract:  A major task of evolutionary biology is the reconstruction of phylogenetic\ntrees from molecular data. The evolutionary model is given by a Markov chain on\na tree. Given samples from the leaves of the Markov chain, the goal is to\nreconstruct the leaf-labelled tree.\nIt is well known that in order to reconstruct a tree on $n$ leaves, sample\nsequences of length $\\Omega(\\log n)$ are needed. It was conjectured by M. Steel\nthat for the CFN/Ising evolutionary model, if the mutation probability on all\nedges of the tree is less than $p^{\\ast} = (\\sqrt{2}-1)/2^{3/2}$, then the tree\ncan be recovered from sequences of length $O(\\log n)$. The value $p^{\\ast}$ is\ngiven by the transition point for the extremality of the free Gibbs measure for\nthe Ising model on the binary tree. Steel\'s conjecture was proven by the second\nauthor in the special case where the tree is ""balanced."" The second author also\nproved that if all edges have mutation probability larger than $p^{\\ast}$ then\nthe length needed is $n^{\\Omega(1)}$. Here we show that Steel\'s conjecture\nholds true for general trees by giving a reconstruction algorithm that recovers\nthe tree from $O(\\log n)$-length sequences when the mutation probabilities are\ndiscretized and less than $p^\\ast$. Our proof and results demonstrate that\nextremality of the free Gibbs measure on the infinite binary tree, which has\nbeen studied before in probability, statistical physics and computer science,\ndetermines how distinguishable are Gibbs measures on finite binary trees.'"
Randall Davis,Davis_Randall,arXiv:1606.07163,https://arxiv.org/abs/1606.07163,"b'Abstract:  The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular\nneuropsychological screening tool for cognitive conditions. The Digital Clock\nDrawing Test (dCDT) uses novel software to analyze data from a digitizing\nballpoint pen that reports its position with considerable spatial and temporal\nprecision, making possible the analysis of both the drawing process and final\nproduct. We developed methodology to analyze pen stroke data from these\ndrawings, and computed a large collection of features which were then analyzed\nwith a variety of machine learning techniques. The resulting scoring systems\nwere designed to be more accurate than the systems currently used by\nclinicians, but just as interpretable and easy to use. The systems also allow\nus to quantify the tradeoff between accuracy and interpretability. We created\nautomated versions of the CDT scoring systems currently used by clinicians,\nallowing us to benchmark our models, which indicated that our machine learning\nmodels substantially outperformed the existing scoring systems.'"
Randall Davis,Davis_Randall,arXiv:1604.07429,https://arxiv.org/abs/1604.07429,"b'Abstract:  We describe a sketch interpretation system that detects and classifies clock\nnumerals created by subjects taking the Clock Drawing Test, a clinical tool\nwidely used to screen for cognitive impairments (e.g., dementia). We describe\nhow it balances appearance and context, and document its performance on some\n2,000 drawings (about 24K clock numerals) produced by a wide spectrum of\npatients. We calibrate the utility of different forms of context, describing\nexperiments with Conditional Random Fields trained and tested using a variety\nof features. We identify context that contributes to interpreting otherwise\nambiguous or incomprehensible strokes. We describe ST-slices, a novel\nrepresentation that enables ""unpeeling"" the layers of ink that result when\npeople overwrite, which often produces ink impossible to analyze if only the\nfinal drawing is examined. We characterize when ST-slices work, calibrate their\nimpact on performance, and consider their breadth of applicability.'"
Erik Demaine,Demaine_Erik,arXiv:1903.03560,https://arxiv.org/abs/1903.03560,"b'Abstract:  We revisit self-adjusting external memory tree data structures, which combine\nthe optimal (and practical) worst-case I/O performances of B-trees, while\nadapting to the online distribution of queries. Our approach is analogous to\nundergoing efforts in the BST model, where Tango Trees (Demaine et al. 2007)\nwere shown to be $O(\\log\\log N)$-competitive with the runtime of the best\noffline binary search tree on every sequence of searches. Here we formalize the\nB-Tree model as a natural generalization of the BST model. We prove lower\nbounds for the B-Tree model, and introduce a B-Tree model data structure, the\nBelga B-tree, that executes any sequence of searches within a $O(\\log \\log N)$\nfactor of the best offline B-tree model algorithm, provided $B=\\log^{O(1)}N$.\nWe also show how to transform any static BST into a static B-tree which is\nfaster by a $\\Theta(\\log B)$ factor; the transformation is randomized and we\nshow that randomization is necessary to obtain any significant speedup.'"
Erik Demaine,Demaine_Erik,arXiv:1901.08564,https://arxiv.org/abs/1901.08564,"b'Abstract:  We study the problem of deciding whether a crease pattern can be folded by\nsimple folds (folding along one line at a time) under the infinite all-layers\nmodel introduced by [Akitaya et al., 2017], in which each simple fold is\ndefined by an infinite line and must fold all layers of paper that intersect\nthis line. This model is motivated by folding in manufacturing such as\nsheet-metal bending. We improve on [Arkin et al., 2004] by giving a\ndeterministic $O(n)$-time algorithm to decide simple foldability of 1D crease\npatterns in the all-layers model. Then we extend this 1D result to 2D, showing\nthat simple foldability in this model can be decided in linear time for\nunassigned axis-aligned orthogonal crease patterns on axis-aligned 2D\northogonal paper. On the other hand, we show that simple foldability is\nstrongly NP-complete if a subset of the creases have a mountain-valley\nassignment, even for an axis-aligned rectangle of paper.'"
Erik Demaine,Demaine_Erik,arXiv:1812.03592,https://arxiv.org/abs/1812.03592,"b'Abstract:  We build a general theory for characterizing the computational complexity of\nmotion planning of robot(s) through a graph of ""gadgets"", where each gadget has\nits own state defining a set of allowed traversals which in turn modify the\ngadget\'s state. We study two families of such gadgets, one which naturally\nleads to motion planning problems with polynomially bounded solutions, and\nanother which leads to polynomially unbounded (potentially exponential)\nsolutions. We also study a range of competitive game-theoretic scenarios, from\none player controlling one robot to teams of players each controlling their own\nrobot and racing to achieve their team\'s goal. Under small restrictions on\nthese gadgets, we fully characterize the complexity of bounded 1-player motion\nplanning (NL vs. NP-complete), unbounded 1-player motion planning (NL vs.\nPSPACE-complete), and bounded 2-player motion planning (P vs. PSPACE-complete),\nand we partially characterize the complexity of unbounded 2-player motion\nplanning (P vs. EXPTIME-complete), bounded 2-team motion planning (P vs.\nNEXPTIME-complete), and unbounded 2-team motion planning (P vs. undecidable).\nThese results can be seen as an alternative to Constraint Logic (which has\nalready proved useful as a basis for hardness reductions), providing a wide\nvariety of agent-based gadgets, any one of which suffices to prove a problem\nhard.'"
Erik Demaine,Demaine_Erik,arXiv:1812.01167,https://arxiv.org/abs/1812.01167,"b'Abstract:  We characterize when two conic curved creases are compatible with each other,\nwhen the rule lines must converge to conic foci and reflect at the crease.\nNamely, two conics are compatible (can be connected by rule segments in a\nfoldable curved crease pattern) if and only if they have equal or reciprocal\neccentricity. Thus, circles (eccentricity 0) and parabolas (eccentricity 1) are\ncompatible with only themselves (when scaled from a focus), and ellipses\n(eccentricity strictly between 0 and 1) and hyperbolas (eccentricity above 1)\nare compatible with themselves and each other (but only in specific pairings).\nThe foundation of this result is a general condition relating any two curved\ncreases connected by rule segments. We also use our characterization to analyze\nseveral curved crease designs.'"
Erik Demaine,Demaine_Erik,arXiv:1812.01160,https://arxiv.org/abs/1812.01160,"b'Abstract:  In this paper, we show that deciding rigid foldability of a given crease\npattern using all creases is weakly NP-hard by a reduction from Partition, and\nthat deciding rigid foldability with optional creases is strongly NP-hard by a\nreduction from 1-in-3 SAT. Unlike flat foldability of origami or flexibility of\nother kinematic linkages, whose complexity originates in the complexity of the\nlayer ordering and possible self-intersection of the material, rigid\nfoldability from a planar state is hard even though there is no potential\nself-intersection. In fact, the complexity comes from the combinatorial\nbehavior of the different possible rigid folding configurations at each vertex.\nThe results underpin the fact that it is harder to fold from an unfolded sheet\nof paper than to unfold a folded state back to a plane, frequently encountered\nproblem when realizing folding-based systems such as self-folding matter and\nreconfigurable robots.'"
Erik Demaine,Demaine_Erik,arXiv:1808.07540,https://arxiv.org/abs/1808.07540,"b'Abstract:  Cookie Clicker is a popular online incremental game where the goal of the\ngame is to generate as many cookies as possible. In the game you start with an\ninitial cookie generation rate, and you can use cookies as currency to purchase\nvarious items that increase your cookie generation rate. In this paper, we\nanalyze strategies for playing Cookie Clicker optimally. While simple to state,\nthe game gives rise to interesting analysis involving ideas from NP-hardness,\napproximation algorithms, and dynamic programming.'"
Erik Demaine,Demaine_Erik,arXiv:1807.04682,https://arxiv.org/abs/1807.04682,"b""Abstract:  An oritatami system (OS) is a theoretical model of self-assembly via\nco-transcriptional folding. It consists of a growing chain of beads which can\nform bonds with each other as they are transcribed. During the transcription\nprocess, the $\\delta$ most recently produced beads dynamically fold so as to\nmaximize the number of bonds formed, self-assemblying into a shape\nincrementally. The parameter $\\delta$ is called the delay and is related to the\ntranscription rate in nature.\nThis article initiates the study of shape self-assembly using oritatami. A\nshape is a connected set of points in the triangular lattice. We first show\nthat oritatami systems differ fundamentally from tile-assembly systems by\nexhibiting a family of infinite shapes that can be tile-assembled but cannot be\nfolded by any OS. As it is NP-hard in general to determine whether there is an\nOS that folds into (self-assembles) a given finite shape, we explore the\nfolding of upscaled versions of finite shapes. We show that any shape can be\nfolded from a constant size seed, at any scale n >= 3, by an OS with delay 1.\nWe also show that any shape can be folded at the smaller scale 2 by an OS with\nunbounded delay. This leads us to investigate the influence of delay and to\nprove that, for all {\\delta} > 2, there are shapes that can be folded (at scale\n1) with delay {\\delta} but not with delay {\\delta}'<{\\delta}. These results\nserve as a foundation for the study of shape-building in this new model of\nself-assembly, and have the potential to provide better understanding of\ncotranscriptional folding in biology, as well as improved abilities of\nexperimentalists to design artificial systems that self-assemble via this\ncomplex dynamical process."""
Erik Demaine,Demaine_Erik,arXiv:1806.05657,https://arxiv.org/abs/1806.05657,b'Abstract:  We prove computational intractability of variants of checkers: (1) deciding\nwhether there is a move that forces the other player to win in one move is\nNP-complete; (2) checkers where players must always be able to jump on their\nturn is PSPACE-complete; and (3) cooperative versions of (1) and (2) are\nNP-complete. We also give cooperative checkers puzzles whose solutions are the\nletters of the alphabet.'
Erik Demaine,Demaine_Erik,arXiv:1806.03539,https://arxiv.org/abs/1806.03539,"b'Abstract:  We initiate a general theory for analyzing the complexity of motion planning\nof a single robot through a graph of ""gadgets"", each with their own state, set\nof locations, and allowed traversals between locations that can depend on and\nchange the state. This type of setup is common to many robot motion planning\nhardness proofs. We characterize the complexity for a natural simple case: each\ngadget connects up to four locations in a perfect matching (but each direction\ncan be traversable or not in the current state), has one or two states, every\ngadget traversal is immediately undoable, and that gadget locations are\nconnected by an always-traversable forest, possibly restricted to avoid\ncrossings in the plane. Specifically, we show that any single nontrivial\nfour-location two-state gadget type is enough for motion planning to become\nPSPACE-complete, while any set of simpler gadgets (effectively two-location or\none-state) has a polynomial-time motion planning algorithm. As a sample\napplication, our results show that motion planning games with ""spinners"" are\nPSPACE-complete, establishing a new hard aspect of Zelda: Oracle of Seasons.'"
Erik Demaine,Demaine_Erik,arXiv:1806.02771,https://arxiv.org/abs/1806.02771,"b'Abstract:  We develop a new framework for generalizing approximation algorithms from the\nstructural graph algorithm literature so that they apply to graphs somewhat\nclose to that class (a scenario we expect is common when working with\nreal-world networks) while still guaranteeing approximation ratios. The idea is\nto $\\textit{edit}$ a given graph via vertex- or edge-deletions to put the graph\ninto an algorithmically tractable class, apply known approximation algorithms\nfor that class, and then $\\textit{lift}$ the solution to apply to the original\ngraph. We give a general characterization of when an optimization problem is\namenable to this approach, and show that it includes many well-studied graph\nproblems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum\nMaximal Matching, Chromatic Number, ($\\ell$-)Dominating Set, Edge\n($\\ell$-)Dominating Set, and Connected Dominating Set.\nTo enable this framework, we develop new editing algorithms that find the\napproximately-fewest edits required to bring a given graph into one of several\nimportant graph classes (in some cases, also approximating the target parameter\nof the family). For bounded degeneracy, we obtain a bicriteria\n$(4,4)$-approximation which also extends to a smoother bicriteria trade-off.\nFor bounded treewidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log\nw}))$-approximation, and for bounded pathwidth, we obtain a bicriteria\n$(O(\\log^{1.5} n), O(\\sqrt{\\log w} \\cdot \\log n))$-approximation. For treedepth\n$2$ (also related to bounded expansion), we obtain a $4$-approximation. We also\nprove complementary hardness-of-approximation results assuming $\\mathrm{P} \\neq\n\\mathrm{NP}$: in particular, these problems are all log-factor inapproximable,\nexcept the last which is not approximable below some constant factor ($2$\nassuming UGC).'"
Erik Demaine,Demaine_Erik,arXiv:1805.04055,https://arxiv.org/abs/1805.04055,"b""Abstract:  We consider the computational complexity of reconfiguration problems, in\nwhich one is given two combinatorial configurations satisfying some\nconstraints, and is asked to transform one into the other using elementary\ntransformations, while satisfying the constraints at all times. Such problems\nappear naturally in many contexts, such as model checking, motion planning,\nenumeration and sampling, and recreational mathematics. We provide hardness\nresults for problems in this family, in which the constraints and operations\nare particularly simple. More precisely, we prove the PSPACE-completeness of\nthe following decision problems:\n$\\bullet$ Given two satisfying assignments to a planar monotone instance of\nNot-All-Equal 3-SAT, can one assignment be transformed into the other by single\nvariable `flips' (assignment changes), preserving satisfiability at every step?\n$\\bullet$ Given two subsets of a set S of integers with the same sum, can one\nsubset be transformed into the other by adding or removing at most three\nelements of S at a time, such that the intermediate subsets also have the same\nsum?\n$\\bullet$ Given two points in $\\{0,1\\}^n$ contained in a polytope P specified\nby a constant number of linear inequalities, is there a path in the n-hypercube\nconnecting the two points and contained in P?\nThese problems can be interpreted as reconfiguration analogues of standard\nproblems in NP. Interestingly, the instances of the NP problems that appear as\ninput to the reconfiguration problems in our reductions can be shown to lie in\nP. In particular, the elements of S and the coefficients of the inequalities\ndefining P can be restricted to have logarithmic bit-length."""
Erik Demaine,Demaine_Erik,arXiv:1804.10193,https://arxiv.org/abs/1804.10193,"b'Abstract:  We analyze the computational complexity of the many types of\npencil-and-paper-style puzzles featured in the 2016 puzzle video game The\nWitness. In all puzzles, the goal is to draw a simple path in a rectangular\ngrid graph from a start vertex to a destination vertex. The different puzzle\ntypes place different constraints on the path: preventing some edges from being\nvisited (broken edges); forcing some edges or vertices to be visited\n(hexagons); forcing some cells to have certain numbers of incident path edges\n(triangles); or forcing the regions formed by the path to be partially\nmonochromatic (squares), have exactly two special cells (stars), or be singly\ncovered by given shapes (polyominoes) and/or negatively counting shapes\n(antipolyominoes). We show that any one of these clue types (except the first)\nis enough to make path finding NP-complete (""witnesses exist but are hard to\nfind""), even for rectangular boards. Furthermore, we show that a final clue\ntype (antibody), which necessarily ""cancels"" the effect of another clue in the\nsame region, makes path finding $\\Sigma_2$-complete (""witnesses do not exist""),\neven with a single antibody (combined with many anti/polyominoes), and the\nproblem gets no harder with many antibodies. On the positive side, we give a\npolynomial-time algorithm for monomino clues, by reducing to hexagon clues on\nthe boundary of the puzzle, even in the presence of broken edges, and solving\n""subset Hamiltonian path"" for terminals on the boundary of an embedded planar\ngraph in polynomial time.'"
Erik Demaine,Demaine_Erik,arXiv:1804.06932,https://arxiv.org/abs/1804.06932,"b'Abstract:  Since the introduction of retroactive data structures at SODA 2004, a major\nunsolved problem has been to bound the gap between the best partially\nretroactive data structure (where changes can be made to the past, but only the\npresent can be queried) and the best fully retroactive data structure (where\nthe past can also be queried) for any problem. It was proved in 2004 that any\npartially retroactive data structure with operation time $T(n,m)$ can be\ntransformed into a fully retroactive data structure with operation time\n$O(\\sqrt{m} \\cdot T(n,m))$, where $n$ is the size of the data structure and $m$\nis the number of operations in the timeline [Demaine 2004], but it has been\nopen for 14 years whether such a gap is necessary.\nIn this paper, we prove nearly matching upper and lower bounds on this gap\nfor all $n$ and $m$. We improve the upper bound for $n \\ll \\sqrt m$ by showing\na new transformation with multiplicative overhead $n \\log m$. We then prove a\nlower bound of $\\min\\{n \\log m, \\sqrt m\\}^{1-o(1)}$ assuming any of the\nfollowing conjectures:\n- Conjecture I: Circuit SAT requires $2^{n - o(n)}$ time on $n$-input\ncircuits of size $2^{o(n)}$. (Far weaker than the well-believed SETH\nconjecture, which asserts that CNF SAT with $n$ variables and $O(n)$ clauses\nalready requires $2^{n-o(n)}$ time.)\n- Conjecture II: Online $(\\min,+)$ product between an integer $n\\times n$\nmatrix and $n$ vectors requires $n^{3 - o(1)}$ time.\n- Conjecture III (3-SUM Conjecture): Given three sets $A,B,C$ of integers,\neach of size $n$, deciding whether there exist $a \\in A, b \\in B, c \\in C$ such\nthat $a + b + c = 0$ requires $n^{2 - o(1)}$ time.\nOur lower bound construction illustrates an interesting power of fully\nretroactive queries: they can be used to quickly solve batched pair evaluation.\nWe believe this technique can prove useful for other data structure lower\nbounds, especially dynamic ones.'"
Erik Demaine,Demaine_Erik,arXiv:1803.03708,https://arxiv.org/abs/1803.03708,"b'Abstract:  We analyze the computational complexity of optimally playing the two-player\nboard game Push Fight, generalized to an arbitrary board and number of pieces.\nWe prove that the game is PSPACE-hard to decide who will win from a given\nposition, even for simple (almost rectangular) hole-free boards. We also\nanalyze the mate-in-1 problem: can the player win in a single turn? One turn in\nPush Fight consists of up to two ""moves"" followed by a mandatory ""push"". With\nthese rules, or generalizing the number of allowed moves to any constant, we\nshow mate-in-1 can be solved in polynomial time. If, however, the number of\nmoves per turn is part of the input, the problem becomes NP-complete. On the\nother hand, without any limit on the number of moves per turn, the problem\nbecomes polynomially solvable again.'"
Erik Demaine,Demaine_Erik,arXiv:1803.01176,https://arxiv.org/abs/1803.01176,"b'Abstract:  We prove that path puzzles with complete row and column information--or\nequivalently, 2D orthogonal discrete tomography with Hamiltonicity\nconstraint--are strongly NP-complete, ASP-complete, and #P-complete. Along the\nway, we newly establish ASP-completeness and #P-completeness for 3-Dimensional\nMatching and Numerical 3-Dimensional Matching.'"
Erik Demaine,Demaine_Erik,arXiv:1803.01172,https://arxiv.org/abs/1803.01172,"b'Abstract:  We prove that two polygons $A$ and $B$ have a reversible hinged dissection (a\nchain hinged dissection that reverses inside and outside boundaries when\nfolding between $A$ and $B$) if and only if $A$ and $B$ are two non-crossing\nnets of a common polyhedron. Furthermore, monotone hinged dissections (where\nall hinges rotate in the same direction when changing from $A$ to $B$\ncorrespond exactly to non-crossing nets of a common convex polyhedron. By\nenvelope/parcel magic, it becomes easy to design many hinged dissections.'"
Erik Demaine,Demaine_Erik,arXiv:1801.01689,https://arxiv.org/abs/1801.01689,"b'Abstract:  We present a number of breakthroughs for coordinated motion planning, in\nwhich the objective is to reconfigure a swarm of labeled convex objects by a\ncombination of parallel, continuous, collision-free translations into a given\ntarget arrangement. Problems of this type can be traced back to the classic\nwork of Schwartz and Sharir (1983), who gave a method for deciding the\nexistence of a coordinated motion for a set of disks between obstacles; their\napproach is polynomial in the complexity of the obstacles, but exponential in\nthe number of disks. Other previous work has largely focused on {\\em\nsequential} schedules, in which one robot moves at a time.\nWe provide constant-factor approximation algorithms for minimizing the\nexecution time of a coordinated, {\\em parallel} motion plan for a swarm of\nrobots in the absence of obstacles, provided some amount of separability.\nOur algorithm achieves {\\em constant stretch factor}: If all robots are at\nmost $d$ units from their respective starting positions, the total duration of\nthe overall schedule is $O(d)$. Extensions include unlabeled robots and\ndifferent classes of robots. We also prove that finding a plan with minimal\nexecution time is NP-hard, even for a grid arrangement without any stationary\nobstacles. On the other hand, we show that for densely packed disks that cannot\nbe well separated, a stretch factor $\\Omega(N^{1/4})$ may be required. On the\npositive side, we establish a stretch factor of $O(N^{1/2})$ even in this case.'"
Erik Demaine,Demaine_Erik,arXiv:1712.09317,https://arxiv.org/abs/1712.09317,"b'Abstract:  We study the problem of folding a polyomino $P$ into a polycube $Q$, allowing\nfaces of $Q$ to be covered multiple times. First, we define a variety of\nfolding models according to whether the folds (a) must be along grid lines of\n$P$ or can divide squares in half (diagonally and/or orthogonally), (b) must be\nmountain or can be both mountain and valley, (c) can remain flat (forming an\nangle of $180^\\circ$), and (d) must lie on just the polycube surface or can\nhave interior faces as well. Second, we give all the inclusion relations among\nall models that fold on the grid lines of $P$. Third, we characterize all\npolyominoes that can fold into a unit cube, in some models. Fourth, we give a\nlinear-time dynamic programming algorithm to fold a tree-shaped polyomino into\na constant-size polycube, in some models. Finally, we consider the triangular\nversion of the problem, characterizing which polyiamonds fold into a regular\ntetrahedron.'"
Erik Demaine,Demaine_Erik,arXiv:1712.01197,https://arxiv.org/abs/1712.01197,"b'Abstract:  We investigate algorithmic control of a large swarm of mobile particles (such\nas robots, sensors, or building material) that move in a 2D workspace using a\nglobal input signal (such as gravity or a magnetic field). We show that a maze\nof obstacles to the environment can be used to create complex systems. We\nprovide a wide range of results for a wide range of questions. These can be\nsubdivided into external algorithmic problems, in which particle configurations\nserve as input for computations that are performed elsewhere, and internal\nlogic problems, in which the particle configurations themselves are used for\ncarrying out computations. For external algorithms, we give both negative and\npositive results. If we are given a set of stationary obstacles, we prove that\nit is NP-hard to decide whether a given initial configuration of unit-sized\nparticles can be transformed into a desired target configuration. Moreover, we\nshow that finding a control sequence of minimum length is PSPACE-complete. We\nalso work on the inverse problem, providing constructive algorithms to design\nworkspaces that efficiently implement arbitrary permutations between different\nconfigurations. For internal logic, we investigate how arbitrary computations\ncan be implemented. We demonstrate how to encode dual-rail logic to build a\nuniversal logic gate that concurrently evaluates and, nand, nor, and or\noperations. Using many of these gates and appropriate interconnects, we can\nevaluate any logical expression. However, we establish that simulating the full\nrange of complex interactions present in arbitrary digital circuits encounters\na fundamental difficulty: a fan-out gate cannot be generated. We resolve this\nmissing component with the help of 2x1 particles, which can create fan-out\ngates that produce multiple copies of the inputs. Using these gates we provide\nrules for replicating arbitrary digital circuits.'"
Erik Demaine,Demaine_Erik,arXiv:1711.07960,https://arxiv.org/abs/1711.07960,"b'Abstract:  This paper initiates the study of I/O algorithms (minimizing cache misses)\nfrom the perspective of fine-grained complexity (conditional polynomial lower\nbounds). Specifically, we aim to answer why sparse graph problems are so hard,\nand why the Longest Common Subsequence problem gets a savings of a factor of\nthe size of cache times the length of a cache line, but no more. We take the\nreductions and techniques from complexity and fine-grained complexity and apply\nthem to the I/O model to generate new (conditional) lower bounds as well as\nfaster algorithms. We also prove the existence of a time hierarchy for the I/O\nmodel, which motivates the fine-grained reductions.\nUsing fine-grained reductions, we give an algorithm for distinguishing 2 vs.\n3 diameter and radius that runs in $O(|E|^2/(MB))$ cache misses, which for\nsparse graphs improves over the previous $O(|V|^2/B)$ running time. We give new\nreductions from radius and diameter to Wiener index and median. We show\nmeaningful reductions between problems that have linear-time solutions in the\nRAM model. The reductions use low I/O complexity (typically $O(n/B)$), and thus\nhelp to finely capture the relationship between ""I/O linear time"" $\\Theta(n/B)$\nand RAM linear time $\\Theta(n)$. We generate new I/O assumptions based on the\ndifficulty of improving sparse graph problem running times in the I/O model. We\ncreate conjectures that the current best known algorithms for Single Source\nShortest Paths (SSSP), diameter, and radius are optimal. From these I/O-model\nassumptions, we show that many of the known reductions in the word-RAM model\ncan naturally extend to hold in the I/O model as well (e.g., a lower bound on\nthe I/O complexity of Longest Common Subsequence that matches the best known\nrunning time). Finally, we prove an analog of the Time Hierarchy Theorem in the\nI/O model.'"
Erik Demaine,Demaine_Erik,arXiv:1709.01241,https://arxiv.org/abs/1709.01241,"b""Abstract:  This paper proves that push-pull block puzzles in 3D are PSPACE-complete to\nsolve, and push-pull block puzzles in 2D with thin walls are NP-hard to solve,\nsettling an open question by Zubaran and Ritt. Push-pull block puzzles are a\ntype of recreational motion planning problem, similar to Sokoban, that involve\nmoving a `robot' on a square grid with $1 \\times 1$ obstacles. The obstacles\ncannot be traversed by the robot, but some can be pushed and pulled by the\nrobot into adjacent squares. Thin walls prevent movement between two adjacent\nsquares. This work follows in a long line of algorithms and complexity work on\nsimilar problems. The 2D push-pull block puzzle shows up in the video games\nPukoban as well as The Legend of Zelda: A Link to the Past, giving another\nproof of hardness for the latter. This variant of block-pushing puzzles is of\nparticular interest because of its connections to reversibility, since any\naction (e.g., push or pull) can be inverted by another valid action (e.g., pull\nor push)."""
Erik Demaine,Demaine_Erik,arXiv:1708.06730,https://arxiv.org/abs/1708.06730,"b'Abstract:  We analyze a directed variation of the book embedding problem when the page\npartition is prespecified and the nodes on the spine must be in topological\norder (upward book embedding). Given a directed acyclic graph and a partition\nof its edges into $k$ pages, can we linearly order the vertices such that the\ndrawing is upward (a topological sort) and each page avoids crossings? We prove\nthat the problem is NP-complete for $k\\ge 3$, and for $k\\ge 4$ even in the\nspecial case when each page is a matching. By contrast, the problem can be\nsolved in linear time for $k=2$ pages when pages are restricted to matchings.\nThe problem comes from Jack Edmonds (1997), motivated as a generalization of\nthe map folding problem from computational origami.'"
Erik Demaine,Demaine_Erik,arXiv:1707.06343,https://arxiv.org/abs/1707.06343,"b'Abstract:  Pebble games are single-player games on DAGs involving placing and moving\npebbles on nodes of the graph according to a certain set of rules. The goal is\nto pebble a set of target nodes using a minimum number of pebbles. In this\npaper, we present a possibly simpler proof of the result in [CLNV15] and\nstrengthen the result to show that it is PSPACE-hard to determine the minimum\nnumber of pebbles to an additive $n^{1/3-\\epsilon}$ term for all $\\epsilon >\n0$, which improves upon the currently known additive constant hardness of\napproximation [CLNV15] in the standard pebble game. We also introduce a family\nof explicit, constant indegree graphs with $n$ nodes where there exists a graph\nin the family such that using constant $k$ pebbles requires $\\Omega(n^k)$ moves\nto pebble in both the standard and black-white pebble games. This independently\nanswers an open question summarized in [Nor15] of whether a family of DAGs\nexists that meets the upper bound of $O(n^k)$ moves using constant $k$ pebbles\nwith a different construction than that presented in [AdRNV17].'"
Erik Demaine,Demaine_Erik,arXiv:1707.03146,https://arxiv.org/abs/1707.03146,"b'Abstract:  The 15 puzzle is a classic reconfiguration puzzle with fifteen uniquely\nlabeled unit squares within a $4 \\times 4$ board in which the goal is to slide\nthe squares (without ever overlapping) into a target configuration. By\ngeneralizing the puzzle to an $n \\times n$ board with $n^2-1$ squares, we can\nstudy the computational complexity of problems related to the puzzle; in\nparticular, we consider the problem of determining whether a given end\nconfiguration can be reached from a given start configuration via at most a\ngiven number of moves. This problem was shown NP-complete in Ratner and Warmuth\n(1990). We provide an alternative simpler proof of this fact by reduction from\nthe rectilinear Steiner tree problem.'"
Erik Demaine,Demaine_Erik,arXiv:1706.10046,https://arxiv.org/abs/1706.10046,"b'Abstract:  In 2007, Arkin et al. initiated a systematic study of the complexity of the\nHamiltonian cycle problem on square, triangular, or hexagonal grid graphs,\nrestricted to polygonal, thin, superthin, degree-bounded, or solid grid graphs.\nThey solved many combinations of these problems, proving them either\npolynomially solvable or NP-complete, but left three combinations open. In this\npaper, we prove two of these unsolved combinations to be NP-complete:\nHamiltonicity of Square Polygonal Grid Graphs and Hamiltonicity of Hexagonal\nThin Grid Graphs. We also consider a new restriction, where the grid graph is\nboth thin and polygonal, and prove that Hamiltonicity then becomes polynomially\nsolvable for square, triangular, and hexagonal grid graphs.'"
Erik Demaine,Demaine_Erik,arXiv:1706.07900,https://arxiv.org/abs/1706.07900,"b'Abstract:  In this paper, we introduce a new problem called Tree-Residue Vertex-Breaking\n(TRVB): given a multigraph $G$ some of whose vertices are marked ""breakable,""\nis it possible to convert $G$ into a tree via a sequence of ""vertex-breaking""\noperations (replacing a degree-$k$ breakable vertex by $k$ degree-$1$ vertices,\ndisconnecting the $k$ incident edges)?\nWe characterize the computational complexity of TRVB with any combination of\nthe following additional constraints: $G$ must be planar, $G$ must be a simple\ngraph, the degree of every breakable vertex must belong to an allowed list $B$,\nand the degree of every unbreakable vertex must belong to an allowed list $U$.\nThe two results which we expect to be most generally applicable are that (1)\nTRVB is polynomially solvable when breakable vertices are restricted to have\ndegree at most $3$; and (2) for any $k \\ge 4$, TRVB is NP-complete when the\ngiven multigraph is restricted to be planar and to consist entirely of\ndegree-$k$ breakable vertices. To demonstrate the use of TRVB, we give a simple\nproof of the known result that Hamiltonicity in max-degree-$3$ square grid\ngraphs is NP-hard.\nWe also demonstrate a connection between TRVB and the Hypergraph Spanning\nTree problem. This connection allows us to show that the Hypergraph Spanning\nTree problem in $k$-uniform $2$-regular hypergraphs is NP-complete for any $k\n\\ge 4$, even when the incidence graph of the hypergraph is planar.'"
Erik Demaine,Demaine_Erik,arXiv:1706.06708,https://arxiv.org/abs/1706.06708,"b""Abstract:  In this paper, we prove that optimally solving an $n \\times n \\times n$\nRubik's Cube is NP-complete by reducing from the Hamiltonian Cycle problem in\nsquare grid graphs. This improves the previous result that optimally solving an\n$n \\times n \\times n$ Rubik's Cube with missing stickers is NP-complete. We\nprove this result first for the simpler case of the Rubik's Square---an $n\n\\times n \\times 1$ generalization of the Rubik's Cube---and then proceed with a\nsimilar but more complicated proof for the Rubik's Cube case."""
Erik Demaine,Demaine_Erik,arXiv:1703.06373,https://arxiv.org/abs/1703.06373,b'Abstract:  This paper addresses the problem of finding minimum forcing sets in origami.\nThe origami material folds flat along straight lines called creases that can be\nlabeled as mountains or valleys. A forcing set is a subset of creases that\nforce all the other creases to fold according to their labels. The result is a\nflat folding of the origami material. In this paper we develop a linear time\nalgorithm that finds minimum forcing sets in one dimensional origami.'
Erik Demaine,Demaine_Erik,arXiv:1703.02671,https://arxiv.org/abs/1703.02671,"b'Abstract:  We study the complexity of symmetric assembly puzzles: given a collection of\nsimple polygons, can we translate, rotate, and possibly flip them so that their\ninterior-disjoint union is line symmetric? On the negative side, we show that\nthe problem is strongly NP-complete even if the pieces are all polyominos. On\nthe positive side, we show that the problem can be solved in polynomial time if\nthe number of pieces is a fixed constant.'"
Erik Demaine,Demaine_Erik,arXiv:1701.05999,https://arxiv.org/abs/1701.05999,"b""Abstract:  A conflict-free k-coloring of a graph assigns one of k different colors to\nsome of the vertices such that, for every vertex v, there is a color that is\nassigned to exactly one vertex among v and v's neighbors. Such colorings have\napplications in wireless networking, robotics, and geometry, and are\nwell-studied in graph theory. Here we study the natural problem of the\nconflict-free chromatic number chi_CF(G) (the smallest k for which\nconflict-free k-colorings exist). We provide results both for closed\nneighborhoods N[v], for which a vertex v is a member of its neighborhood, and\nfor open neighborhoods N(v), for which vertex v is not a member of its\nneighborhood.\nFor closed neighborhoods, we prove the conflict-free variant of the famous\nHadwiger Conjecture: If an arbitrary graph G does not contain K_{k+1} as a\nminor, then chi_CF(G) <= k. For planar graphs, we obtain a tight worst-case\nbound: three colors are sometimes necessary and always sufficient. We also give\na complete characterization of the computational complexity of conflict-free\ncoloring. Deciding whether chi_CF(G)<= 1 is NP-complete for planar graphs G,\nbut polynomial for outerplanar graphs. Furthermore, deciding whether\nchi_CF(G)<= 2 is NP-complete for planar graphs G, but always true for\nouterplanar graphs. For the bicriteria problem of minimizing the number of\ncolored vertices subject to a given bound k on the number of colors, we give a\nfull algorithmic characterization in terms of complexity and approximation for\nouterplanar and planar graphs.\nFor open neighborhoods, we show that every planar bipartite graph has a\nconflict-free coloring with at most four colors; on the other hand, we prove\nthat for k in {1,2,3}, it is NP-complete to decide whether a planar bipartite\ngraph has a conflict-free k-coloring. Moreover, we establish that any general}\nplanar graph has a conflict-free coloring with at most eight colors."""
Erik Demaine,Demaine_Erik,arXiv:1701.00146,https://arxiv.org/abs/1701.00146,"b'Abstract:  We prove the computational intractability of rotating and placing $n$ square\ntiles into a $1 \\times n$ array such that adjacent tiles are compatible--either\nequal edge colors, as in edge-matching puzzles, or matching tab/pocket shapes,\nas in jigsaw puzzles. Beyond basic NP-hardness, we prove that it is NP-hard\neven to approximately maximize the number of placed tiles (allowing blanks),\nwhile satisfying the compatibility constraint between nonblank tiles, within a\nfactor of 0.9999999851. (On the other hand, there is an easy $1 \\over\n2$-approximation.) This is the first (correct) proof of inapproximability for\nedge-matching and jigsaw puzzles. Along the way, we prove NP-hardness of\ndistinguishing, for a directed graph on $n$ nodes, between having a Hamiltonian\npath (length $n-1$) and having at most $0.999999284 (n-1)$ edges that form a\nvertex-disjoint union of paths. We use this gap hardness and gap-preserving\nreductions to establish similar gap hardness for $1 \\times n$ jigsaw and\nedge-matching puzzles.'"
Erik Demaine,Demaine_Erik,arXiv:1611.10319,https://arxiv.org/abs/1611.10319,"b'Abstract:  We classify the computational complexity of the popular video games Portal\nand Portal 2. We isolate individual mechanics of the game and prove\nNP-hardness, PSPACE-completeness, or (pseudo)polynomiality depending on the\nspecific game mechanics allowed. One of our proofs generalizes to prove\nNP-hardness of many other video games such as Half-Life 2, Halo, Doom, Elder\nScrolls, Fallout, Grand Theft Auto, Left 4 Dead, Mass Effect, Deus Ex, Metal\nGear Solid, and Resident Evil.\nThese results build on the established literature on the complexity of video\ngames.'"
Erik Demaine,Demaine_Erik,arXiv:1611.03187,https://arxiv.org/abs/1611.03187,"b'Abstract:  We present two universal hinge patterns that enable a strip of material to\nfold into any connected surface made up of unit squares on the 3D cube\ngrid--for example, the surface of any polycube. The folding is efficient: for\ntarget surfaces topologically equivalent to a sphere, the strip needs to have\nonly twice the target surface area, and the folding stacks at most two layers\nof material anywhere. These geometric results offer a new way to build\nprogrammable matter that is substantially more efficient than what is possible\nwith a square $N \\times N$ sheet of material, which can fold into all polycubes\nonly of surface area $O(N)$ and may stack $\\Theta(N^2)$ layers at one point. We\nalso show how our strip foldings can be executed by a rigid motion without\ncollisions, which is not possible in general with 2D sheet folding.\nTo achieve these results, we develop new approximation algorithms for milling\nthe surface of a grid polyhedron, which simultaneously give a 2-approximation\nin tour length and an 8/3-approximation in the number of turns. Both length and\nturns consume area when folding a strip, so we build on past approximation\nalgorithms for these two objectives from 2D milling.'"
Erik Demaine,Demaine_Erik,arXiv:1611.00106,https://arxiv.org/abs/1611.00106,"b'Abstract:  We show that every orthogonal polyhedron of genus at most 2 can be unfolded\nwithout overlap while using only a linear number of orthogonal cuts (parallel\nto the polyhedron edges). This is the first result on unfolding general\northogonal polyhedra beyond genus-0. Our unfolding algorithm relies on the\nexistence of at most 2 special leaves in what we call the ""unfolding tree""\n(which ties back to the genus), so unfolding polyhedra of genus 3 and beyond\nrequires new techniques.'"
Erik Demaine,Demaine_Erik,arXiv:1608.00477,https://arxiv.org/abs/1608.00477,"b'Abstract:  We show how to design a universal shape replicator in a self-assembly system\nwith both attractive and repulsive forces. More precisely, we show that there\nis a universal set of constant-size objects that, when added to any unknown\nhole-free polyomino shape, produces an unbounded number of copies of that shape\n(plus constant-size garbage objects). The constant-size objects can be easily\nconstructed from a constant number of individual tile types using a constant\nnumber of preprocessing self-assembly steps. Our construction uses the\nwell-studied 2-Handed Assembly Model (2HAM) of tile self-assembly, in the\nsimple model where glues interact only with identical glues, allowing glue\nstrengths that are either positive (attractive) or negative (repulsive), and\nconstant temperature (required glue strength for parts to hold together). We\nalso require that the given shape has specified glue types on its surface, and\nthat the feature size (smallest distance between nonincident edges) is bounded\nbelow by a constant. Shape replication necessarily requires a self-assembly\nmodel where parts can both attach and detach, and this construction is the\nfirst to do so using the natural model of negative/repulsive glues (also\nstudied before for other problems such as fuel-efficient computation); previous\nreplication constructions require more powerful global operations such as an\n""enzyme"" that destroys a subset of the tile types.'"
Erik Demaine,Demaine_Erik,arXiv:1607.04220,https://arxiv.org/abs/1607.04220,"b'Abstract:  This paper proves that arrangement of music is NP-hard when subject to\nvarious constraints: avoiding musical dissonance, limiting how many notes can\nbe played simultaneously, and limiting transition speed between chords. These\nresults imply the computational complexity of related musical problems,\nincluding musical choreography and rhythm games.'"
Erik Demaine,Demaine_Erik,arXiv:1607.01826,https://arxiv.org/abs/1607.01826,"b'Abstract:  We study the computational complexity of the Buttons \\& Scissors game and\nobtain sharp thresholds with respect to several parameters. Specifically we\nshow that the game is NP-complete for $C = 2$ colors but polytime solvable for\n$C = 1$. Similarly the game is NP-complete if every color is used by at most $F\n= 4$ buttons but polytime solvable for $F \\leq 3$. We also consider\nrestrictions on the board size, cut directions, and cut sizes. Finally, we\nintroduce several natural two-player versions of the game and show that they\nare PSPACE-complete.'"
Erik Demaine,Demaine_Erik,arXiv:1605.08475,https://arxiv.org/abs/1605.08475,"b'Abstract:  We introduce a new programming language for expressing reversibility,\nEnergy-Efficient Language (Eel), geared toward algorithm design and\nimplementation. Eel is the first language to take advantage of a partially\nreversible computation model, where programs can be composed of both reversible\nand irreversible operations. In this model, irreversible operations cost energy\nfor every bit of information created or destroyed. To handle programs of\nvarying degrees of reversibility, Eel supports a log stack to automatically\ntrade energy costs for space costs, and introduces many powerful control logic\noperators including protected conditional, general conditional, protected\nloops, and general loops. In this paper, we present the design and compiler for\nthe three language levels of Eel along with an interpreter to simulate and\nannotate incurred energy costs of a program.'"
Erik Demaine,Demaine_Erik,arXiv:1605.08448,https://arxiv.org/abs/1605.08448,"b""Abstract:  We initiate the systematic study of the energy complexity of algorithms (in\naddition to time and space complexity) based on Landauer's Principle in\nphysics, which gives a lower bound on the amount of energy a system must\ndissipate if it destroys information. We propose energy-aware variations of\nthree standard models of computation: circuit RAM, word RAM, and\ntransdichotomous RAM. On top of these models, we build familiar high-level\nprimitives such as control logic, memory allocation, and garbage collection\nwith zero energy complexity and only constant-factor overheads in space and\ntime complexity, enabling simple expression of energy-efficient algorithms. We\nanalyze several classic algorithms in our models and develop low-energy\nvariations: comparison sort, insertion sort, counting sort, breadth-first\nsearch, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL\ntrees, binary heaps, and dynamic arrays. We explore the time/space/energy\ntrade-off and develop several general techniques for analyzing algorithms and\nreducing their energy complexity. These results lay a theoretical foundation\nfor a new field of semi-reversible computing and provide a new framework for\nthe investigation of algorithms."""
Erik Demaine,Demaine_Erik,arXiv:1601.05747,https://arxiv.org/abs/1601.05747,"b'Abstract:  Modeling folding surfaces with nonzero thickness is of practical interest for\nmechanical engineering. There are many existing approaches that account for\nmaterial thickness in folding applications. We propose a new systematic and\nbroadly applicable algorithm to transform certain flat-foldable crease patterns\ninto new crease patterns with similar folded structure but with a\nfacet-separated folded state. We provide conditions on input crease patterns\nfor the algorithm to produce a thickened crease pattern avoiding local self\nintersection, and provide bounds for the maximum thickness that the algorithm\ncan produce for a given input. We demonstrate these results in parameterized\nnumerical simulations and physical models.'"
Erik Demaine,Demaine_Erik,arXiv:1601.05706,https://arxiv.org/abs/1601.05706,"b'Abstract:  Inspired by the Japanese game Pachinko, we study simple (perfectly\n""inelastic"" collisions) dynamics of a unit ball falling amidst point obstacles\n(pins) in the plane. A classic example is that a checkerboard grid of pins\nproduces the binomial distribution, but what probability distributions result\nfrom different pin placements? In the 50-50 model, where the pins form a subset\nof this grid, not all probability distributions are possible, but surprisingly\nthe uniform distribution is possible for $\\{1,2,4,8,16\\}$ possible drop\nlocations. Furthermore, every probability distribution can be approximated\narbitrarily closely, and every dyadic probability distribution can be divided\nby a suitable power of $2$ and then constructed exactly (along with extra\n""junk"" outputs). In a more general model, if a ball hits a pin off center, it\nfalls left or right accordingly. Then we prove a universality result: any\ndistribution of $n$ dyadic probabilities, each specified by $k$ bits, can be\nconstructed using $O(n k^2)$ pins, which is close to the information-theoretic\nlower bound of $\\Omega(n k)$.'"
Erik Demaine,Demaine_Erik,arXiv:1512.06706,https://arxiv.org/abs/1512.06706,"b'Abstract:  We prove that it is NP-hard to dissect one simple orthogonal polygon into\nanother using a given number of pieces, as is approximating the fewest pieces\nto within a factor of $1+1/1080-\\varepsilon$.'"
Erik Demaine,Demaine_Erik,arXiv:1507.01644,https://arxiv.org/abs/1507.01644,"b'Abstract:  We develop an intrinsic necessary and sufficient condition for single-vertex\norigami crease patterns to be able to fold rigidly. We classify such patterns\nin the case where the creases are pre-assigned to be mountains and valleys as\nwell as in the unassigned case. We also illustrate the utility of this result\nby applying it to the new concept of minimal forcing sets for rigid origami\nmodels, which are the smallest collection of creases that, when folded, will\nforce all the other creases to fold in a prescribed way.'"
Erik Demaine,Demaine_Erik,arXiv:1506.08409,https://arxiv.org/abs/1506.08409,"b'Abstract:  We prove that the classic 1994 Taito video game, known as Puzzle Bobble or\nBust-a-Move, is NP-complete. Our proof applies to the perfect-information\nversion where the bubble sequence is known in advance, and it uses just three\nbubble colors.'"
Erik Demaine,Demaine_Erik,arXiv:1505.07862,https://arxiv.org/abs/1505.07862,"b'Abstract:  We consider staged self-assembly systems, in which square-shaped tiles can be\nadded to bins in several stages. Within these bins, the tiles may connect to\neach other, depending on the glue types of their edges. Previous work by\nDemaine et al. showed that a relatively small number of tile types suffices to\nproduce arbitrary shapes in this model. However, these constructions were only\nbased on a spanning tree of the geometric shape, so they did not produce full\nconnectivity of the underlying grid graph in the case of shapes with holes;\ndesigning fully connected assemblies with a polylogarithmic number of stages\nwas left as a major open problem. We resolve this challenge by presenting new\nsystems for staged assembly that produce fully connected polyominoes in O(log^2\nn) stages, for various scale factors and temperature {\\tau} = 2 as well as\n{\\tau} = 1. Our constructions work even for shapes with holes and uses only a\nconstant number of glues and tiles. Moreover, the underlying approach is more\ngeometric in nature, implying that it promised to be more feasible for shapes\nwith compact geometric description.'"
Erik Demaine,Demaine_Erik,arXiv:1502.03191,https://arxiv.org/abs/1502.03191,"b'Abstract:  We describe a general family of curved-crease folding tessellations\nconsisting of a repeating ""lens"" motif formed by two convex curved arcs. The\nthird author invented the first such design in 1992, when he made both a sketch\nof the crease pattern and a vinyl model (pictured below). Curve fitting\nsuggests that this initial design used circular arcs. We show that in fact the\ncurve can be chosen to be any smooth convex curve without inflection point. We\nidentify the ruling configuration through qualitative properties that a curved\nfolding satisfies, and prove that the folded form exists with no additional\ncreases, through the use of differential geometry.'"
Erik Demaine,Demaine_Erik,arXiv:1411.6371,https://arxiv.org/abs/1411.6371,"b'Abstract:  In this paper, we study how to fold a specified origami crease pattern in\norder to minimize the impact of paper thickness. Specifically, origami designs\nare often expressed by a mountain-valley pattern (plane graph of creases with\nrelative fold orientations), but in general this specification is consistent\nwith exponentially many possible folded states. We analyze the complexity of\nfinding the best consistent folded state according to two metrics: minimizing\nthe total number of layers in the folded state (so that a ""flat folding"" is\nindeed close to flat), and minimizing the total amount of paper required to\nexecute the folding (where ""thicker"" creases consume more paper). We prove both\nproblems strongly NP-complete even for 1D folding. On the other hand, we prove\nthe first problem fixed-parameter tractable in 1D with respect to the number of\nlayers.'"
Erik Demaine,Demaine_Erik,arXiv:1410.6520,https://arxiv.org/abs/1410.6520,"b""Abstract:  Given a sheet of paper and a prescribed folding of its boundary, is there a\nway to fold the paper's interior without stretching so that the boundary lines\nup with the prescribed boundary folding? For polygonal boundaries\nnonexpansively folded at finitely many points, we prove that a consistent\nisometric mapping of the polygon interior always exists and is computable in\npolynomial time."""
Erik Demaine,Demaine_Erik,arXiv:1410.5845,https://arxiv.org/abs/1410.5845,"b""Abstract:  When can $t$ terminal pairs in an $m \\times n$ grid be connected by $t$\nvertex-disjoint paths that cover all vertices of the grid? We prove that this\nproblem is NP-complete. Our hardness result can be compared to two previous\nNP-hardness proofs: Lynch's 1975 proof without the ``cover all vertices''\nconstraint, and Kotsuma and Takenaga's 2010 proof when the paths are restricted\nto have the fewest possible corners within their homotopy class. The latter\nrestriction is a common form of the famous Nikoli puzzle \\emph{Numberlink}; our\nproblem is another common form of Numberlink, sometimes called \\emph{Zig-Zag\nNumberlink} and popularized by the smartphone app \\emph{Flow Free}."""
Erik Demaine,Demaine_Erik,arXiv:1408.6771,https://arxiv.org/abs/1408.6771,"b'Abstract:  When can a plane graph with prescribed edge lengths and prescribed angles\n(from among $\\{0,180^\\circ, 360^\\circ$\\}) be folded flat to lie in an\ninfinitesimally thin line, without crossings? This problem generalizes the\nclassic theory of single-vertex flat origami with prescribed mountain-valley\nassignment, which corresponds to the case of a cycle graph. We characterize\nsuch flat-foldable plane graphs by two obviously necessary but also sufficient\nconditions, proving a conjecture made in 2001: the angles at each vertex should\nsum to $360^\\circ$, and every face of the graph must itself be flat foldable.\nThis characterization leads to a linear-time algorithm for testing flat\nfoldability of plane graphs with prescribed edge lengths and angles, and a\npolynomial-time algorithm for counting the number of distinct folded states.'"
Erik Demaine,Demaine_Erik,arXiv:1406.6576,https://arxiv.org/abs/1406.6576,"b'Abstract:  Suppose that we are given two independent sets $I_b$ and $I_r$ of a graph\nsuch that $|I_b|=|I_r|$, and imagine that a token is placed on each vertex in\n$I_b$. Then, the sliding token problem is to determine whether there exists a\nsequence of independent sets which transforms $I_b$ into $I_r$ so that each\nindependent set in the sequence results from the previous one by sliding\nexactly one token along an edge in the graph. This problem is known to be\nPSPACE-complete even for planar graphs, and also for bounded treewidth graphs.\nIn this paper, we thus study the problem restricted to trees, and give the\nfollowing three results: (1) the decision problem is solvable in linear time;\n(2) for a yes-instance, we can find in quadratic time an actual sequence of\nindependent sets between $I_b$ and $I_r$ whose length (i.e., the number of\ntoken-slides) is quadratic; and (3) there exists an infinite family of\ninstances on paths for which any sequence requires quadratic length.'"
Erik Demaine,Demaine_Erik,arXiv:1406.2587,https://arxiv.org/abs/1406.2587,"b'Abstract:  This research establishes that many real-world networks exhibit bounded\nexpansion, a strong notion of structural sparsity, and demonstrates that it can\nbe leveraged to design efficient algorithms for network analysis. We analyze\nseveral common network models regarding their structural sparsity. We show\nthat, with high probability, (1) graphs sampled with a prescribed s parse\ndegree sequence; (2) perturbed bounded-degree graphs; (3) stochastic block\nmodels with small probabilities; result in graphs of bounded expansion.\nIn contrast, we show that the Kleinberg and the Barabasi-Albert model have\nunbounded expansion. We support our findings with empirical measurements on a\ncorpus of real-world networks.'"
Erik Demaine,Demaine_Erik,arXiv:1405.3739,https://arxiv.org/abs/1405.3739,"b""Abstract:  In this paper, we study the problem of fast dynamic pointer following: given\na directed graph $G$ where each vertex has outdegree $1$, efficiently support\nthe operations of i) changing the outgoing edge of any vertex, and ii) find the\nvertex $k$ vertices `after' a given vertex. We exhibit a solution to this\nproblem based on link-cut trees that requires $O(\\lg n)$ time per operation,\nand prove that this is optimal in the cell-probe complexity model."""
Erik Demaine,Demaine_Erik,arXiv:1405.2378,https://arxiv.org/abs/1405.2378,"b""Abstract:  Can folding a piece of paper flat make it larger? We explore whether a shape\n$S$ must be scaled to cover a flat-folded copy of itself. We consider both\nsingle folds and arbitrary folds (continuous piecewise isometries $S\\rightarrow\nR^2$). The underlying problem is motivated by computational origami, and is\nrelated to other covering and fixturing problems, such as Lebesgue's universal\ncover problem and force closure grasps. In addition to considering special\nshapes (squares, equilateral triangles, polygons and disks), we give upper and\nlower bounds on scale factors for single folds of convex objects and arbitrary\nfolds of simply connected objects."""
Erik Demaine,Demaine_Erik,arXiv:1404.1775,https://arxiv.org/abs/1404.1775,"b'Abstract:  Over the past decade, we have designed six typefaces based on mathematical\ntheorems and open problems, specifically computational geometry. These\ntypefaces expose the general public in a unique way to intriguing results and\nhard problems in hinged dissections, geometric tours, origami design,\ncomputer-aided glass design, physical simulation, and protein folding. In\nparticular, most of these typefaces include puzzle fonts, where reading the\nintended message requires solving a series of puzzles which illustrate the\nchallenge of the underlying algorithmic problem.'"
Erik Demaine,Demaine_Erik,arXiv:1403.7980,https://arxiv.org/abs/1403.7980,"b'Abstract:  A stacking operation adds a $d$-simplex on top of a facet of a simplicial\n$d$-polytope while maintaining the convexity of the polytope. A stacked\n$d$-polytope is a polytope that is obtained from a $d$-simplex and a series of\nstacking operations. We show that for a fixed $d$ every stacked $d$-polytope\nwith $n$ vertices can be realized with nonnegative integer coordinates. The\ncoordinates are bounded by $O(n^{2\\log(2d)})$, except for one axis, where the\ncoordinates are bounded by $O(n^{3\\log(2d)})$. The described realization can be\ncomputed with an easy algorithm.\nThe realization of the polytopes is obtained with a lifting technique which\nproduces an embedding on a large grid. We establish a rounding scheme that\nplaces the vertices on a sparser grid, while maintaining the convexity of the\nembedding.'"
Erik Demaine,Demaine_Erik,arXiv:1402.3749,https://arxiv.org/abs/1402.3749,"b'Abstract:  Micro- and nanorobots are often controlled by global input signals, such as\nan electromagnetic or gravitational field. These fields move each robot\nmaximally until it hits a stationary obstacle or another stationary robot. This\npaper investigates 2D motion-planning complexity for large swarms of simple\nmobile robots (such as bacteria, sensors, or smart building material).\nIn previous work we proved it is NP-hard to decide whether a given initial\nconfiguration can be transformed into a desired target configuration; in this\npaper we prove a stronger result: the problem of finding an optimal control\nsequence is PSPACE-complete. On the positive side, we show we can build useful\nsystems by designing obstacles. We present a reconfigurable hardware platform\nand demonstrate how to form arbitrary permutations and build a compact absolute\nencoder. We then take the same platform and use dual-rail logic to build a\nuniversal logic gate that concurrently evaluates AND, NAND, NOR and OR\noperations. Using many of these gates and appropriate interconnects we can\nevaluate any logical expression.'"
Erik Demaine,Demaine_Erik,arXiv:1401.7970,https://arxiv.org/abs/1401.7970,"b'Abstract:  We study the power of fractional allocations of resources to maximize\ninfluence in a network. This work extends in a natural way the well-studied\nmodel by Kempe, Kleinberg, and Tardos (2003), where a designer selects a\n(small) seed set of nodes in a social network to influence directly, this\ninfluence cascades when other nodes reach certain thresholds of neighbor\ninfluence, and the goal is to maximize the final number of influenced nodes.\nDespite extensive study from both practical and theoretical viewpoints, this\nmodel limits the designer to a binary choice for each node, with no way to\napply intermediate levels of influence. This model captures some settings\nprecisely, e.g. exposure to an idea or pathogen, but it fails to capture very\nrelevant concerns in others, for example, a manufacturer promoting a new\nproduct by distributing five ""20% off"" coupons instead of giving away one free\nproduct.\nWhile fractional versions of problems tend to be easier to solve than\nintegral versions, for influence maximization, we show that the two versions\nhave essentially the same computational complexity. On the other hand, the two\nversions can have vastly different solutions: the added flexibility of\nfractional allocation can lead to significantly improved influence. Our main\ntheoretical contribution is to show how to adapt the major positive results\nfrom the integral case to the fractional case. Specifically, Mossel and Roch\n(2006) used the submodularity of influence to obtain their integral results; we\nintroduce a new notion of continuous submodularity, and use this to obtain\nmatching fractional results. We conclude that we can achieve the same greedy\n$(1-1/e-\\epsilon)$-approximation for the fractional case as the integral case.\nIn practice, we find that the fractional model performs substantially better\nthan the integral model, according to simulations on real-world social network\ndata.'"
Erik Demaine,Demaine_Erik,arXiv:1310.4561,https://arxiv.org/abs/1310.4561,"b'Abstract:  We define a new class of orthogonal polyhedra, called orthogrids, that can be\nunfolded without overlap with constant refinement of the gridded surface.'"
Erik Demaine,Demaine_Erik,arXiv:1306.6710,https://arxiv.org/abs/1306.6710,"b'Abstract:  The well-studied Two-Handed Tile Assembly Model (2HAM) is a model of tile\nassembly in which pairs of large assemblies can bind, or self-assemble,\ntogether. In order to bind, two assemblies must have matching glues that can\nsimultaneously touch each other, and stick together with strength that is at\nleast the temperature $\\tau$, where $\\tau$ is some fixed positive integer. We\nask whether the 2HAM is intrinsically universal, in other words we ask: is\nthere a single universal 2HAM tile set $U$ which can be used to simulate any\ninstance of the model? Our main result is a negative answer to this question.\nWe show that for all $\\tau\' < \\tau$, each temperature-$\\tau\'$ 2HAM tile system\ndoes not simulate at least one temperature-$\\tau$ 2HAM tile system. This\nimpossibility result proves that the 2HAM is not intrinsically universal, in\nstark contrast to the simpler (single-tile addition only) abstract Tile\nAssembly Model which is intrinsically universal (""The tile assembly model is\nintrinsically universal"", FOCS 2012). However, on the positive side, we prove\nthat, for every fixed temperature $\\tau \\geq 2$, temperature-$\\tau$ 2HAM tile\nsystems are indeed intrinsically universal: in other words, for each $\\tau$\nthere is a single universal 2HAM tile set $U$ that, when appropriately\ninitialized, is capable of simulating the behavior of any temperature-$\\tau$\n2HAM tile system. As a corollary of these results we find an infinite set of\ninfinite hierarchies of 2HAM systems with strictly increasing simulation power\nwithin each hierarchy. Finally, we show that for each $\\tau$, there is a\ntemperature-$\\tau$ 2HAM system that simultaneously simulates all\ntemperature-$\\tau$ 2HAM systems.'"
Erik Demaine,Demaine_Erik,arXiv:1304.7604,https://arxiv.org/abs/1304.7604,"b'Abstract:  We present a general transformation for combining a constant number of binary\nsearch tree data structures (BSTs) into a single BST whose running time is\nwithin a constant factor of the minimum of any ""well-behaved"" bound on the\nrunning time of the given BSTs, for any online access sequence.\n(A BST has a well behaved bound with $f(n)$ overhead if it spends at most\n\\bigoh{f(n)} time per access and its bound satisfies a weak sense of closure\nunder subsequences.) In particular, we obtain a BST data structure that is\n\\bigoh{\\log\\log n} competitive, satisfies the working set bound (and thus\nsatisfies the static finger bound and the static optimality bound), satisfies\nthe dynamic finger bound, satisfies the unified bound with an additive\n\\bigoh{\\log\\log n} factor, and performs each access in worst-case \\bigoh{\\log\nn} time.'"
Erik Demaine,Demaine_Erik,arXiv:1212.4771,https://arxiv.org/abs/1212.4771,"b'Abstract:  We give subquadratic algorithms that, given two necklaces each with n beads\nat arbitrary positions, compute the optimal rotation of the necklaces to best\nalign the beads. Here alignment is measured according to the p norm of the\nvector of distances between pairs of beads from opposite necklaces in the best\nperfect matching. We show surprisingly different results for p = 1, p even, and\np = \\infty. For p even, we reduce the problem to standard convolution, while\nfor p = \\infty and p = 1, we reduce the problem to (min, +) convolution and\n(median, +) convolution. Then we solve the latter two convolution problems in\nsubquadratic time, which are interesting results in their own right. These\nresults shed some light on the classic sorting X + Y problem, because the\nconvolutions can be viewed as computing order statistics on the antidiagonals\nof the X + Y matrix. All of our algorithms run in o(n^2) time, whereas the\nobvious algorithms for these problems run in \\Theta(n^2) time.'"
Erik Demaine,Demaine_Erik,arXiv:1212.4756,https://arxiv.org/abs/1212.4756,"b'Abstract:  In this paper we explore the power of tile self-assembly models that extend\nthe well-studied abstract Tile Assembly Model (aTAM) by permitting tiles of\nshapes beyond unit squares. Our main result shows the surprising fact that any\naTAM system, consisting of many different tile types, can be simulated by a\nsingle tile type of a general shape. As a consequence, we obtain a single\nuniversal tile type of a single (constant-size) shape that serves as a\n""universal tile machine"": the single universal tile type can simulate any\ndesired aTAM system when given a single seed assembly that encodes the desired\naTAM system. We also show how to adapt this result to convert any of a variety\nof plane tiling systems (such as Wang tiles) into a ""nearly"" plane tiling\nsystem with a single tile (but with small gaps between the tiles). All of these\nresults rely on the ability to both rotate and translate tiles; by contrast, we\nshow that a single nonrotatable tile, of arbitrary shape, can produce\nassemblies which either grow infinitely or cannot grow at all, implying\ndrastically limited computational power.\nOn the positive side, we show how to simulate arbitrary cellular automata for\na limited number of steps using a single nonrotatable tile and a linear-size\nseed assembly.'"
Erik Demaine,Demaine_Erik,arXiv:1205.6960,https://arxiv.org/abs/1205.6960,"b'Abstract:  We study an extensive class of movement minimization problems which arise\nfrom many practical scenarios but so far have little theoretical study. In\ngeneral, these problems involve planning the coordinated motion of a collection\nof agents (representing robots, people, map labels, network messages, etc.) to\nachieve a global property in the network while minimizing the maximum or\naverage movement (expended energy). The only previous theoretical results about\nthis class of problems are about approximation, and mainly negative: many\nmovement problems of interest have polynomial inapproximability. Given that the\nnumber of mobile agents is typically much smaller than the complexity of the\nenvironment, we turn to fixed-parameter tractability. We characterize the\nboundary between tractable and intractable movement problems in a very general\nset up: it turns out the complexity of the problem fundamentally depends on the\ntreewidth of the minimal configurations. Thus the complexity of a particular\nproblem can be determined by answering a purely combinatorial question. Using\nour general tools, we determine the complexity of several concrete problems and\nfortunately show that many movement problems of interest can be solved\nefficiently.'"
Erik Demaine,Demaine_Erik,arXiv:1203.3602,https://arxiv.org/abs/1203.3602,"b'Abstract:  We show how to hang a picture by wrapping rope around n nails, making a\npolynomial number of twists, such that the picture falls whenever any k out of\nthe n nails get removed, and the picture remains hanging when fewer than k\nnails get removed. This construction makes for some fun mathematical magic\nperformances. More generally, we characterize the possible Boolean functions\ncharacterizing when the picture falls in terms of which nails get removed as\nall monotone Boolean functions. This construction requires an exponential\nnumber of twists in the worst case, but exponential complexity is almost always\nnecessary for general functions.'"
Erik Demaine,Demaine_Erik,arXiv:1203.1895,https://arxiv.org/abs/1203.1895,"b""Abstract:  We prove NP-hardness results for five of Nintendo's largest video game\nfranchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokemon. Our\nresults apply to generalized versions of Super Mario Bros. 1-3, The Lost\nLevels, and Super Mario World; Donkey Kong Country 1-3; all Legend of Zelda\ngames; all Metroid games; and all Pokemon role-playing games. In addition, we\nprove PSPACE-completeness of the Donkey Kong Country games and several Legend\nof Zelda games."""
Erik Demaine,Demaine_Erik,arXiv:1201.1650,https://arxiv.org/abs/1201.1650,"b'Abstract:  We study the difference between the standard seeded model of tile\nself-assembly, and the ""seedless"" two-handed model of tile self-assembly. Most\nof our results suggest that the two-handed model is more powerful. In\nparticular, we show how to simulate any seeded system with a two-handed system\nthat is essentially just a constant factor larger. We exhibit finite shapes\nwith a busy-beaver separation in the number of distinct tiles required by\nseeded versus two-handed, and exhibit an infinite shape that can be constructed\ntwo-handed but not seeded. Finally, we show that verifying whether a given\nsystem uniquely assembles a desired supertile is co-NP-complete in the\ntwo-handed model, while it was known to be polynomially solvable in the seeded\nmodel.'"
Erik Demaine,Demaine_Erik,arXiv:1112.4791,https://arxiv.org/abs/1112.4791,"b'Abstract:  We show that every orthogonal polyhedron homeomorphic to a sphere can be\nunfolded without overlap while using only polynomially many (orthogonal) cuts.\nBy contrast, the best previous such result used exponentially many cuts. More\nprecisely, given an orthogonal polyhedron with n vertices, the algorithm cuts\nthe polyhedron only where it is met by the grid of coordinate planes passing\nthrough the vertices, together with Theta(n^2) additional coordinate planes\nbetween every two such grid planes.'"
Erik Demaine,Demaine_Erik,arXiv:1106.5736,https://arxiv.org/abs/1106.5736,"b'Abstract:  The Rubik\'s Cube is perhaps the world\'s most famous and iconic puzzle,\nwell-known to have a rich underlying mathematical structure (group theory). In\nthis paper, we show that the Rubik\'s Cube also has a rich underlying\nalgorithmic structure. Specifically, we show that the n x n x n Rubik\'s Cube,\nas well as the n x n x 1 variant, has a ""God\'s Number"" (diameter of the\nconfiguration space) of Theta(n^2/log n). The upper bound comes from\neffectively parallelizing standard Theta(n^2) solution algorithms, while the\nlower bound follows from a counting argument. The upper bound gives an\nasymptotically optimal algorithm for solving a general Rubik\'s Cube in the\nworst case. Given a specific starting state, we show how to find the shortest\nsolution in an n x O(1) x O(1) Rubik\'s Cube. Finally, we show that finding this\noptimal solution becomes NP-hard in an n x n x 1 Rubik\'s Cube when the\npositions and colors of some of the cubies are ignored (not used in determining\nwhether the cube is solved).'"
Erik Demaine,Demaine_Erik,arXiv:1103.4513,https://arxiv.org/abs/1103.4513,"b'Abstract:  The separating words problem asks for the size of the smallest DFA needed to\ndistinguish between two words of length <= n (by accepting one and rejecting\nthe other). In this paper we survey what is known and unknown about the\nproblem, consider some variations, and prove several new results.'"
Erik Demaine,Demaine_Erik,arXiv:1009.5628,https://arxiv.org/abs/1009.5628,"b'Abstract:  An n-town, for a natural number n, is a group of n buildings, each occupying\na distinct position on a 2-dimensional integer grid. If we measure the distance\nbetween two buildings along the axis-parallel street grid, then an n-town has\noptimal shape if the sum of all pairwise Manhattan distances is minimized. This\nproblem has been studied for cities, i.e., the limiting case of very large n.\nFor cities, it is known that the optimal shape can be described by a\ndifferential equation, for which no closed-form is known. We show that optimal\nn-towns can be computed in O(n^7.5) time. This is also practically useful, as\nit allows us to compute optimal solutions up to n=80.'"
Erik Demaine,Demaine_Erik,arXiv:1008.1224,https://arxiv.org/abs/1008.1224,"b'Abstract:  We show that deciding whether a given set of circles can be packed into a\nrectangle, an equilateral triangle, or a unit square are NP-hard problems,\nsettling the complexity of these natural packing problems. On the positive\nside, we show that any set of circles of total area 1 can be packed into a\nsquare of size 4/\\sqrt{pi}=2.2567... These results are motivated by problems\narising in the context of origami design.'"
Erik Demaine,Demaine_Erik,arXiv:1007.3607,https://arxiv.org/abs/1007.3607,"b'Abstract:  We introduce a notion of $k$-convexity and explore polygons in the plane that\nhave this property. Polygons which are \\mbox{$k$-convex} can be triangulated\nwith fast yet simple algorithms. However, recognizing them in general is a\n3SUM-hard problem. We give a characterization of \\mbox{$2$-convex} polygons, a\nparticularly interesting class, and show how to recognize them in \\mbox{$O(n\n\\log n)$} time. A description of their shape is given as well, which leads to\nErd\xc5\x91s-Szekeres type results regarding subconfigurations of their vertex\nsets. Finally, we introduce the concept of generalized geometric permutations,\nand show that their number can be exponential in the number of\n\\mbox{$2$-convex} objects considered.'"
Erik Demaine,Demaine_Erik,arXiv:1004.4383,https://arxiv.org/abs/1004.4383,"b""Abstract:  We consider a model of algorithmic self-assembly of geometric shapes out of\nsquare Wang tiles studied in SODA 2010, in which there are two types of tiles\n(e.g., constructed out of DNA and RNA material) and one operation that destroys\nall tiles of a particular type (e.g., an RNAse enzyme destroys all RNA tiles).\nWe show that a single use of this destruction operation enables much more\nefficient construction of arbitrary shapes. In particular, an arbitrary shape\ncan be constructed using an asymptotically optimal number of distinct tile\ntypes (related to the shape's Kolmogorov complexity), after scaling the shape\nby only a logarithmic factor. By contrast, without the destruction operation,\nthe best such result has a scale factor at least linear in the size of the\nshape, and is connected only by a spanning tree of the scaled tiles. We also\ncharacterize a large collection of shapes that can be constructed efficiently\nwithout any scaling."""
Erik Demaine,Demaine_Erik,arXiv:1003.2851,https://arxiv.org/abs/1003.2851,"b'Abstract:  This paper investigates the popular card game UNO from the viewpoint of\nalgorithmic combinatorial game theory. We define simple and concise\nmathematical models for the game, including both cooperative and uncooperative\nversions, and analyze their computational complexity. In particular, we prove\nthat even a single-player version of UNO is NP-complete, although some\nrestricted cases are in P. Surprisingly, we show that the uncooperative\ntwo-player version is also in P.'"
Erik Demaine,Demaine_Erik,arXiv:0910.1643,https://arxiv.org/abs/0910.1643,"b'Abstract:  For a set of n points in the plane, we consider the axis--aligned (p,k)-Box\nCovering problem: Find p axis-aligned, pairwise-disjoint boxes that together\ncontain n-k points. In this paper, we consider the boxes to be either squares\nor rectangles, and we want to minimize the area of the largest box. For general\np we show that the problem is NP-hard for both squares and rectangles. For a\nsmall, fixed number p, we give algorithms that find the solution in the\nfollowing running times:\nFor squares we have O(n+k log k) time for p=1, and O(n log n+k^p log^p k time\nfor p = 2,3. For rectangles we get O(n + k^3) for p = 1 and O(n log n+k^{2+p}\nlog^{p-1} k) time for p = 2,3.\nIn all cases, our algorithms use O(n) space.'"
Erik Demaine,Demaine_Erik,arXiv:0909.5388,https://arxiv.org/abs/0909.5388,"b'Abstract:  We present a universal crease pattern--known in geometry as the tetrakis\ntiling and in origami as box pleating--that can fold into any object made up of\nunit cubes joined face-to-face (polycubes). More precisely, there is one\nuniversal finite crease pattern for each number n of unit cubes that need to be\nfolded. This result contrasts previous universality results for origami, which\nrequire a different crease pattern for each target object, and confirms\nintuition in the origami community that box pleating is a powerful design\ntechnique.'"
Erik Demaine,Demaine_Erik,arXiv:0909.3221,https://arxiv.org/abs/0909.3221,"b""Abstract:  The Stackelberg Minimum Spanning Tree Game is a two-level combinatorial\npricing problem played on a graph representing a network. Its edges are colored\neither red or blue, and the red edges have a given fixed cost, representing the\ncompetitor's prices. The first player chooses an assignment of prices to the\nblue edges, and the second player then buys the cheapest spanning tree, using\nany combination of red and blue edges. The goal of the first player is to\nmaximize the total price of purchased blue edges.\nWe study this problem in the cases of planar and bounded-treewidth graphs. We\nshow that the problem is NP-hard on planar graphs but can be solved in\npolynomial time on graphs of bounded treewidth."""
Erik Demaine,Demaine_Erik,arXiv:0908.2493,https://arxiv.org/abs/0908.2493,"b'Abstract:  The minimum feature size of a crossing-free straight line drawing is the\nminimum distance between a vertex and a non-incident edge. This quantity\nmeasures the resolution needed to display a figure or the tool size needed to\nmill the figure. The spread is the ratio of the diameter to the minimum feature\nsize. While many algorithms (particularly in meshing) depend on the spread of\nthe input, none explicitly consider finding a mesh whose spread is similar to\nthe input. When a polygon is partitioned into smaller regions, such as\ntriangles or quadrangles, the degradation is the ratio of original to final\nspread (the final spread is always greater).\nHere we present an algorithm to quadrangulate a simple n-gon, while achieving\nconstant degradation. Note that although all faces have a quadrangular shape,\nthe number of edges bounding each face may be larger. This method uses Theta(n)\nSteiner points and produces Theta(n) quadrangles. In fact to obtain constant\ndegradation, Omega(n) Steiner points are required by any algorithm.\nWe also show that, for some polygons, a constant factor cannot be achieved by\nany triangulation, even with an unbounded number of Steiner points. The\nspecific lower bounds depend on whether Steiner vertices are used or not.'"
Erik Demaine,Demaine_Erik,arXiv:0908.2440,https://arxiv.org/abs/0908.2440,"b'Abstract:  We consider the theoretical model of Crystalline robots, which have been\nintroduced and prototyped by the robotics community. These robots consist of\nindependently manipulable unit-square atoms that can extend/contract arms on\neach side and attach/detach from neighbors. These operations suffice to\nreconfigure between any two given (connected) shapes. The worst-case number of\nsequential moves required to transform one connected configuration to another\nis known to be Theta(n). However, in principle, atoms can all move\nsimultaneously. We develop a parallel algorithm for reconfiguration that runs\nin only O(log n) parallel steps, although the total number of operations\nincreases slightly to Theta(nlogn). The result is the first (theoretically)\nalmost-instantaneous universally reconfigurable robot built from simple units.'"
Erik Demaine,Demaine_Erik,arXiv:0906.4747,https://arxiv.org/abs/0906.4747,"b'Abstract:  We prove that the pleated hyperbolic paraboloid, a familiar origami model\nknown since 1927, in fact cannot be folded with the standard crease pattern in\nthe standard mathematical model of zero-thickness paper. In contrast, we show\nthat the model can be folded with additional creases, suggesting that real\npaper ""folds"" into this model via small such creases. We conjecture that the\ncircular version of this model, consisting simply of concentric circular\ncreases, also folds without extra creases.\nAt the heart of our results is a new structural theorem characterizing\nuncreased intrinsically flat surfaces--the portions of paper between the\ncreases. Differential geometry has much to say about the local behavior of such\nsurfaces when they are sufficiently smooth, e.g., that they are torsal ruled.\nBut this classic result is simply false in the context of the whole surface.\nOur structural characterization tells the whole story, and even applies to\nsurfaces with discontinuities in the second derivative. We use our theorem to\nprove fundamental properties about how paper folds, for example, that straight\ncreases on the piece of paper must remain piecewise-straight (polygonal) by\nfolding.'"
Erik Demaine,Demaine_Erik,arXiv:0906.2461,https://arxiv.org/abs/0906.2461,"b'Abstract:  We construct the first two continuous bloomings of all convex polyhedra.\nFirst, the source unfolding can be continuously bloomed. Second, any unfolding\nof a convex polyhedron can be refined (further cut, by a linear number of cuts)\nto have a continuous blooming.'"
Erik Demaine,Demaine_Erik,arXiv:0902.1400,https://arxiv.org/abs/0902.1400,"b'Abstract:  In general, the games are played on a host graph, where each node is a\nselfish independent agent (player) and each edge has a fixed link creation cost\n\\alpha. Together the agents create a network (a subgraph of the host graph)\nwhile selfishly minimizing the link creation costs plus the sum of the\ndistances to all other players (usage cost). In this paper, we pursue two\nimportant facets of the network creation game. First, we study extensively a\nnatural version of the game, called the cooperative model, where nodes can\ncollaborate and share the cost of creating any edge in the host graph. We prove\nthe first nontrivial bounds in this model, establishing that the price of\nanarchy is polylogarithmic in n for all values of &#945; in complete host\ngraphs. This bound is the first result of this type for any version of the\nnetwork creation game; most previous general upper bounds are polynomial in n.\nInterestingly, we also show that equilibrium graphs have polylogarithmic\ndiameter for the most natural range of \\alpha (at most n polylg n). Second, we\nstudy the impact of the natural assumption that the host graph is a general\ngraph, not necessarily complete. This model is a simple example of nonuniform\ncreation costs among the edges (effectively allowing weights of \\alpha and\n\\infty). We prove the first assemblage of upper and lower bounds for this\ncontext, stablishing nontrivial tight bounds for many ranges of \\alpha, for\nboth the unilateral and cooperative versions of network creation. In\nparticular, we establish polynomial lower bounds for both versions and many\nranges of \\alpha, even for this simple nonuniform cost model, which sharply\ncontrasts the conjectured constant bounds for these games in complete (uniform)\ngraphs.'"
Erik Demaine,Demaine_Erik,arXiv:0902.1043,https://arxiv.org/abs/0902.1043,"b'Abstract:  We present the first polynomial-time approximation schemes (PTASes) for the\nfollowing subset-connectivity problems in edge-weighted graphs of bounded\ngenus: Steiner tree, low-connectivity survivable-network design, and subset\nTSP. The schemes run in O(n log n) time for graphs embedded on both orientable\nand non-orientable surfaces. This work generalizes the PTAS frameworks of\nBorradaile, Klein, and Mathieu from planar graphs to bounded-genus graphs: any\nfuture problems shown to admit the required structure theorem for planar graphs\nwill similarly extend to bounded-genus graphs.'"
Erik Demaine,Demaine_Erik,arXiv:0901.1322,https://arxiv.org/abs/0901.1322,"b""Abstract:  The Carpenter's Rule Theorem states that any chain linkage in the plane can\nbe folded continuously between any two configurations while preserving the bar\nlengths and without the bars crossing. However, this theorem applies only to\nstrictly simple configurations, where bars intersect only at their common\nendpoints. We generalize the theorem to self-touching configurations, where\nbars can touch but not properly cross. At the heart of our proof is a new\ndefinition of self-touching configurations of planar linkages, based on an\nannotated configuration space and limits of nontouching configurations. We show\nthat this definition is equivalent to the previously proposed definition of\nself-touching configurations, which is based on a combinatorial description of\noverlapping features. Using our new definition, we prove the generalized\nCarpenter's Rule Theorem using a topological argument. We believe that our\ntopological methodology provides a powerful tool for manipulating many kinds of\nself-touching objects, such as 3D hinged assemblies of polygons and rigid\norigami. In particular, we show how to apply our methodology to extend to\nself-touching configurations universal reconfigurability results for open\nchains with slender polygonal adornments, and single-vertex rigid origami with\nconvex cones."""
Erik Demaine,Demaine_Erik,arXiv:0812.5030,https://arxiv.org/abs/0812.5030,"b""Abstract:  Alexandrov's Theorem states that every metric with the global topology and\nlocal geometry required of a convex polyhedron is in fact the intrinsic metric\nof a unique convex polyhedron. Recent work by Bobenko and Izmestiev describes a\ndifferential equation whose solution leads to the polyhedron corresponding to a\ngiven metric. We describe an algorithm based on this differential equation to\ncompute the polyhedron to arbitrary precision given the metric, and prove a\npseudopolynomial bound on its running time. Along the way, we develop\npseudopolynomial algorithms for computing shortest paths and weighted Delaunay\ntriangulations on a polyhedral surface, even when the surface edges are not\nshortest paths."""
Erik Demaine,Demaine_Erik,arXiv:0804.0986,https://arxiv.org/abs/0804.0986,"b""Abstract:  We propose a variant of Cauchy's Lemma, proving that when a convex chain on\none sphere is redrawn (with the same lengths and angles) on a larger sphere,\nthe distance between its endpoints increases. The main focus of this work is a\ncomparison of three alternate proofs, to show the links between Toponogov's\nComparison Theorem, Legendre's Theorem and Cauchy's Arm Lemma."""
Erik Demaine,Demaine_Erik,arXiv:0803.0316,https://arxiv.org/abs/0803.0316,"b'Abstract:  We introduce staged self-assembly of Wang tiles, where tiles can be added\ndynamically in sequence and where intermediate constructions can be stored for\nlater mixing. This model and its various constraints and performance measures\nare motivated by a practical nanofabrication scenario through protein-based\nbioengineering. Staging allows us to break through the traditional lower bounds\nin tile self-assembly by encoding the shape in the staging algorithm instead of\nthe tiles. All of our results are based on the practical assumption that only a\nconstant number of glues, and thus only a constant number of tiles, can be\nengineered, as each new glue type requires significant biochemical research and\nexperiments. Under this assumption, traditional tile self-assembly cannot even\nmanufacture an n*n square; in contrast, we show how staged assembly enables\nmanufacture of arbitrary orthogonal shapes in a variety of precise formulations\nof the model.'"
Erik Demaine,Demaine_Erik,arXiv:0801.4405,https://arxiv.org/abs/0801.4405,"b'Abstract:  We give a counterexample to a conjecture of Poon [Poo06] that any orthogonal\ntree in two dimensions can always be flattened by a continuous motion that\npreserves edge lengths and avoids self-intersection. We show our example is\nlocked by extending results on strongly locked self-touching linkages due to\nConnelly, Demaine and Rote [CDR02] to allow zero-length edges as defined in\n[ADG07], which may be of independent interest. Our results also yield a locked\ntree with only eleven edges, which is the smallest known example of a locked\ntree.'"
Erik Demaine,Demaine_Erik,arXiv:0712.2094,https://arxiv.org/abs/0712.2094,"b""Abstract:  We prove that any finite collection of polygons of equal area has a common\nhinged dissection. That is, for any such collection of polygons there exists a\nchain of polygons hinged at vertices that can be folded in the plane\ncontinuously without self-intersection to form any polygon in the collection.\nThis result settles the open problem about the existence of hinged dissections\nbetween pairs of polygons that goes back implicitly to 1864 and has been\nstudied extensively in the past ten years. Our result generalizes and indeed\nbuilds upon the result from 1814 that polygons have common dissections (without\nhinges). We also extend our common dissection result to edge-hinged dissections\nof solid 3D polyhedra that have a common (unhinged) dissection, as determined\nby Dehn's 1900 solution to Hilbert's Third Problem. Our proofs are\nconstructive, giving explicit algorithms in all cases. For a constant number of\nplanar polygons, both the number of pieces and running time required by our\nconstruction are pseudopolynomial. This bound is the best possible, even for\nunhinged dissections. Hinged dissections have possible applications to\nreconfigurable robotics, programmable matter, and nanomanufacturing."""
Erik Demaine,Demaine_Erik,arXiv:0711.2605,https://arxiv.org/abs/0711.2605,"b'Abstract:  A convex surface that is flat everywhere but on finitely many smooth curves\n(or ""seams"") and points is a seam form. We show that the only creases through\nthe flat components of a seam form are either between vertices or tangent to\nthe seams. As corollaries we resolve open problems about certain special seam\nforms: the flat components of a D-form have no creases at all, and the flat\ncomponent of a pita-form has at most one crease, between the seam\'s endpoints.'"
Erik Demaine,Demaine_Erik,arXiv:0705.4085,https://arxiv.org/abs/0705.4085,"b'Abstract:  We demonstrate relationships between the classic Euclidean algorithm and many\nother fields of study, particularly in the context of music and distance\ngeometry. Specifically, we show how the structure of the Euclidean algorithm\ndefines a family of rhythms which encompass over forty timelines\n(\\emph{ostinatos}) from traditional world music. We prove that these\n\\emph{Euclidean rhythms} have the mathematical property that their onset\npatterns are distributed as evenly as possible: they maximize the sum of the\nEuclidean distances between all pairs of onsets, viewing onsets as points on a\ncircle. Indeed, Euclidean rhythms are the unique rhythms that maximize this\nnotion of \\emph{evenness}. We also show that essentially all Euclidean rhythms\nare \\emph{deep}: each distinct distance between onsets occurs with a unique\nmultiplicity, and these multiplicies form an interval $1,2,...,k-1$. Finally,\nwe characterize all deep rhythms, showing that they form a subclass of\ngenerated rhythms, which in turn proves a useful property called shelling. All\nof our results for musical rhythms apply equally well to musical scales. In\naddition, many of the problems we explore are interesting in their own right as\ndistance geometry problems on the circle; some of the same problems were\nexplored by Erd\xc5\x91s in the plane.'"
Erik Demaine,Demaine_Erik,arXiv:cs/0703019,https://arxiv.org/abs/cs/0703019,"b""Abstract:  We consider a one-round two-player network pricing game, the Stackelberg\nMinimum Spanning Tree game or StackMST.\nThe game is played on a graph (representing a network), whose edges are\ncolored either red or blue, and where the red edges have a given fixed cost\n(representing the competitor's prices). The first player chooses an assignment\nof prices to the blue edges, and the second player then buys the cheapest\npossible minimum spanning tree, using any combination of red and blue edges.\nThe goal of the first player is to maximize the total price of purchased blue\nedges. This game is the minimum spanning tree analog of the well-studied\nStackelberg shortest-path game.\nWe analyze the complexity and approximability of the first player's best\nstrategy in StackMST. In particular, we prove that the problem is APX-hard even\nif there are only two different red costs, and give an approximation algorithm\nwhose approximation ratio is at most $\\min \\{k,1+\\ln b,1+\\ln W\\}$, where $k$ is\nthe number of distinct red costs, $b$ is the number of blue edges, and $W$ is\nthe maximum ratio between red costs. We also give a natural integer linear\nprogramming formulation of the problem, and show that the integrality gap of\nthe fractional relaxation asymptotically matches the approximation guarantee of\nour algorithm."""
Erik Demaine,Demaine_Erik,arXiv:cs/0604037,https://arxiv.org/abs/cs/0604037,"b""Abstract:  The {\\em edit distance} between two ordered trees with vertex labels is the\nminimum cost of transforming one tree into the other by a sequence of\nelementary operations consisting of deleting and relabeling existing nodes, as\nwell as inserting new nodes. In this paper, we present a worst-case\n$O(n^3)$-time algorithm for this problem, improving the previous best\n$O(n^3\\log n)$-time algorithm~\\cite{Klein}. Our result requires a novel\nadaptive strategy for deciding how a dynamic program divides into subproblems\n(which is interesting in its own right), together with a deeper understanding\nof the previous algorithms for the problem. We also prove the optimality of our\nalgorithm among the family of \\emph{decomposition strategy} algorithms--which\nalso includes the previous fastest algorithms--by tightening the known lower\nbound of $\\Omega(n^2\\log^2 n)$~\\cite{Touzet} to $\\Omega(n^3)$, matching our\nalgorithm's running time. Furthermore, we obtain matching upper and lower\nbounds of $\\Theta(n m^2 (1 + \\log \\frac{n}{m}))$ when the two trees have\ndifferent sizes $m$ and~$n$, where $m < n$."""
Erik Demaine,Demaine_Erik,arXiv:cs/0604022,https://arxiv.org/abs/cs/0604022,"b'Abstract:  We extend linkage unfolding results from the well-studied case of polygonal\nlinkages to the more general case of linkages of polygons. More precisely, we\nconsider chains of nonoverlapping rigid planar shapes (Jordan regions) that are\nhinged together sequentially at rotatable joints. Our goal is to characterize\nthe families of planar shapes that admit locked chains, where some\nconfigurations cannot be reached by continuous reconfiguration without\nself-intersection, and which families of planar shapes guarantee universal\nfoldability, where every chain is guaranteed to have a connected configuration\nspace. Previously, only obtuse triangles were known to admit locked shapes, and\nonly line segments were known to guarantee universal foldability. We show that\na surprisingly general family of planar shapes, called slender adornments,\nguarantees universal foldability: roughly, the distance from each edge along\nthe path along the boundary of the slender adornment to each hinge should be\nmonotone. In contrast, we show that isosceles triangles with any desired apex\nangle less than 90 degrees admit locked chains, which is precisely the\nthreshold beyond which the inward-normal property no longer holds.'"
Erik Demaine,Demaine_Erik,arXiv:cs/0512091,https://arxiv.org/abs/cs/0512091,"b'Abstract:  We consider preprocessing a set $S$ of $n$ points in convex position in the\nplane into a data structure supporting queries of the following form: given a\npoint $q$ and a directed line $\\ell$ in the plane, report the point of $S$ that\nis farthest from (or, alternatively, nearest to) the point $q$ among all points\nto the left of line $\\ell$. We present two data structures for this problem.\nThe first data structure uses $O(n^{1+\\varepsilon})$ space and preprocessing\ntime, and answers queries in $O(2^{1/\\varepsilon} \\log n)$ time, for any $0 <\n\\varepsilon < 1$. The second data structure uses $O(n \\log^3 n)$ space and\npolynomial preprocessing time, and answers queries in $O(\\log n)$ time. These\nare the first solutions to the problem with $O(\\log n)$ query time and $o(n^2)$\nspace.\nThe second data structure uses a new representation of nearest- and\nfarthest-point Voronoi diagrams of points in convex position. This\nrepresentation supports the insertion of new points in clockwise order using\nonly $O(\\log n)$ amortized pointer changes, in addition to $O(\\log n)$-time\npoint-location queries, even though every such update may make $\\Theta(n)$\ncombinatorial changes to the Voronoi diagram. This data structure is the first\ndemonstration that deterministically and incrementally constructed Voronoi\ndiagrams can be maintained in $o(n)$ amortized pointer changes per operation\nwhile keeping $O(\\log n)$-time point-location queries.'"
Erik Demaine,Demaine_Erik,arXiv:cs/0512081,https://arxiv.org/abs/cs/0512081,"b'Abstract:  We develop dynamic dictionaries on the word RAM that use asymptotically\noptimal space, up to constant factors, subject to insertions and deletions, and\nsubject to supporting perfect-hashing queries and/or membership queries, each\noperation in constant time with high probability. When supporting only\nmembership queries, we attain the optimal space bound of Theta(n lg(u/n)) bits,\nwhere n and u are the sizes of the dictionary and the universe, respectively.\nPrevious dictionaries either did not achieve this space bound or had time\nbounds that were only expected and amortized. When supporting perfect-hashing\nqueries, the optimal space bound depends on the range {1,2,...,n+t} of\nhashcodes allowed as output. We prove that the optimal space bound is Theta(n\nlglg(u/n) + n lg(n/(t+1))) bits when supporting only perfect-hashing queries,\nand it is Theta(n lg(u/n) + n lg(n/(t+1))) bits when also supporting membership\nqueries. All upper bounds are new, as is the Omega(n lg(n/(t+1))) lower bound.'"
Erik Demaine,Demaine_Erik,arXiv:cs/0502070,https://arxiv.org/abs/cs/0502070,"b'Abstract:  In this paper we extend the theory of bidimensionality to two families of\ngraphs that do not exclude fixed minors: map graphs and power graphs. In both\ncases we prove a polynomial relation between the treewidth of a graph in the\nfamily and the size of the largest grid minor. These bounds improve the running\ntimes of a broad class of fixed-parameter algorithms. Our novel technique of\nusing approximate max-min relations between treewidth and size of grid minors\nis powerful, and we show how it can also be used, e.g., to prove a linear\nrelation between the treewidth of a bounded-genus graph and the treewidth of\nits dual.'"
Erik Demaine,Demaine_Erik,arXiv:cs/0502041,https://arxiv.org/abs/cs/0502041,"b""Abstract:  We develop a new technique for proving cell-probe lower bounds on dynamic\ndata structures. This technique enables us to prove an amortized randomized\nOmega(lg n) lower bound per operation for several data structural problems on n\nelements, including partial sums, dynamic connectivity among disjoint paths (or\na forest or a graph), and several other dynamic graph problems (by simple\nreductions). Such a lower bound breaks a long-standing barrier of Omega(lg n /\nlglg n) for any dynamic language membership problem. It also establishes the\noptimality of several existing data structures, such as Sleator and Tarjan's\ndynamic trees. We also prove the first Omega(log_B n) lower bound in the\nexternal-memory model without assumptions on the data structure (such as the\ncomparison model). Our lower bounds also give a query-update trade-off curve\nmatched, e.g., by several data structures for dynamic connectivity in graphs.\nWe also prove matching upper and lower bounds for partial sums when\nparameterized by the word size and the maximum additive change in an update."""
Erik Demaine,Demaine_Erik,arXiv:cs/0410048,https://arxiv.org/abs/cs/0410048,"b'Abstract:  Consider laying out a fixed-topology tree of N nodes into external memory\nwith block size B so as to minimize the worst-case number of block memory\ntransfers required to traverse a path from the root to a node of depth D. We\nprove that the optimal number of memory transfers is $$ \\cases{\n\\displaystyle\n\\Theta\\left( {D \\over \\lg (1{+}B)} \\right)\n& when $D = O(\\lg N)$, \\cr\n\\displaystyle\n\\Theta\\left( {\\lg N \\over \\lg \\left(1{+}{B \\lg N \\over D}\\right)} \\right)\n& when $D = \\Omega(\\lg N)$ and $D = O(B \\lg N)$, \\cr\n\\displaystyle\n\\Theta\\left( {D \\over B} \\right)\n& when $D = \\Omega(B \\lg N)$.\n} $$'"
Srini Devadas,Devadas_Srini,arXiv:1902.06101,https://arxiv.org/abs/1902.06101,"b'Abstract:  Privacy concerns with sensitive data in machine learning are receiving\nincreasing attention. In this paper, we study privacy-preserving distributed\nlearning under the framework of Alternating Direction Method of Multipliers\n(ADMM). While secure distributed learning has been previously exploited in\ncryptographic or non-cryptographic (noise perturbation) approaches, it comes at\na cost of either prohibitive computation overhead or a heavy loss of accuracy.\nMoreover, convergence in noise perturbation is hardly explored in existing\nprivacy-preserving ADMM schemes. In this work, we propose two modified private\nADMM schemes in the scenario of peer-to-peer semi-honest agents: First, for\nbounded colluding agents, we show that with merely linear secret sharing,\ninformation-theoretically private distributed optimization can be achieved.\nSecond, using the notion of differential privacy, we propose first-order\napproximation based ADMM schemes with random parameters. We prove that the\nproposed private ADMM schemes can be implemented with a linear convergence rate\nand with a sharpened privacy loss bound in relation to prior work. Finally, we\nprovide experimental results to support the theory.'"
Fredo Durand,Durand_Fredo,arXiv:1902.09383,https://arxiv.org/abs/1902.09383,"b'Abstract:  Biomedical image segmentation is an important task in many medical\napplications. Segmentation methods based on convolutional neural networks\nattain state-of-the-art accuracy; however, they typically rely on supervised\ntraining with large labeled datasets. Labeling datasets of medical images\nrequires significant expertise and time, and is infeasible at large scales. To\ntackle the lack of labeled data, researchers use techniques such as\nhand-engineered preprocessing steps, hand-tuned architectures, and data\naugmentation. However, these techniques involve costly engineering efforts, and\nare typically dataset-specific.\nWe present an automated data augmentation method for medical images. We\ndemonstrate our method on the task of segmenting magnetic resonance imaging\n(MRI) brain scans, focusing on the one-shot segmentation scenario -- a\npractical challenge in many medical applications. Our method requires only a\nsingle segmented scan, and leverages other unlabeled scans in a semi-supervised\napproach. We learn a model of transforms from the images, and use the model\nalong with the labeled example to synthesize additional labeled training\nexamples for supervised segmentation. Each transform is comprised of a spatial\ndeformation field and an intensity change, enabling the synthesis of complex\neffects such as variations in anatomy and image acquisition procedures.\nAugmenting the training of a supervised segmenter with these new examples\nprovides significant improvements over state-of-the-art methods for one-shot\nbiomedical image segmentation. Our code is available at\nthis https URL.'"
Fredo Durand,Durand_Fredo,arXiv:1810.09718,https://arxiv.org/abs/1810.09718,"b'Abstract:  Texture, highlights, and shading are some of many visual cues that allow\nhumans to perceive material appearance in single pictures. Yet, recovering\nspatially-varying bi-directional reflectance distribution functions (SVBRDFs)\nfrom a single image based on such cues has challenged researchers in computer\ngraphics for decades. We tackle lightweight appearance capture by training a\ndeep neural network to automatically extract and make sense of these visual\ncues. Once trained, our network is capable of recovering per-pixel normal,\ndiffuse albedo, specular albedo and specular roughness from a single picture of\na flat surface lit by a hand-held flash. We achieve this goal by introducing\nseveral innovations on training data acquisition and network design. For\ntraining, we leverage a large dataset of artist-created, procedural SVBRDFs\nwhich we sample and render under multiple lighting directions. We further\namplify the data by material mixing to cover a wide diversity of shading\neffects, which allows our network to work across many material classes.\nMotivated by the observation that distant regions of a material sample often\noffer complementary visual cues, we design a network that combines an\nencoder-decoder convolutional track for local feature extraction with a\nfully-connected track for global feature extraction and propagation. Many\nimportant material effects are view-dependent, and as such ambiguous when\nobserved in a single image. We tackle this challenge by defining the loss as a\ndifferentiable SVBRDF similarity metric that compares the renderings of the\npredicted maps against renderings of the ground truth from several lighting and\nviewing directions. Combined together, these novel ingredients bring clear\nimprovement over state of the art methods for single-shot capture of spatially\nvarying BRDFs.'"
Fredo Durand,Durand_Fredo,arXiv:1809.07851,https://arxiv.org/abs/1809.07851,"b'Abstract:  Winograd-based convolution has quickly gained traction as a preferred\napproach to implement convolutional neural networks (ConvNet) on various\nhardware platforms because it requires fewer floating point operations than\nFFT-based or direct convolutions.\nThis paper compares three highly optimized implementations (regular FFT--,\nGauss--FFT--, and Winograd--based convolutions) on modern multi-- and\nmany--core CPUs. Although all three implementations employed the same\noptimizations for modern CPUs, our experimental results with two popular\nConvNets (VGG and AlexNet) show that the FFT--based implementations generally\noutperform the Winograd--based approach, contrary to the popular belief.\nTo understand the results, we use a Roofline performance model to analyze the\nthree implementations in detail, by looking at each of their computation phases\nand by considering not only the number of floating point operations, but also\nthe memory bandwidth and the cache sizes. The performance analysis explains\nwhy, and under what conditions, the FFT--based implementations outperform the\nWinograd--based one, on modern CPUs.'"
Fredo Durand,Durand_Fredo,arXiv:1807.10441,https://arxiv.org/abs/1807.10441,"b""Abstract:  Widely used in news, business, and educational media, infographics are\nhandcrafted to effectively communicate messages about complex and often\nabstract topics including `ways to conserve the environment' and `understanding\nthe financial crisis'. Composed of stylistically and semantically diverse\nvisual and textual elements, infographics pose new challenges for computer\nvision. While automatic text extraction works well on infographics, computer\nvision approaches trained on natural images fail to identify the stand-alone\nvisual elements in infographics, or `icons'. To bridge this representation gap,\nwe propose a synthetic data generation strategy: we augment background patches\nin infographics from our Visually29K dataset with Internet-scraped icons which\nwe use as training data for an icon proposal mechanism. On a test set of 1K\nannotated infographics, icons are located with 38% precision and 34% recall\n(the best model trained with natural images achieves 14% precision and 7%\nrecall). Combining our icon proposals with icon classification and text\nextraction, we present a multi-modal summarization application. Our application\ntakes an infographic as input and automatically produces text tags and visual\nhashtags that are textually and visually representative of the infographic's\ntopics respectively."""
Fredo Durand,Durand_Fredo,arXiv:1804.07739,https://arxiv.org/abs/1804.07739,"b'Abstract:  We address the computational problem of novel human pose synthesis. Given an\nimage of a person and a desired pose, we produce a depiction of that person in\nthat pose, retaining the appearance of both the person and background. We\npresent a modular generative neural network that synthesizes unseen poses using\ntraining pairs of images and poses taken from human action videos. Our network\nseparates a scene into different body part and background layers, moves body\nparts to new locations and refines their appearances, and composites the new\nforeground with a hole-filled background. These subtasks, implemented with\nseparate modules, are trained jointly using only a single target image as a\nsupervised label. We use an adversarial discriminator to force our network to\nsynthesize realistic details conditioned on pose. We demonstrate image\nsynthesis results on three action classes: golf, yoga/workouts and tennis, and\nshow that our method produces accurate results within action classes as well as\nacross action classes. Given a sequence of desired poses, we also produce\ncoherent videos of actions.'"
Fredo Durand,Durand_Fredo,arXiv:1804.02684,https://arxiv.org/abs/1804.02684,"b'Abstract:  Video motion magnification techniques allow us to see small motions\npreviously invisible to the naked eyes, such as those of vibrating airplane\nwings, or swaying buildings under the influence of the wind. Because the motion\nis small, the magnification results are prone to noise or excessive blurring.\nThe state of the art relies on hand-designed filters to extract representations\nthat may not be optimal. In this paper, we seek to learn the filters directly\nfrom examples using deep convolutional neural networks. To make training\ntractable, we carefully design a synthetic dataset that captures small motion\nwell, and use two-frame input for training. We show that the learned filters\nachieve high-quality results on real videos, with less ringing artifacts and\nbetter noise characteristics than previous methods. While our model is not\ntrained with temporal filters, we found that the temporal filters can be used\nwith our extracted representations up to a moderate magnification, enabling a\nfrequency-based motion selection. Finally, we analyze the learned filters and\nshow that they behave similarly to the derivative filters used in previous\nworks. Our code, trained model, and datasets will be available online.'"
Fredo Durand,Durand_Fredo,arXiv:1709.09215,https://arxiv.org/abs/1709.09215,"b'Abstract:  We introduce the problem of visual hashtag discovery for infographics:\nextracting visual elements from an infographic that are diagnostic of its\ntopic. Given an infographic as input, our computational approach automatically\noutputs textual and visual elements predicted to be representative of the\ninfographic content. Concretely, from a curated dataset of 29K large\ninfographic images sampled across 26 categories and 391 tags, we present an\nautomated two step approach. First, we extract the text from an infographic and\nuse it to predict text tags indicative of the infographic content. And second,\nwe use these predicted text tags as a supervisory signal to localize the most\ndiagnostic visual elements from within the infographic i.e. visual hashtags. We\nreport performances on a categorization and multi-label tag prediction problem\nand compare our proposed visual hashtags to human annotations.'"
Fredo Durand,Durand_Fredo,arXiv:1708.02660,https://arxiv.org/abs/1708.02660,"b'Abstract:  Knowing where people look and click on visual designs can provide clues about\nhow the designs are perceived, and where the most important or relevant content\nlies. The most important content of a visual design can be used for effective\nsummarization or to facilitate retrieval from a database. We present automated\nmodels that predict the relative importance of different elements in data\nvisualizations and graphic designs. Our models are neural networks trained on\nhuman clicks and importance annotations on hundreds of designs. We collected a\nnew dataset of crowdsourced importance, and analyzed the predictions of our\nmodels with respect to ground truth importance and human eye movements. We\ndemonstrate how such predictions of importance can be used for automatic design\nretargeting and thumbnailing. User studies with hundreds of MTurk participants\nvalidate that, with limited post-processing, our importance-driven applications\nare on par with, or outperform, current state-of-the-art methods, including\nnatural image saliency. We also provide a demonstration of how our importance\npredictions can be built into interactive design tools to offer immediate\nfeedback during the design process.'"
Fredo Durand,Durand_Fredo,arXiv:1707.02880,https://arxiv.org/abs/1707.02880,"b'Abstract:  Performance is a critical challenge in mobile image processing. Given a\nreference imaging pipeline, or even human-adjusted pairs of images, we seek to\nreproduce the enhancements and enable real-time evaluation. For this, we\nintroduce a new neural network architecture inspired by bilateral grid\nprocessing and local affine color transforms. Using pairs of input/output\nimages, we train a convolutional neural network to predict the coefficients of\na locally-affine model in bilateral space. Our architecture learns to make\nlocal, global, and content-dependent decisions to approximate the desired image\ntransformation. At runtime, the neural network consumes a low-resolution\nversion of the input image, produces a set of affine transformations in\nbilateral space, upsamples those transformations in an edge-preserving fashion\nusing a new slicing node, and then applies those upsampled transformations to\nthe full-resolution image. Our algorithm processes high-resolution images on a\nsmartphone in milliseconds, provides a real-time viewfinder at 1080p\nresolution, and matches the quality of state-of-the-art approximation\ntechniques on a large class of image operators. Unlike previous work, our model\nis trained off-line from data and therefore does not require access to the\noriginal operator at runtime. This allows our model to learn complex,\nscene-dependent transformations for which no reference implementation is\navailable, such as the photographic edits of a human retoucher.'"
Fredo Durand,Durand_Fredo,arXiv:1702.05150,https://arxiv.org/abs/1702.05150,"b'Abstract:  In this paper, we present BubbleView, an alternative methodology for eye\ntracking using discrete mouse clicks to measure which information people\nconsciously choose to examine. BubbleView is a mouse-contingent, moving-window\ninterface in which participants are presented with a series of blurred images\nand click to reveal ""bubbles"" - small, circular areas of the image at original\nresolution, similar to having a confined area of focus like the eye fovea.\nAcross 10 experiments with 28 different parameter combinations, we evaluated\nBubbleView on a variety of image types: information visualizations, natural\nimages, static webpages, and graphic designs, and compared the clicks to eye\nfixations collected with eye-trackers in controlled lab settings. We found that\nBubbleView clicks can both (i) successfully approximate eye fixations on\ndifferent images, and (ii) be used to rank image and design elements by\nimportance. BubbleView is designed to collect clicks on static images, and\nworks best for defined tasks such as describing the content of an information\nvisualization or measuring image importance. BubbleView data is cleaner and\nmore consistent than related methodologies that use continuous mouse movements.\nOur analyses validate the use of mouse-contingent, moving-window methodologies\nas approximating eye fixations for different image and task types.'"
Fredo Durand,Durand_Fredo,arXiv:1612.04007,https://arxiv.org/abs/1612.04007,"b""Abstract:  For many movement disorders, such as Parkinson's disease and ataxia, disease\nprogression is visually assessed by a clinician using a numerical disease\nrating scale. These tests are subjective, time-consuming, and must be\nadministered by a professional. This can be problematic where specialists are\nnot available, or when a patient is not consistently evaluated by the same\nclinician. We present an automated method for quantifying the severity of\nmotion impairment in patients with ataxia, using only video recordings. We\nconsider videos of the finger-to-nose test, a common movement task used as part\nof the assessment of ataxia progression during the course of routine clinical\ncheckups.\nOur method uses neural network-based pose estimation and optical flow\ntechniques to track the motion of the patient's hand in a video recording. We\nextract features that describe qualities of the motion such as speed and\nvariation in performance. Using labels provided by an expert clinician, we\ntrain a supervised learning model that predicts severity according to the Brief\nAtaxia Rating Scale (BARS). The performance of our system is comparable to that\nof a group of ataxia specialists in terms of mean error and correlation, and\nour system's predictions were consistently within the range of inter-rater\nvariability. This work demonstrates the feasibility of using computer vision\nand machine learning to produce consistent and clinically useful measures of\nmotor impairment."""
Fredo Durand,Durand_Fredo,arXiv:1610.02769,https://arxiv.org/abs/1610.02769,"b'Abstract:  The inverse diffusion curve problem focuses on automatic creation of\ndiffusion curve images that resemble user provided color fields. This problem\nis challenging since the 1D curves have a nonlinear and global impact on\nresulting color fields via a partial differential equation (PDE). We introduce\na new approach complementary to previous methods by optimizing curve geometry.\nIn particular, we propose a novel iterative algorithm based on the theory of\nshape derivatives. The resulting diffusion curves are clean and well-shaped,\nand the final image closely approximates the input. Our method provides a\nuser-controlled parameter to regularize curve complexity, and generalizes to\nhandle input color fields represented in a variety of formats.'"
Fredo Durand,Durand_Fredo,arXiv:1604.03605,https://arxiv.org/abs/1604.03605,"b""Abstract:  How best to evaluate a saliency model's ability to predict where humans look\nin images is an open research question. The choice of evaluation metric depends\non how saliency is defined and how the ground truth is represented. Metrics\ndiffer in how they rank saliency models, and this results from how false\npositives and false negatives are treated, whether viewing biases are accounted\nfor, whether spatial deviations are factored in, and how the saliency maps are\npre-processed. In this paper, we provide an analysis of 8 different evaluation\nmetrics and their properties. With the help of systematic experiments and\nvisualizations of metric computations, we add interpretability to saliency\nscores and more transparency to the evaluation of saliency models. Building off\nthe differences in metric properties and behaviors, we make recommendations for\nmetric selections under specific assumptions and for specific applications."""
Joel Emer,Emer_Joel,arXiv:1807.07928,https://arxiv.org/abs/1807.07928,"b""Abstract:  The design of DNNs has increasingly focused on reducing the computational\ncomplexity in addition to improving accuracy. While emerging DNNs tend to have\nfewer weights and operations, they also reduce the amount of data reuse with\nmore widely varying layer shapes and sizes. This leads to a diverse set of\nDNNs, ranging from large ones with high reuse (e.g., AlexNet) to compact ones\nwith high bandwidth requirements (e.g., MobileNet). However, many existing DNN\nprocessors depend on certain DNN properties, e.g., a large number of channels,\nto achieve high performance and energy efficiency and do not have sufficient\nflexibility to efficiently process a diverse set of DNNs. In this work, we\npresent Eyexam, a performance analysis framework that quantitatively identifies\nthe sources of performance loss in DNN processors. It highlights two\narchitectural bottlenecks in many existing designs. First, their dataflows are\nnot flexible enough to adapt to the varying layer shapes and sizes of different\nDNNs. Second, their network-on-chip (NoC) can't adapt to support both high data\nreuse and high bandwidth scenarios. Based on this analysis, we present Eyeriss\nv2, a high-performance DNN accelerator that adapts to a wide range of DNNs.\nEyeriss v2 has a new dataflow, called Row-Stationary Plus (RS+), that enables\nthe spatial tiling of data from all dimensions to fully utilize the parallelism\nfor high performance. To support RS+, it has a low-cost and scalable NoC\ndesign, called hierarchical mesh, that connects the high-bandwidth global\nbuffer to the array of processing elements (PEs) in a two-level hierarchy. This\nenables high-bandwidth data delivery while still being able to harness any\navailable data reuse. Compared with Eyeriss, Eyeriss v2 has a performance\nincrease of 10.4x-17.9x for 256 PEs, 37.7x-71.5x for 1024 PEs, and\n448.8x-1086.7x for 16384 PEs on DNNs with widely varying amounts of data reuse."""
Joel Emer,Emer_Joel,arXiv:1708.04485,https://arxiv.org/abs/1708.04485,"b'Abstract:  Convolutional Neural Networks (CNNs) have emerged as a fundamental technology\nfor machine learning. High performance and extreme energy efficiency are\ncritical for deployments of CNNs in a wide range of situations, especially\nmobile platforms such as autonomous vehicles, cameras, and electronic personal\nassistants. This paper introduces the Sparse CNN (SCNN) accelerator\narchitecture, which improves performance and energy efficiency by exploiting\nthe zero-valued weights that stem from network pruning during training and\nzero-valued activations that arise from the common ReLU operator applied during\ninference. Specifically, SCNN employs a novel dataflow that enables maintaining\nthe sparse weights and activations in a compressed encoding, which eliminates\nunnecessary data transfers and reduces storage requirements. Furthermore, the\nSCNN dataflow facilitates efficient delivery of those weights and activations\nto the multiplier array, where they are extensively reused. In addition, the\naccumulation of multiplication products are performed in a novel accumulator\narray. Our results show that on contemporary neural networks, SCNN can improve\nboth performance and energy by a factor of 2.7x and 2.3x, respectively, over a\ncomparably provisioned dense CNN accelerator.'"
Joel Emer,Emer_Joel,arXiv:1703.09039,https://arxiv.org/abs/1703.09039,"b'Abstract:  Deep neural networks (DNNs) are currently widely used for many artificial\nintelligence (AI) applications including computer vision, speech recognition,\nand robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it\ncomes at the cost of high computational complexity. Accordingly, techniques\nthat enable efficient processing of DNNs to improve energy efficiency and\nthroughput without sacrificing application accuracy or increasing hardware cost\nare critical to the wide deployment of DNNs in AI systems.\nThis article aims to provide a comprehensive tutorial and survey about the\nrecent advances towards the goal of enabling efficient processing of DNNs.\nSpecifically, it will provide an overview of DNNs, discuss various hardware\nplatforms and architectures that support DNNs, and highlight key trends in\nreducing the computation cost of DNNs either solely via hardware design changes\nor via joint hardware design and DNN algorithm changes. It will also summarize\nvarious development resources that enable researchers and practitioners to\nquickly get started in this field, and highlight important benchmarking metrics\nand design considerations that should be used for evaluating the rapidly\ngrowing number of DNN hardware designs, optionally including algorithmic\nco-designs, being proposed in academia and industry.\nThe reader will take away the following concepts from this article:\nunderstand the key design considerations for DNNs; be able to evaluate\ndifferent DNN hardware implementations with benchmarks and comparison metrics;\nunderstand the trade-offs between various hardware architectures and platforms;\nbe able to evaluate the utility of various DNN design techniques for efficient\nprocessing; and understand recent implementation trends and opportunities.'"
Joel Emer,Emer_Joel,arXiv:1703.05853,https://arxiv.org/abs/1703.05853,"b'Abstract:  Computer vision enables a wide range of applications in robotics/drones,\nself-driving cars, smart Internet of Things, and portable/wearable electronics.\nFor many of these applications, local embedded processing is preferred due to\nprivacy and/or latency concerns. Accordingly, energy-efficient embedded vision\nhardware delivering real-time and robust performance is crucial. While deep\nlearning is gaining popularity in several computer vision algorithms, a\nsignificant energy consumption difference exists compared to traditional\nhand-crafted approaches. In this paper, we provide an in-depth analysis of the\ncomputation, energy and accuracy trade-offs between learned features such as\ndeep Convolutional Neural Networks (CNN) and hand-crafted features such as\nHistogram of Oriented Gradients (HOG). This analysis is supported by\nmeasurements from two chips that implement these algorithms. Our goal is to\nunderstand the source of the energy discrepancy between the two approaches and\nto provide insight about the potential areas where CNNs can be improved and\neventually approach the energy-efficiency of HOG while maintaining its\noutstanding performance accuracy.'"
Joel Emer,Emer_Joel,arXiv:1612.07625,https://arxiv.org/abs/1612.07625,"b'Abstract:  Machine learning plays a critical role in extracting meaningful information\nout of the zetabytes of sensor data collected every day. For some applications,\nthe goal is to analyze and understand the data to identify trends (e.g.,\nsurveillance, portable/wearable electronics); in other applications, the goal\nis to take immediate action based the data (e.g., robotics/drones, self-driving\ncars, smart Internet of Things). For many of these applications, local embedded\nprocessing near the sensor is preferred over the cloud due to privacy or\nlatency concerns, or limitations in the communication bandwidth. However, at\nthe sensor there are often stringent constraints on energy consumption and cost\nin addition to throughput and accuracy requirements. Furthermore, flexibility\nis often required such that the processing can be adapted for different\napplications or environments (e.g., update the weights and model in the\nclassifier). In many applications, machine learning often involves transforming\nthe input data into a higher dimensional space, which, along with programmable\nweights, increases data movement and consequently energy consumption. In this\npaper, we will discuss how these challenges can be addressed at various levels\nof hardware design ranging from architecture, hardware-friendly algorithms,\nmixed-signal circuits, and advanced technologies (including memories and\nsensors).'"
William Freeman,Freeman_William,arXiv:1903.05136,https://arxiv.org/abs/1903.05136,"b'Abstract:  Humans easily recognize object parts and their hierarchical structure by\nwatching how they move; they can then predict how each part moves in the\nfuture. In this paper, we propose a novel formulation that simultaneously\nlearns a hierarchical, disentangled object representation and a dynamics model\nfor object parts from unlabeled videos. Our Parts, Structure, and Dynamics\n(PSD) model learns to, first, recognize the object parts via a layered image\nrepresentation; second, predict hierarchy via a structural descriptor that\ncomposes low-level concepts into a hierarchical structure; and third, model the\nsystem dynamics by predicting the future. Experiments on multiple real and\nsynthetic datasets demonstrate that our PSD model works well on all three\ntasks: segmenting object parts, building their hierarchical structure, and\ncapturing their motion distributions.'"
William Freeman,Freeman_William,arXiv:1902.09554,https://arxiv.org/abs/1902.09554,"b'Abstract:  We present ionized gas kinematics for 708 galaxies at $z\\sim 1.4-3.8$ from\nthe MOSFIRE Deep Evolution Field survey, measured using models which account\nfor random galaxy-slit misalignments together with structural parameters\nderived from CANDELS Hubble Space Telescope (HST) imaging. Kinematics and sizes\nare used to derive dynamical masses. Baryonic masses are estimated from stellar\nmasses and inferred gas masses from dust-corrected star formation rates (SFRs)\nand the Kennicutt-Schmidt relation. We measure resolved rotation for 108\ngalaxies. For the remaining 600 galaxies we use models based on HST imaging\nstructural parameters together with integrated velocity dispersions and\nbaryonic masses to statistically constrain the median ratio of intrinsic\nordered-to-disordered motion, $V/\\sigma_{V,0}$. We find $V/\\sigma_{V,0}$\nincreases with increasing stellar mass and decreasing specific SFR (SSFR).\nThese trends may reflect marginal disk stability, where systems with higher gas\nfractions have thicker disks. For galaxies with detected rotation we assess\ntrends between their kinematics and mass, SSFR, and baryon surface density\n($\\Sigma_{\\mathrm{bar},e}$). Intrinsic dispersion correlates most with\n$\\Sigma_{\\mathrm{bar},e}$ and velocity correlates most with mass. By comparing\ndynamical and baryonic masses, we find that galaxies at $z\\sim 1.4-3.8$ are\nbaryon dominated within their effective radii ($R_E$), with Mdyn/Mbaryon\nincreasing over time. The inferred baryon fractions within $R_E$,\n$f_{\\mathrm{bar}}$, decrease over time, even at fixed mass, size, or surface\ndensity. At fixed redshift, $f_{\\mathrm{bar}}$ does not appear to vary with\nstellar mass, but increases with decreasing $R_E$ and increasing\n$\\Sigma_{\\mathrm{bar},e}$. For galaxies at $z\\geq2$, the median inferred baryon\nfractions generally exceed 100%. We discuss possible explanations and future\navenues to resolve this tension.'"
William Freeman,Freeman_William,arXiv:1901.09887,https://arxiv.org/abs/1901.09887,"b'Abstract:  Generative Adversarial Networks (GANs) have achieved impressive results for\nmany real-world applications. As an active research topic, many GAN variants\nhave emerged with improvements in sample quality and training stability.\nHowever, visualization and understanding of GANs is largely missing. How does a\nGAN represent our visual world internally? What causes the artifacts in GAN\nresults? How do architectural choices affect GAN learning? Answering such\nquestions could enable us to develop new insights and better models. In this\nwork, we present an analytic framework to visualize and understand GANs at the\nunit-, object-, and scene-level. We first identify a group of interpretable\nunits that are closely related to concepts with a segmentation-based network\ndissection method. We quantify the causal effect of interpretable units by\nmeasuring the ability of interventions to control objects in the output.\nFinally, we examine the contextual relationship between these units and their\nsurrounding by inserting the discovered concepts into new images. We show\nseveral practical applications enabled by our framework, from comparing\ninternal representations across different layers, models, and datasets, to\nimproving GANs by locating and removing artifact-causing units, to\ninteractively manipulating objects in the scene. We will open source our\ninteractive tools to help researchers and practitioners better understand their\nmodels.'"
William Freeman,Freeman_William,arXiv:1901.02875,https://arxiv.org/abs/1901.02875,"b'Abstract:  Human perception of 3D shapes goes beyond reconstructing them as a set of\npoints or a composition of geometric primitives: we also effortlessly\nunderstand higher-level shape structure such as the repetition and reflective\nsymmetry of object parts. In contrast, recent advances in 3D shape sensing\nfocus more on low-level geometry but less on these higher-level relationships.\nIn this paper, we propose 3D shape programs, integrating bottom-up recognition\nsystems with top-down, symbolic program structure to capture both low-level\ngeometry and high-level structural priors for 3D shapes. Because there are no\nannotations of shape programs for real shapes, we develop neural modules that\nnot only learn to infer 3D shape programs from raw, unannotated shapes, but\nalso to execute these programs for shape reconstruction. After initial\nbootstrapping, our end-to-end differentiable model learns 3D shape programs by\nreconstructing shapes in a self-supervised manner. Experiments demonstrate that\nour model accurately infers and executes 3D shape programs for highly complex\nshapes from various categories. It can also be integrated with an\nimage-to-shape module to infer 3D shape programs directly from an RGB image,\nleading to 3D shape reconstructions that are both more accurate and more\nphysically plausible.'"
William Freeman,Freeman_William,arXiv:1812.11166,https://arxiv.org/abs/1812.11166,"b'Abstract:  From a single image, humans are able to perceive the full 3D shape of an\nobject by exploiting learned shape priors from everyday life. Contemporary\nsingle-image 3D reconstruction algorithms aim to solve this task in a similar\nfashion, but often end up with priors that are highly biased by training\nclasses. Here we present an algorithm, Generalizable Reconstruction (GenRe),\ndesigned to capture more generic, class-agnostic shape priors. We achieve this\nwith an inference network and training procedure that combine 2.5D\nrepresentations of visible surfaces (depth and silhouette), spherical shape\nrepresentations of both visible and non-visible surfaces, and 3D voxel-based\nrepresentations, in a principled manner that exploits the causal structure of\nhow 3D shapes give rise to 2D images. Experiments demonstrate that GenRe\nperforms well on single-view shape reconstruction, and generalizes to diverse\nnovel objects from categories not seen during training.'"
William Freeman,Freeman_William,arXiv:1812.10972,https://arxiv.org/abs/1812.10972,"b'Abstract:  Object-based factorizations provide a useful level of abstraction for\ninteracting with the world. Building explicit object representations, however,\noften requires supervisory signals that are difficult to obtain in practice. We\npresent a paradigm for learning object-centric representations for physical\nscene understanding without direct supervision of object properties. Our model,\nObject-Oriented Prediction and Planning (O2P2), jointly learns a perception\nfunction to map from image observations to object representations, a pairwise\nphysics interaction function to predict the time evolution of a collection of\nobjects, and a rendering function to map objects back to pixels. For\nevaluation, we consider not only the accuracy of the physical predictions of\nthe model, but also its utility for downstream tasks that require an actionable\nrepresentation of intuitive physics. After training our model on an image\nprediction task, we can use its learned representations to build block towers\nmore complicated than those observed during training.'"
William Freeman,Freeman_William,arXiv:1812.02725,https://arxiv.org/abs/1812.02725,"b""Abstract:  Recent progress in deep generative models has led to tremendous breakthroughs\nin image generation. However, while existing models can synthesize\nphotorealistic images, they lack an understanding of our underlying 3D world.\nWe present a new generative model, Visual Object Networks (VON), synthesizing\nnatural images of objects with a disentangled 3D representation. Inspired by\nclassic graphics rendering pipelines, we unravel our image formation process\ninto three conditionally independent factors---shape, viewpoint, and\ntexture---and present an end-to-end adversarial learning framework that jointly\nmodels 3D shapes and 2D images. Our model first learns to synthesize 3D shapes\nthat are indistinguishable from real shapes. It then renders the object's 2.5D\nsketches (i.e., silhouette and depth map) from its shape under a sampled\nviewpoint. Finally, it learns to add realistic texture to these 2.5D sketches\nto generate natural images. The VON not only generates images that are more\nrealistic than state-of-the-art 2D image synthesis methods, but also enables\nmany 3D operations such as changing the viewpoint of a generated image, editing\nof shape and texture, linear interpolation in texture and shape space, and\ntransferring appearance across different objects and viewpoints."""
William Freeman,Freeman_William,arXiv:1811.11767,https://arxiv.org/abs/1811.11767,"b'Abstract:  We use extensive spectroscopy from the MOSFIRE Deep Evolution Field (MOSDEF)\nsurvey to investigate the relationships between rest-frame optical emission\nline equivalent widths ($W$) and a number of galaxy and ISM characteristics for\na sample of $1134$ star-forming galaxies at redshifts $1.4\\lesssim z\\lesssim\n3.8$. We examine how the equivalent widths of [OII]$\\lambda\\lambda 3727, 3730$,\nH$\\beta$, [OIII]$\\lambda\\lambda 4960, 5008$, [OIII]$+$H$\\beta$, H$\\alpha$, and\nH$\\alpha$+[NII]$\\lambda\\lambda 6550, 6585$, depend on stellar mass, UV slope,\nage, star-formation rate (SFR) and specific SFR (sSFR), ionization parameter\nand excitation conditions (O32 and [OIII]/H$\\beta$), gas-phase metallicity, and\nionizing photon production efficiency ($\\xi_{\\rm ion}$). The trend of\nincreasing $W$ with decreasing stellar mass is strongest for [OIII] (and\n[OIII]+H$\\beta$). More generally, the equivalent widths of all the lines\nincrease with redshift at a fixed stellar mass or fixed gas-phase metallicity,\nsuggesting that high equivalent width galaxies are common at high redshift.\nThis redshift evolution in equivalent widths can be explained by the increase\nin SFR and decrease in metallicity with redshift at a fixed stellar mass.\nConsequently, the dependence of $W$ on sSFR is largely invariant with redshift,\nparticularly when examined for galaxies of a given metallicity. Our results\nshow that high equivalent width galaxies, specifically those with high $W({\\rm\n[OIII]})$, have low stellar masses, blue UV slopes, young ages, high sSFRs, ISM\nline ratios indicative of high ionization parameters, high $\\xi_{\\rm ion}$, and\nlow metallicities. As these characteristics are often attributed to galaxies\nwith high ionizing escape fractions, galaxies with high $W$ are likely\ncandidates for the population that dominates cosmic reionization.'"
William Freeman,Freeman_William,arXiv:1811.10597,https://arxiv.org/abs/1811.10597,"b'Abstract:  Generative Adversarial Networks (GANs) have recently achieved impressive\nresults for many real-world applications, and many GAN variants have emerged\nwith improvements in sample quality and training stability. However, they have\nnot been well visualized or understood. How does a GAN represent our visual\nworld internally? What causes the artifacts in GAN results? How do\narchitectural choices affect GAN learning? Answering such questions could\nenable us to develop new insights and better models.\nIn this work, we present an analytic framework to visualize and understand\nGANs at the unit-, object-, and scene-level. We first identify a group of\ninterpretable units that are closely related to object concepts using a\nsegmentation-based network dissection method. Then, we quantify the causal\neffect of interpretable units by measuring the ability of interventions to\ncontrol objects in the output. We examine the contextual relationship between\nthese units and their surroundings by inserting the discovered object concepts\ninto new images. We show several practical applications enabled by our\nframework, from comparing internal representations across different layers,\nmodels, and datasets, to improving GANs by locating and removing\nartifact-causing units, to interactively manipulating objects in a scene. We\nprovide open source interpretation tools to help researchers and practitioners\nbetter understand their GAN models.'"
William Freeman,Freeman_William,arXiv:1811.05443,https://arxiv.org/abs/1811.05443,"b'Abstract:  Deep neural networks, trained with large amount of labeled data, can fail to\ngeneralize well when tested with examples from a \\emph{target domain} whose\ndistribution differs from the training data distribution, referred as the\n\\emph{source domain}. It can be expensive or even infeasible to obtain required\namount of labeled data in all possible domains. Unsupervised domain adaptation\nsets out to address this problem, aiming to learn a good predictive model for\nthe target domain using labeled examples from the source domain but only\nunlabeled examples from the target domain. Domain alignment approaches this\nproblem by matching the source and target feature distributions, and has been\nused as a key component in many state-of-the-art domain adaptation methods.\nHowever, matching the marginal feature distributions does not guarantee that\nthe corresponding class conditional distributions will be aligned across the\ntwo domains. We propose co-regularized domain alignment for unsupervised domain\nadaptation, which constructs multiple diverse feature spaces and aligns source\nand target distributions in each of them individually, while encouraging that\nalignments agree with each other with regard to the class predictions on the\nunlabeled target examples. The proposed method is generic and can be used to\nimprove any domain adaptation method which uses domain alignment. We\ninstantiate it in the context of a recent state-of-the-art method and observe\nthat it provides significant performance improvements on several domain\nadaptation benchmarks.'"
William Freeman,Freeman_William,arXiv:1810.07204,https://arxiv.org/abs/1810.07204,"b'Abstract:  Using the MOSDEF rest-frame optical spectroscopic survey, we investigate the\nstar-formation histories (SFHs) of different galaxy types, ranging from\nactively star forming to quiescent at $1.4\\leq~z\\leq2.6$. SFHs are constrained\nutilizing stellar continuum spectroscopy, specifically through a combination of\nBalmer absorption lines, the 4000$\\AA$ break, and the equivalent width of the\nH$\\alpha$ emission line. To attain a sufficiently high signal-to-noise ratio\n(S/N) to conduct these measurements we stack spectra of galaxies with similar\nspectral types, as determined from their rest-frame $U-V$ and $V-J$ colors. We\nbin the MOSDEF sample into five spectral types, subdividing the quiescent and\nstar-forming bins to better explore galaxies transitioning between the two. We\nconstrain the average SFHs for each type, finding that quiescent and\ntransitional galaxies in the MOSDEF sample are dominated by an SFH with an\naverage star-formation timescale of $\\tau\\sim0.1-0.2$~Gyr. These findings\ncontrast with measurements from the low-redshift Universe where, on average,\ngalaxies form their stars over a more extended time period ($\\tau>1$~Gyr).\nFurthermore, our spectral index measurements correlate with mass surface\ndensity for all spectral types. Finally, we compare the average properties of\nthe galaxies in our transitional bins to investigate possible paths to\nquiescence, and speculate on the viability of a dusty post-starburst phase.'"
William Freeman,Freeman_William,arXiv:1810.01054,https://arxiv.org/abs/1810.01054,"b'Abstract:  Physical simulators have been widely used in robot planning and control.\nAmong them, differentiable simulators are particularly favored, as they can be\nincorporated into gradient-based optimization algorithms that are efficient in\nsolving inverse problems such as optimal control and motion planning.\nSimulating deformable objects is, however, more challenging compared to rigid\nbody dynamics. The underlying physical laws of deformable objects are more\ncomplex, and the resulting systems have orders of magnitude more degrees of\nfreedom and therefore they are significantly more computationally expensive to\nsimulate. Computing gradients with respect to physical design or controller\nparameters is typically even more computationally challenging. In this paper,\nwe propose a real-time, differentiable hybrid Lagrangian-Eulerian physical\nsimulator for deformable objects, ChainQueen, based on the Moving Least Squares\nMaterial Point Method (MLS-MPM). MLS-MPM can simulate deformable objects\nincluding contact and can be seamlessly incorporated into inference, control\nand co-design systems. We demonstrate that our simulator achieves high\nprecision in both forward simulation and backward gradient computation. We have\nsuccessfully employed it in a diverse set of control tasks for soft robots,\nincluding problems with nearly 3,000 decision variables.'"
William Freeman,Freeman_William,arXiv:1809.05491,https://arxiv.org/abs/1809.05491,"b""Abstract:  We present a system that allows users to visualize complex human motion via\n3D motion sculptures---a representation that conveys the 3D structure swept by\na human body as it moves through space. Given an input video, our system\ncomputes the motion sculptures and provides a user interface for rendering it\nin different styles, including the options to insert the sculpture back into\nthe original video, render it in a synthetic scene or physically print it.\nTo provide this end-to-end workflow, we introduce an algorithm that estimates\nthat human's 3D geometry over time from a set of 2D images and develop a\n3D-aware image-based rendering approach that embeds the sculpture back into the\nscene. By automating the process, our system takes motion sculpture creation\nout of the realm of professional artists, and makes it applicable to a wide\nrange of existing video material.\nBy providing viewers with 3D information, motion sculptures reveal space-time\nmotion information that is difficult to perceive with the naked eye, and allow\nviewers to interpret how different parts of the object interact over time. We\nvalidate the effectiveness of this approach with user studies, finding that our\nmotion sculpture visualizations are significantly more informative about motion\nthan existing stroboscopic and space-time visualization methods."""
William Freeman,Freeman_William,arXiv:1809.05070,https://arxiv.org/abs/1809.05070,"b""Abstract:  Objects are made of parts, each with distinct geometry, physics,\nfunctionality, and affordances. Developing such a distributed, physical,\ninterpretable representation of objects will facilitate intelligent agents to\nbetter explore and interact with the world. In this paper, we study physical\nprimitive decomposition---understanding an object through its components, each\nwith physical and geometric attributes. As annotated data for object parts and\nphysics are rare, we propose a novel formulation that learns physical\nprimitives by explaining both an object's appearance and its behaviors in\nphysical events. Our model performs well on block towers and tools in both\nsynthetic and real scenarios; we also demonstrate that visual and physical\nobservations often provide complementary signals. We further present ablation\nand behavioral studies to better understand our model and contrast it with\nhuman performance."""
William Freeman,Freeman_William,arXiv:1809.05068,https://arxiv.org/abs/1809.05068,"b'Abstract:  The problem of single-view 3D shape completion or reconstruction is\nchallenging, because among the many possible shapes that explain an\nobservation, most are implausible and do not correspond to natural objects.\nRecent research in the field has tackled this problem by exploiting the\nexpressiveness of deep convolutional networks. In fact, there is another level\nof ambiguity that is often overlooked: among plausible shapes, there are still\nmultiple shapes that fit the 2D image equally well; i.e., the ground truth\nshape is non-deterministic given a single-view input. Existing fully supervised\napproaches fail to address this issue, and often produce blurry mean shapes\nwith smooth surfaces but no fine details.\nIn this paper, we propose ShapeHD, pushing the limit of single-view shape\ncompletion and reconstruction by integrating deep generative models with\nadversarially learned shape priors. The learned priors serve as a regularizer,\npenalizing the model only if its output is unrealistic, not if it deviates from\nthe ground truth. Our design thus overcomes both levels of ambiguity\naforementioned. Experiments demonstrate that ShapeHD outperforms state of the\nart by a large margin in both shape completion and shape reconstruction on\nmultiple real datasets.'"
William Freeman,Freeman_William,arXiv:1809.05067,https://arxiv.org/abs/1809.05067,"b'Abstract:  Humans recognize object structure from both their appearance and motion;\noften, motion helps to resolve ambiguities in object structure that arise when\nwe observe object appearance only. There are particular scenarios, however,\nwhere neither appearance nor spatial-temporal motion signals are informative:\noccluding twigs may look connected and have almost identical movements, though\nthey belong to different, possibly disconnected branches. We propose to tackle\nthis problem through spectrum analysis of motion signals, because vibrations of\ndisconnected branches, though visually similar, often have distinctive natural\nfrequencies. We propose a novel formulation of tree structure based on a\nphysics-based link model, and validate its effectiveness by theoretical\nanalysis, numerical simulation, and empirical experiments. With this\nformulation, we use nonparametric Bayesian inference to reconstruct tree\nstructure from both spectral vibration signals and appearance cues. Our model\nperforms well in recognizing hierarchical tree structure from real-world videos\nof trees and vessels.'"
William Freeman,Freeman_William,arXiv:1808.09978,https://arxiv.org/abs/1808.09978,"b'Abstract:  We study the properties of 30 spectroscopically-identified pairs of galaxies\nobserved during the peak epoch of star formation in the universe. These systems\nare drawn from the MOSFIRE Deep Evolution Field (MOSDEF) Survey at $1.4 \\leq z\n\\leq 3.8$, and are interpreted as early-stage galaxy mergers. Galaxy pairs in\nour sample are identified as two objects whose spectra were collected on the\nsame Keck/MOSFIRE spectroscopic slit. Accordingly, all pairs in the sample have\nprojected separations $R_{\\rm proj}\\leq 60$ kpc. The velocity separation for\npairs was required to be $\\Delta v \\leq 500 \\mbox{ km s}^{-1}$, which is a\nstandard threshold for defining interacting galaxy pairs at low redshift.\nStellar mass ratios in our sample range from 1.1 to 550, with 12 ratios closer\nthan or equal to 3:1, the common definition of a ""major merger."" Studies of\nmerging pairs in the local universe indicate anenhancement in star-formation\nactivity and deficit in gas-phase oxygen abundance relative to isolated\ngalaxies of the same mass. We compare the MOSDEF pairs sample to a control\nsample of isolated galaxies at the same redshift, finding no measurable SFR\nenhancement or metallicity deficit at fixed stellar mass for the pairs sample.\nThe lack of significant difference between the average properties of pairs and\ncontrol samples appears in contrast to results from low-redshift studies,\nalthough the small sample size and lower signal-to-noise of the high-redshift\ndata limit definitive conclusions on redshift evolution. These results are\nconsistent with some theoretical works suggesting a reduced differential effect\nof pre-coalescence mergers on galaxy properties at high redshift --\nspecifically that pre-coalescence mergers do not drive strong starbursts.'"
William Freeman,Freeman_William,arXiv:1808.09351,https://arxiv.org/abs/1808.09351,"b'Abstract:  We aim to obtain an interpretable, expressive, and disentangled scene\nrepresentation that contains comprehensive structural and textural information\nfor each object. Previous scene representations learned by neural networks are\noften uninterpretable, limited to a single object, or lacking 3D knowledge. In\nthis work, we propose 3D scene de-rendering networks (3D-SDN) to address the\nabove issues by integrating disentangled representations for semantics,\ngeometry, and appearance into a deep generative model. Our scene encoder\nperforms inverse graphics, translating a scene into a structured object-wise\nrepresentation. Our decoder has two components: a differentiable shape renderer\nand a neural texture generator. The disentanglement of semantics, geometry, and\nappearance supports 3D-aware scene manipulation, e.g., rotating and moving\nobjects freely while keeping the consistent shape and texture, and changing the\nobject appearance without affecting its shape. Experiments demonstrate that our\nediting scheme based on 3D-SDN is superior to its 2D counterpart.'"
William Freeman,Freeman_William,arXiv:1808.05732,https://arxiv.org/abs/1808.05732,"b'Abstract:  We present an algorithm for creating high resolution anatomically plausible\nimages consistent with acquired clinical brain MRI scans with large inter-slice\nspacing. Although large data sets of clinical images contain a wealth of\ninformation, time constraints during acquisition result in sparse scans that\nfail to capture much of the anatomy. These characteristics often render\ncomputational analysis impractical as many image analysis algorithms tend to\nfail when applied to such images. Highly specialized algorithms that explicitly\nhandle sparse slice spacing do not generalize well across problem domains. In\ncontrast, we aim to enable application of existing algorithms that were\noriginally developed for high resolution research scans to significantly\nundersampled scans. We introduce a generative model that captures fine-scale\nanatomical structure across subjects in clinical image collections and derive\nan algorithm for filling in the missing data in scans with large inter-slice\nspacing. Our experimental results demonstrate that the resulting method\noutperforms state-of-the-art upsampling super-resolution techniques, and\npromises to facilitate subsequent analysis not previously possible with scans\nof this quality. Our implementation is freely available at\nthis https URL .'"
William Freeman,Freeman_William,arXiv:1808.03247,https://arxiv.org/abs/1808.03247,"b""Abstract:  Perceiving accurate 3D object shape is important for robots to interact with\nthe physical world. Current research along this direction has been primarily\nrelying on visual observations. Vision, however useful, has inherent\nlimitations due to occlusions and the 2D-3D ambiguities, especially for\nperception with a monocular camera. In contrast, touch gets precise local shape\ninformation, though its efficiency for reconstructing the entire shape could be\nlow. In this paper, we propose a novel paradigm that efficiently perceives\naccurate 3D object shape by incorporating visual and tactile observations, as\nwell as prior knowledge of common object shapes learned from large-scale shape\nrepositories. We use vision first, applying neural networks with learned shape\npriors to predict an object's 3D shape from a single-view color image. We then\nuse tactile sensing to refine the shape; the robot actively touches the object\nregions where the visual prediction has high uncertainty. Our method\nefficiently builds the 3D shape of common objects from a color image and a\nsmall number of tactile explorations (around 10). Our setup is easy to apply\nand has potentials to help robots better perform grasping or manipulation tasks\non real-world objects."""
William Freeman,Freeman_William,arXiv:1807.09245,https://arxiv.org/abs/1807.09245,"b'Abstract:  We study the problem of synthesizing a number of likely future frames from a\nsingle input image. In contrast to traditional methods that have tackled this\nproblem in a deterministic or non-parametric way, we propose to model future\nframes in a probabilistic manner. Our probabilistic model makes it possible for\nus to sample and synthesize many possible future frames from a single input\nimage. To synthesize realistic movement of objects, we propose a novel network\nstructure, namely a Cross Convolutional Network; this network encodes image and\nmotion information as feature maps and convolutional kernels, respectively. In\nexperiments, our model performs well on synthetic data, such as 2D shapes and\nanimated game sprites, and on real-world video frames. We present analyses of\nthe learned network representations, showing it is implicitly learning a\ncompact encoding of object appearance and motion. We also demonstrate a few of\nits applications, including visual analogy-making and video extrapolation.'"
William Freeman,Freeman_William,arXiv:1806.08989,https://arxiv.org/abs/1806.08989,"b'Abstract:  We present an analysis using the MOSFIRE Deep Evolution Field (MOSDEF) survey\non the nature of ""MIR-excess"" galaxies, which have star formation rates (SFR)\ninferred from mid-infrared (MIR) data that is substantially elevated relative\nto that estimated from dust-corrected UV data. We use a sample of $\\sim$200\ngalaxies and AGN at $1.40<z<2.61$ with 24 $\\mu$m detections (rest-frame\n8$\\mu$m) from MIPS/\\textit{Spitzer}. We find that the identification of\nMIR-excess galaxies strongly depends on the methodologies used to estimate IR\nluminosity ($\\rm L_{IR}$) and to correct the UV light for dust attenuation. We\nfind that extrapolations of the SFR from the observed 24 $\\mu$m flux, using\nluminosity-dependent templates based on local galaxies, substantially\noverestimate $\\rm L_{IR}$ in $z\\sim2$ galaxies. By including \\textit{Herschel}\nobservations and using a stellar mass-dependent, luminosity-independent $\\rm\nL_{IR}$, we obtain more reliable estimates of the SFR and a lower fraction of\nMIR-excess galaxies. Once stellar mass selection biases are taken into account,\nwe identify $\\sim24\\%$ of our galaxies as MIR-excess. However, $\\rm\nSFR_{H\\alpha}$ is not elevated in MIR-excess galaxies compared to MIR-normal\ngalaxies, indicating that the intrinsic fraction of MIR-excess may be lower.\nUsing X-ray, IR, and optically-selected AGN in MOSDEF, we do not find a higher\nprevalence for AGN in MIR-excess galaxies relative to MIR-normal galaxies. A\nstacking analysis of X-ray undetected galaxies does not reveal a harder\nspectrum in MIR-excess galaxies relative to MIR-normal galaxies. Our analysis\nindicates that AGN activity does not contribute substantially to the MIR excess\nand instead implies that it is likely due to the enhanced PAH emission.'"
William Freeman,Freeman_William,arXiv:1806.06098,https://arxiv.org/abs/1806.06098,"b'Abstract:  We present a method for training a regression network from image pixels to 3D\nmorphable model coordinates using only unlabeled photographs. The training loss\nis based on features from a facial recognition network, computed on-the-fly by\nrendering the predicted faces with a differentiable renderer. To make training\nfrom features feasible and avoid network fooling effects, we introduce three\nobjectives: a batch distribution loss that encourages the output distribution\nto match the distribution of the morphable model, a loopback loss that ensures\nthe network can correctly reinterpret its own output, and a multi-view identity\nloss that compares the features of the predicted 3D face and the input\nphotograph from multiple viewing angles. We train a regression network using\nthese objectives, a set of unlabeled photographs, and the morphable model\nitself, and demonstrate state-of-the-art results.'"
William Freeman,Freeman_William,arXiv:1804.04610,https://arxiv.org/abs/1804.04610,"b'Abstract:  We study 3D shape modeling from a single image and make contributions to it\nin three aspects. First, we present Pix3D, a large-scale benchmark of diverse\nimage-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications\nin shape-related tasks including reconstruction, retrieval, viewpoint\nestimation, etc. Building such a large-scale dataset, however, is highly\nchallenging; existing datasets either contain only synthetic data, or lack\nprecise alignment between 2D images and 3D shapes, or only have a small number\nof images. Second, we calibrate the evaluation criteria for 3D shape\nreconstruction through behavioral studies, and use them to objectively and\nsystematically benchmark cutting-edge reconstruction algorithms on Pix3D.\nThird, we design a novel model that simultaneously performs 3D reconstruction\nand pose estimation; our multi-task learning approach achieves state-of-the-art\nperformance on both tasks.'"
William Freeman,Freeman_William,arXiv:1804.03619,https://arxiv.org/abs/1804.03619,"b'Abstract:  We present a joint audio-visual model for isolating a single speech signal\nfrom a mixture of sounds such as other speakers and background noise. Solving\nthis task using only audio as input is extremely challenging and does not\nprovide an association of the separated speech signals with speakers in the\nvideo. In this paper, we present a deep network-based model that incorporates\nboth visual and auditory signals to solve this task. The visual features are\nused to ""focus"" the audio on desired speakers in a scene and to improve the\nspeech separation quality. To train our joint audio-visual model, we introduce\nAVSpeech, a new dataset comprised of thousands of hours of video segments from\nthe Web. We demonstrate the applicability of our method to classic speech\nseparation tasks, as well as real-world scenarios involving heated interviews,\nnoisy bars, and screaming children, only requiring the user to specify the face\nof the person in the video whose speech they want to isolate. Our method shows\nclear advantage over state-of-the-art audio-only speech separation in cases of\nmixed speech. In addition, our model, which is speaker-independent (trained\nonce, applicable to any speaker), produces better results than recent\naudio-visual speech separation methods that are speaker-dependent (require\ntraining a separate model for each speaker of interest).'"
William Freeman,Freeman_William,arXiv:1804.02684,https://arxiv.org/abs/1804.02684,"b'Abstract:  Video motion magnification techniques allow us to see small motions\npreviously invisible to the naked eyes, such as those of vibrating airplane\nwings, or swaying buildings under the influence of the wind. Because the motion\nis small, the magnification results are prone to noise or excessive blurring.\nThe state of the art relies on hand-designed filters to extract representations\nthat may not be optimal. In this paper, we seek to learn the filters directly\nfrom examples using deep convolutional neural networks. To make training\ntractable, we carefully design a synthetic dataset that captures small motion\nwell, and use two-frame input for training. We show that the learned filters\nachieve high-quality results on real videos, with less ringing artifacts and\nbetter noise characteristics than previous methods. While our model is not\ntrained with temporal filters, we found that the temporal filters can be used\nwith our extracted representations up to a moderate magnification, enabling a\nfrequency-based motion selection. Finally, we analyze the learned filters and\nshow that they behave similarly to the derivative filters used in previous\nworks. Our code, trained model, and datasets will be available online.'"
William Freeman,Freeman_William,arXiv:1804.00782,https://arxiv.org/abs/1804.00782,"b'Abstract:  Understanding 3D object structure from a single image is an important but\nchallenging task in computer vision, mostly due to the lack of 3D object\nannotations to real images. Previous research tackled this problem by either\nsearching for a 3D shape that best explains 2D annotations, or training purely\non synthetic data with ground truth 3D information. In this work, we propose 3D\nINterpreter Networks (3D-INN), an end-to-end trainable framework that\nsequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses.\nOur system learns from both 2D-annotated real images and synthetic 3D data.\nThis is made possible mainly by two technical innovations. First, heatmaps of\n2D keypoints serve as an intermediate representation to connect real and\nsynthetic data. 3D-INN is trained on real images to estimate 2D keypoint\nheatmaps from an input image; it then predicts 3D object structure from\nheatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN\nbenefits from the variation and abundance of synthetic 3D objects, without\nsuffering from the domain difference between real and synthesized images, often\ndue to imperfect rendering. Second, we propose a Projection Layer, mapping\nestimated 3D structure back to 2D. During training, it ensures 3D-INN to\npredict 3D structure whose projection is consistent with the 2D annotations to\nreal images. Experiments show that the proposed system performs well on both 2D\nkeypoint estimation and 3D structure recovery. We also demonstrate that the\nrecovered 3D information has wide vision applications, such as image retrieval.'"
William Freeman,Freeman_William,arXiv:1712.08232,https://arxiv.org/abs/1712.08232,"b'Abstract:  We study the problem of reconstructing an image from information stored at\ncontour locations. We show that high-quality reconstructions with high fidelity\nto the source image can be obtained from sparse input, e.g., comprising less\nthan $6\\%$ of image pixels. This is a significant improvement over existing\ncontour-based reconstruction methods that require much denser input to capture\nsubtle texture information and to ensure image quality. Our model, based on\ngenerative adversarial networks, synthesizes texture and details in regions\nwhere no input information is provided. The semantic knowledge encoded into our\nmodel and the sparsity of the input allows to use contours as an intuitive\ninterface for semantically-aware image manipulation: local edits in contour\ndomain translate to long-range and coherent changes in pixel space. We can\nperform complex structural changes such as changing facial expression by simple\nedits of contours. Our experiments demonstrate that humans as well as a face\nrecognition system mostly cannot distinguish between our reconstructions and\nthe source images.'"
William Freeman,Freeman_William,arXiv:1712.07271,https://arxiv.org/abs/1712.07271,"b'Abstract:  The sound of crashing waves, the roar of fast-moving cars -- sound conveys\nimportant information about the objects in our surroundings. In this work, we\nshow that ambient sounds can be used as a supervisory signal for learning\nvisual models. To demonstrate this, we train a convolutional neural network to\npredict a statistical summary of the sound associated with a video frame. We\nshow that, through this process, the network learns a representation that\nconveys information about objects and scenes. We evaluate this representation\non several recognition tasks, finding that its performance is comparable to\nthat of other state-of-the-art unsupervised learning methods. Finally, we show\nthrough visualizations that the network learns units that are selective to\nobjects that are often associated with characteristic sounds. This paper\nextends an earlier conference paper, Owens et al. 2016, with additional\nexperiments and discussion.'"
William Freeman,Freeman_William,arXiv:1711.09078,https://arxiv.org/abs/1711.09078,"b'Abstract:  Many video enhancement algorithms rely on optical flow to register frames in\na video sequence. Precise flow estimation is however intractable; and optical\nflow itself is often a sub-optimal representation for particular video\nprocessing tasks. In this paper, we propose task-oriented flow (TOFlow), a\nmotion representation learned in a self-supervised, task-specific manner. We\ndesign a neural network with a trainable motion estimation component and a\nvideo processing component, and train them jointly to learn the task-oriented\nflow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video\ndataset for low-level video processing. TOFlow outperforms traditional optical\nflow on standard benchmarks as well as our Vimeo-90K dataset in three video\nprocessing tasks: frame interpolation, video denoising/deblocking, and video\nsuper-resolution.'"
William Freeman,Freeman_William,arXiv:1711.06297,https://arxiv.org/abs/1711.06297,"b'Abstract:  Active non-line-of-sight imaging systems are of growing interest for diverse\napplications. The most commonly proposed approaches to date rely on exploiting\ntime-resolved measurements, i.e., measuring the time it takes for short light\npulses to transit the scene. This typically requires expensive, specialized,\nultrafast lasers and detectors that must be carefully calibrated. We develop an\nalternative approach that exploits the valuable role that natural occluders in\na scene play in enabling accurate and practical image formation in such\nsettings without such hardware complexity. In particular, we demonstrate that\nthe presence of occluders in the hidden scene can obviate the need for\ncollecting time-resolved measurements, and develop an accompanying analysis for\nsuch systems and their generalizations. Ultimately, the results suggest the\npotential to develop increasingly sophisticated future systems that are able to\nidentify and exploit diverse structural features of the environment to\nreconstruct scenes hidden from view.'"
William Freeman,Freeman_William,arXiv:1711.03129,https://arxiv.org/abs/1711.03129,"b'Abstract:  3D object reconstruction from a single image is a highly under-determined\nproblem, requiring strong prior knowledge of plausible 3D shapes. This\nintroduces challenges for learning-based approaches, as 3D object annotations\nare scarce in real images. Previous work chose to train on synthetic data with\nground truth 3D information, but suffered from domain adaptation when tested on\nreal data. In this work, we propose MarrNet, an end-to-end trainable model that\nsequentially estimates 2.5D sketches and 3D object shape. Our disentangled,\ntwo-step formulation has three advantages. First, compared to full 3D shape,\n2.5D sketches are much easier to be recovered from a 2D image; models that\nrecover 2.5D sketches are also more likely to transfer from synthetic to real\ndata. Second, for 3D reconstruction from 2.5D sketches, systems can learn\npurely from synthetic data. This is because we can easily render realistic 2.5D\nsketches without modeling object appearance variations in real images,\nincluding lighting, texture, etc. This further relieves the domain adaptation\nproblem. Third, we derive differentiable projective functions from 3D shape to\n2.5D sketches; the framework is therefore end-to-end trainable on real images,\nrequiring no human annotations. Our model achieves state-of-the-art performance\non 3D shape reconstruction.'"
William Freeman,Freeman_William,arXiv:1711.01357,https://arxiv.org/abs/1711.01357,"b""Abstract:  Very long baseline interferometry (VLBI) makes it possible to recover images\nof astronomical sources with extremely high angular resolution. Most recently,\nthe Event Horizon Telescope (EHT) has extended VLBI to short millimeter\nwavelengths with a goal of achieving angular resolution sufficient for imaging\nthe event horizons of nearby supermassive black holes. VLBI provides\nmeasurements related to the underlying source image through a sparse set\nspatial frequencies. An image can then be recovered from these measurements by\nmaking assumptions about the underlying image. One of the most important\nassumptions made by conventional imaging methods is that over the course of a\nnight's observation the image is static. However, for quickly evolving sources,\nsuch as the galactic center's supermassive black hole (Sgr A*) targeted by the\nEHT, this assumption is violated and these conventional imaging approaches\nfail. In this work we propose a new way to model VLBI measurements that allows\nus to recover both the appearance and dynamics of an evolving source by\nreconstructing a video rather than a static image. By modeling VLBI\nmeasurements using a Gaussian Markov Model, we are able to propagate\ninformation across observations in time to reconstruct a video, while\nsimultaneously learning about the dynamics of the source's emission region. We\ndemonstrate our proposed Expectation-Maximization (EM) algorithm, StarWarps, on\nrealistic synthetic observations of black holes, and show how it substantially\nimproves results compared to conventional imaging algorithms. Additionally, we\ndemonstrate StarWarps on real VLBI data of the M87 Jet from the VLBA."""
William Freeman,Freeman_William,arXiv:1711.00224,https://arxiv.org/abs/1711.00224,"b'Abstract:  We investigate the nature of the relation among stellar mass, star-formation\nrate, and gas-phase metallicity (the M$_*$-SFR-Z relation) at high redshifts\nusing a sample of 260 star-forming galaxies at $z\\sim2.3$ from the MOSDEF\nsurvey. We present an analysis of the high-redshift M$_*$-SFR-Z relation based\non several emission-line ratios for the first time. We show that a M$_*$-SFR-Z\nrelation clearly exists at $z\\sim2.3$. The strength of this relation is similar\nto predictions from cosmological hydrodynamical simulations. By performing a\ndirect comparison of stacks of $z\\sim0$ and $z\\sim2.3$ galaxies, we find that\n$z\\sim2.3$ galaxies have $\\sim0.1$ dex lower metallicity at fixed M$_*$ and\nSFR. In the context of chemical evolution models, this evolution of the\nM$_*$-SFR-Z relation suggests an increase with redshift of the mass-loading\nfactor at fixed M$_*$, as well as a decrease in the metallicity of infalling\ngas that is likely due to a lower importance of gas recycling relative to\naccretion from the intergalactic medium at high redshifts. Performing this\nanalysis simultaneously with multiple metallicity-sensitive line ratios allows\nus to rule out the evolution in physical conditions (e.g., N/O ratio,\nionization parameter, and hardness of the ionizing spectrum) at fixed\nmetallicity as the source of the observed trends with redshift and with SFR at\nfixed M$_*$ at $z\\sim2.3$. While this study highlights the promise of\nperforming high-order tests of chemical evolution models at high redshifts,\ndetailed quantitative comparisons ultimately await a full understanding of the\nevolution of metallicity calibrations with redshift.'"
William Freeman,Freeman_William,arXiv:1711.00013,https://arxiv.org/abs/1711.00013,"b'Abstract:  We combine spectroscopic measurements of H$\\alpha$ and H$\\beta$ and UV\ncontinuum photometry for a sample of 673 galaxies from the MOSFIRE Deep\nEvolution Field survey to constrain hydrogen ionizing photon production\nefficiencies ($\\xi_{\\rm ion}$, xi_ion) at z=1.4-2.6. We find average\nlog(xi_ion/[Hz erg$^{-1}$])=25.06 (25.34), assuming the Calzetti (SMC) curve\nfor the UV dust correction and a scatter of 0.28 dex in xi_ion distribution.\nAfter accounting for observational uncertainties and variations in dust\nattenuation, we conclude that the remaining scatter in xi_ion is likely\ndominated by galaxy-to-galaxy variations in stellar populations, including the\nslope and upper-mass cutoff of the initial mass function, stellar metallicity,\nstar-formation burstiness, and stellar evolution (e.g., single/binary star\nevolution). Moreover, xi_ion is elevated in galaxies with high ionization\nstates (high [OIII]/[OII]) and low oxygen abundances (low [NII]/H$\\alpha$ and\nhigh [OIII]/H$\\beta$) in the ionized ISM. However, xi_ion does not correlate\nwith the offset from the z~0 star-forming locus in the BPT diagram, suggesting\nno change in the hardness of ionizing radiation accompanying the offset from\nthe z~0 sequence. We also find that galaxies with blue UV spectral slopes\n($\\langle\\beta\\rangle$=-2.1) have elevated xi_ion by a factor of ~2 relative to\nthe average xi_ion of the sample ($\\langle\\beta\\rangle$=-1.4). If these blue\ngalaxies are similar to those at z > 6, our results suggest that a lower Lyman\ncontinuum escape fraction is required for galaxies to maintain reionization,\ncompared to the canonical xi_ion predictions from stellar population models.\nFurthermore, we demonstrate that even with robustly dust-corrected H$\\alpha$,\nthe UV dust attenuation can cause on average a ~0.3dex systematic uncertainty\nin xi_ion calculations.'"
William Freeman,Freeman_William,arXiv:1710.03230,https://arxiv.org/abs/1710.03230,"b'Abstract:  We present results from the MOSFIRE Deep Evolution Field (MOSDEF) survey on\nbroad flux from the nebular emission lines H$\\alpha$, [NII], [OIII], H$\\beta$,\nand [SII]. The sample consists of 127 star-forming galaxies at $1.37 < z <\n2.61$ and 84 galaxies at $2.95 < z < 3.80$. We decompose the emission lines\nusing narrow ($\\text{FWHM} < 275 \\ \\text{km s}^{-1}$) and broad ($\\text{FWHM} >\n300 \\ \\text{km s}^{-1}$) Gaussian components for individual galaxies and\nstacks. Broad emission is detected at $>3\\sigma$ in $<10$% of galaxies and the\nbroad flux accounts for 10-70% of the total flux. We find a slight increase in\nbroad to narrow flux ratio with mass but note that we cannot reliably detect\nbroad emission with $\\text{FWHM} < 275 \\ \\text{km s}^{-1}$, which may be\nsignificant at low masses. Notably, there is a correlation between higher\nsignal-to-noise (S/N) spectra and a broad component detection indicating a S/N\ndependence in our ability to detect broad flux. When placed on the N2-BPT\ndiagram ([OIII]/H$\\beta$ vs. [NII]/H$\\alpha$) the broad components of the\nstacks are shifted towards higher [OIII]/H$\\beta$ and [NII]/$\\alpha$ ratios\ncompared to the narrow component. We compare the location of the broad\ncomponents to shock models and find that the broad component could be due to\nshocks, but we do not rule out other possibilities such as the presence of an\nAGN. We estimate the mass loading factor (mass outflow rate/star formation\nrate) assuming the broad component is a photoionized outflow and find that the\nmass loading factor increases as a function of mass which agrees with previous\nstudies. We show that adding emission from shocked gas to $z\\sim0$ SDSS spectra\nshifts galaxies towards the location of $z\\sim2$ galaxies on several emission\nline diagnostic diagrams.'"
William Freeman,Freeman_William,arXiv:1707.05331,https://arxiv.org/abs/1707.05331,"b'Abstract:  We present the first spectroscopic measurement of multiple rest-frame optical\nemission lines at $z>4$. During the MOSFIRE Deep Evolution Field (MOSDEF)\nsurvey, we observed the galaxy GOODSN-17940 with the Keck I/MOSFIRE\nspectrograph. The K-band spectrum of GOODSN-17940 includes significant\ndetections of the [OII]$\\lambda\\lambda 3726,3729$, [NeIII]$\\lambda3869$, and\nH$\\gamma$ emission lines and a tentative detection of H$\\delta$, indicating\n$z_{\\rm{spec}}=4.4121$. GOODSN-17940 is an actively star-forming $z>4$ galaxy\nbased on its K-band spectrum and broadband spectral energy distribution. A\nsignificant excess relative to the surrounding continuum is present in the\nSpitzer/IRAC channel 1 photometry of GOODSN-17940, due primarily to strong\nH$\\alpha$ emission with a rest-frame equivalent width of\n$\\mbox{EW(H}\\alpha)=1200$ \\AA. Based on the assumption of $0.5 Z_{\\odot}$\nmodels and the Calzetti attenuation curve, GOODSN-17940 is characterized by\n$M_*=5.0^{+4.3}_{-0.2}\\times 10^9 M_{\\odot}$. The Balmer decrement inferred\nfrom H$\\alpha$/H$\\gamma$ is used to dust correct the H$\\alpha$ emission,\nyielding $\\mbox{SFR(H}\\alpha)=320^{+190}_{-140} M_{\\odot}\\mbox{ yr}^{-1}$.\nThese $M_*$ and SFR values place GOODSN-17940 an order of magnitude in SFR\nabove the $z\\sim 4$ star-forming ""main sequence."" Finally, we use the observed\nratio of [NeIII]/[OII] to estimate the nebular oxygen abundance in\nGOODSN-17940, finding $\\mbox{O/H}\\sim 0.2 \\mbox{ (O/H)}_{\\odot}$. Combining our\nnew [NeIII]/[OII] measurement with those from stacked spectra at $z\\sim 0, 2,\n\\mbox{ and } 3$, we show that GOODSN-17940 represents an extension to $z>4$ of\nthe evolution towards higher [NeIII]/[OII] (i.e., lower $\\mbox{O/H}$) at fixed\nstellar mass. It will be possible to perform the measurements presented here\nout to $z\\sim 10$ using the James Webb Space Telescope.'"
William Freeman,Freeman_William,arXiv:1703.10255,https://arxiv.org/abs/1703.10255,"b'Abstract:  Using observations from the first two years of the MOSFIRE Deep Evolution\nField (MOSDEF) survey, we study 13 AGN-driven outflows detected from a sample\nof 67 X-ray, IR and/or optically-selected AGN at $z \\sim 2$. The AGN have\nbolometric luminosities of $\\sim10^{44}-10^{46} ~\\mathrm{erg~s^{-1}}$,\nincluding both quasars and moderate-luminosity AGN. We detect blueshifted,\nionized gas outflows in the H$\\beta$ , [OIII], H$\\alpha$ ~and/or [NII] emission\nlines of $19\\%$ of the AGN, while only 1.8\\% of the MOSDEF galaxies have\nsimilarly-detected outflows. The outflow velocities span $\\sim$300 to 1000 km\ns$^{-1}$. Eight of the 13 outflows are spatially extended on similar scales as\nthe host galaxies, with spatial extents of 2.5 to 11.0 kpc. Outflows are\ndetected uniformly across the star-forming main sequence, showing little trend\nwith the host galaxy SFR. Line ratio diagnostics indicate that the outflowing\ngas is photoionized by the AGN. We do not find evidence for positive AGN\nfeedback, in either our small MOSDEF sample or a much larger SDSS sample, using\nthe BPT diagram. Given that a galaxy with an AGN is ten times more likely to\nhave a detected outflow, the outflowing gas is photoionzed by the AGN, and\nestimates of the mass and energy outflow rates indicate that stellar feedback\nis insufficient to drive at least some of these outflows, they are very likely\nto be AGN-driven. The outflows have mass-loading factors of the order of unity,\nsuggesting that they help regulate star formation in their host galaxies,\nthough they may be insufficient to fully quench it.'"
William Freeman,Freeman_William,arXiv:1701.04851,https://arxiv.org/abs/1701.04851,"b""Abstract:  We present a method for synthesizing a frontal, neutral-expression image of a\nperson's face given an input face photograph. This is achieved by learning to\ngenerate facial landmarks and textures from features extracted from a\nfacial-recognition network. Unlike previous approaches, our encoding feature\nvector is largely invariant to lighting, pose, and facial expression.\nExploiting this invariance, we train our decoder network using only frontal,\nneutral-expression photographs. Since these photographs are well aligned, we\ncan decompose them into a sparse set of landmark points and aligned texture\nmaps. The decoder then predicts landmarks and textures independently and\ncombines them using a differentiable image warping operation. The resulting\nimages can be used for a number of applications, such as analyzing facial\nattributes, exposure and white balance adjustment, or creating a 3-D avatar."""
William Freeman,Freeman_William,arXiv:1610.07584,https://arxiv.org/abs/1610.07584,"b'Abstract:  We study the problem of 3D object generation. We propose a novel framework,\nnamely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects\nfrom a probabilistic space by leveraging recent advances in volumetric\nconvolutional networks and generative adversarial nets. The benefits of our\nmodel are three-fold: first, the use of an adversarial criterion, instead of\ntraditional heuristic criteria, enables the generator to capture object\nstructure implicitly and to synthesize high-quality 3D objects; second, the\ngenerator establishes a mapping from a low-dimensional probabilistic space to\nthe space of 3D objects, so that we can sample objects without a reference\nimage or CAD models, and explore the 3D object manifold; third, the adversarial\ndiscriminator provides a powerful 3D shape descriptor which, learned without\nsupervision, has wide applications in 3D object recognition. Experiments\ndemonstrate that our method generates high-quality 3D objects, and our\nunsupervisedly learned features achieve impressive performance on 3D object\nrecognition, comparable with those of supervised learning methods.'"
William Freeman,Freeman_William,arXiv:1609.04814,https://arxiv.org/abs/1609.04814,"b'Abstract:  We present results on the variation of 7.7 micron Polycyclic Aromatic\nHydrocarbon (PAH) emission in galaxies spanning a wide range in metallicity at\nz ~ 2. For this analysis, we use rest-frame optical spectra of 476 galaxies at\n1.37 < z < 2.61 from the MOSFIRE Deep Evolution Field (MOSDEF) survey to infer\nmetallicities and ionization states. Spitzer/MIPS 24 micron and Herschel/PACS\n100 and 160 micron observations are used to derive rest-frame 7.7 micron\nluminosities (L(7.7)) and total IR luminosities (L(IR)), respectively. We find\nsignificant trends between the ratio of L(7.7) to L(IR) (and to dust-corrected\nSFR) and both metallicity and [OIII]/[OII] (O32) emission-line ratio. The\nlatter is an empirical proxy for the ionization parameter. These trends\nindicate a paucity of PAH emission in low metallicity environments with harder\nand more intense radiation fields. Additionally, L(7.7)/L(IR) is significantly\nlower in the youngest quartile of our sample (ages of 500 Myr) compared to\nolder galaxies, which may be a result of the delayed production of PAHs by AGB\nstars. The relative strength of L(7.7) to L(IR) is also lower by a factor of ~\n2 for galaxies with masses $M_* < 10^{10}M_{\\odot}$, compared to the more\nmassive ones. We demonstrate that commonly-used conversions of L(7.7) (or 24\nmicron flux density; f(24)) to L(IR) underestimate the IR luminosity by more\nthan a factor of 2 at $M_*$ ~ $10^{9.6-10.0} M_{\\odot}$. We adopt a\nmass-dependent conversion of L(7.7) to L(IR) with L(7.7)/L(IR)= 0.09 and 0.22\nfor $M_* < 10^{10}$ and $> 10^{10} M_{\\odot}$, respectively. Based on the new\nscaling, the SFR-$M_*$ relation has a shallower slope than previously derived.\nOur results also suggest a higher IR luminosity density at z ~ 2 than\npreviously measured, corresponding to a ~ 30% increase in the SFR density.'"
William Freeman,Freeman_William,arXiv:1609.01571,https://arxiv.org/abs/1609.01571,"b'Abstract:  We propose a novel method for template matching in unconstrained\nenvironments. Its essence is the Best-Buddies Similarity (BBS), a useful,\nrobust, and parameter-free similarity measure between two sets of points. BBS\nis based on counting the number of Best-Buddies Pairs (BBPs)--pairs of points\nin source and target sets, where each point is the nearest neighbor of the\nother. BBS has several key features that make it robust against complex\ngeometric deformations and high levels of outliers, such as those arising from\nbackground clutter and occlusions. We study these properties, provide a\nstatistical analysis that justifies them, and demonstrate the consistent\nsuccess of BBS on a challenging real-world dataset while using different types\nof features.'"
William Freeman,Freeman_William,arXiv:1608.07017,https://arxiv.org/abs/1608.07017,"b'Abstract:  The sound of crashing waves, the roar of fast-moving cars -- sound conveys\nimportant information about the objects in our surroundings. In this work, we\nshow that ambient sounds can be used as a supervisory signal for learning\nvisual models. To demonstrate this, we train a convolutional neural network to\npredict a statistical summary of the sound associated with a video frame. We\nshow that, through this process, the network learns a representation that\nconveys information about objects and scenes. We evaluate this representation\non several recognition tasks, finding that its performance is comparable to\nthat of other state-of-the-art unsupervised learning methods. Finally, we show\nthrough visualizations that the network learns units that are selective to\nobjects that are often associated with characteristic sounds.'"
William Freeman,Freeman_William,arXiv:1608.05890,https://arxiv.org/abs/1608.05890,"b'Abstract:  We present results from the MOSFIRE Deep Evolution Field (MOSDEF) survey on\nthe identification, selection biases, and host galaxy properties of 55 X-ray,\nIR and optically-selected active galactic nuclei (AGN) at $1.4 < z < 3.8$. We\nobtain rest-frame optical spectra of galaxies and AGN and use the BPT diagram\nto identify optical AGN. We examine the uniqueness and overlap of the AGN\nidentified at different wavelengths. There is a strong bias against identifying\nAGN at any wavelength in low mass galaxies, and an additional bias against\nidentifying IR AGN in the most massive galaxies. AGN hosts span a wide range of\nstar formation rate (SFR), similar to inactive galaxies once stellar mass\nselection effects are accounted for. However, we find (at $\\sim 2-3\\sigma$\nsignificance) that IR AGN are in less dusty galaxies with relatively higher SFR\nand optical AGN in dusty galaxies with relatively lower SFR. X-ray AGN\nselection does not display a bias with host galaxy SFR. These results are\nconsistent with those from larger studies at lower redshifts. Within\nstar-forming galaxies, once selection biases are accounted for, we find AGN in\ngalaxies with similar physical properties as inactive galaxies, with no\nevidence for AGN activity in particular types of galaxies. This is consistent\nwith AGN being fueled stochastically in any star-forming host galaxy. We do not\ndetect a significant correlation between SFR and AGN luminosity for individual\nAGN hosts, which may indicate the timescale difference between the growth of\ngalaxies and their supermassive black holes.'"
William Freeman,Freeman_William,arXiv:1607.03034,https://arxiv.org/abs/1607.03034,"b'Abstract:  Originally developed to image the shadow region of the central black hole in\nSagittarius A* and in the nearby galaxy M87, the Event Horizon Telescope (EHT)\nprovides deep, very high angular resolution data on other AGN sources too. The\nchallenges of working with EHT data have spurred the development of new image\nreconstruction algorithms. This work briefly reviews the status of the EHT and\nits utility for observing AGN sources, with emphasis on novel imaging\ntechniques that offer the promise of better reconstructions at 1.3 mm and other\nwavelengths.'"
William Freeman,Freeman_William,arXiv:1607.02586,https://arxiv.org/abs/1607.02586,"b'Abstract:  We study the problem of synthesizing a number of likely future frames from a\nsingle input image. In contrast to traditional methods, which have tackled this\nproblem in a deterministic or non-parametric way, we propose a novel approach\nthat models future frames in a probabilistic manner. Our probabilistic model\nmakes it possible for us to sample and synthesize many possible future frames\nfrom a single input image. Future frame synthesis is challenging, as it\ninvolves low- and high-level image and motion understanding. We propose a novel\nnetwork structure, namely a Cross Convolutional Network to aid in synthesizing\nfuture frames; this network structure encodes image and motion information as\nfeature maps and convolutional kernels, respectively. In experiments, our model\nperforms well on synthetic data, such as 2D shapes and animated game sprites,\nas well as on real-wold videos. We also show that our model can be applied to\ntasks such as visual analogy-making, and present an analysis of the learned\nnetwork representations.'"
William Freeman,Freeman_William,arXiv:1606.04107,https://arxiv.org/abs/1606.04107,"b'Abstract:  We present measurements of the electron-temperature based oxygen abundance\nfor a highly star-forming galaxy at z=3.08, COSMOS-1908. This is the highest\nredshift at which [OIII]$\\lambda$4363 has been detected, and the first time\nthat this line has been measured at z>2. We estimate an oxygen abundance of\n12+log(O/H)$=8.00^{+0.13}_{-0.14}$. This galaxy is a low-mass ($10^{9.3}$\nM$_{\\odot}$), highly star-forming ($\\sim50$ M$_{\\odot}$ yr$^{-1}$) system that\nhosts a young stellar population ($\\sim160$ Myr). We investigate the physical\nconditions of the ionized gas in COSMOS-1908 and find that this galaxy has a\nhigh ionization parameter, little nebular reddening ($E(B-V)_{\\rm gas}<0.14$),\nand a high electron density ($n_e\\sim500$ cm$^{-3}$). We compare the ratios of\nstrong oxygen, neon, and hydrogen lines to the direct-method oxygen abundance\nfor COSMOS-1908 and additional star-forming galaxies at z=0-1.8 with\n[OIII]$\\lambda$4363 measurements, and show that galaxies at z$\\sim$1-3 follow\nthe same strong-line correlations as galaxies in the local universe. This\nagreement suggests that the relationship between ionization parameter and O/H\nis similar for z$\\sim$0 and high-redshift galaxies. These results imply that\nmetallicity calibrations based on lines of oxygen, neon, and hydrogen do not\nstrongly evolve with redshift and can reliably estimate abundances out to\nz$\\sim$3, paving the way for robust measurements of the evolution of the\nmass-metallicity relation to high redshift.'"
William Freeman,Freeman_William,arXiv:1606.00469,https://arxiv.org/abs/1606.00469,"b'Abstract:  [Abridged] We present a robust measurement of the rest-frame UV luminosity\nfunction (LF) and its evolution during the peak epoch of cosmic star formation\nat 1<z<3. We use our deep near ultraviolet imaging from WFC3/UVIS on the Hubble\nSpace Telescope (HST) and existing ACS/WFC and WFC3/IR imaging of three lensing\ngalaxy clusters, Abell 2744 and MACSJ0717 from the Hubble Frontier Field survey\nand Abell 1689. We use photometric redshifts to identify 780 ultra-faint\ngalaxies with $M_{UV}$<-12.5 AB mag at 1<z<3. From these samples, we identified\n5 new, faint, multiply imaged systems in A1689. We compute the rest-frame UV LF\nand find the best-fit faint-end slopes of $\\alpha=-1.56\\pm0.04$,\n$\\alpha=-1.72\\pm0.04$ and $\\alpha=-1.94\\pm0.06$ at 1.0<z<1.6, 1.6<z<2.2 and\n2.2<z<3.0, respectively. Our results demonstrate that the UV LF becomes steeper\nfrom z\\sim1.3 to z\\sim2.6 with no sign of a turnover down to $M_{UV}=-14$ AB\nmag. We further derive the UV LFs using the Lyman break ""dropout"" selection and\nconfirm the robustness of our conclusions against different selection\nmethodologies. Because the sample sizes are so large, and extend to such faint\nluminosities, the statistical uncertainties are quite small, and systematic\nuncertainties (due to the assumed size distribution, for example), likely\ndominate. If we restrict our analysis to galaxies and volumes above > 50%\ncompleteness in order to minimize these systematics, we still find that the\nfaint-end slope is steep and getting steeper with redshift, though with\nslightly shallower (less negative) values ($\\alpha=-1.55\\pm0.06$,\n$-1.69\\pm0.07$ and $-1.79\\pm0.08$ for $z\\sim1.3$, 1.9 and 2.6, respectively).\nFinally, we conclude that the faint star-forming galaxies with UV magnitudes of\n$-18.5<M_{UV}<-12.5$ covered in this study, produce the majority (55%-60%) of\nthe unobscured UV luminosity density at 1<z<3.'"
William Freeman,Freeman_William,arXiv:1605.01138,https://arxiv.org/abs/1605.01138,"b'Abstract:  Humans demonstrate remarkable abilities to predict physical events in complex\nscenes. Two classes of models for physical scene understanding have recently\nbeen proposed: ""Intuitive Physics Engines"", or IPEs, which posit that people\nmake predictions by running approximate probabilistic simulations in causal\nmental models similar in nature to video-game physics engines, and memory-based\nmodels, which make judgments based on analogies to stored experiences of\npreviously encountered scenes and physical outcomes. Versions of the latter\nhave recently been instantiated in convolutional neural network (CNN)\narchitectures. Here we report four experiments that, to our knowledge, are the\nfirst rigorous comparisons of simulation-based and CNN-based models, where both\napproaches are concretely instantiated in algorithms that can run on raw image\ninputs and produce as outputs physical judgments such as whether a stack of\nblocks will fall. Both approaches can achieve super-human accuracy levels and\ncan quantitatively predict human judgments to a similar degree, but only the\nsimulation-based models generalize to novel situations in ways that people do,\nand are qualitatively consistent with systematic perceptual illusions and\njudgment asymmetries that people show.'"
William Freeman,Freeman_William,arXiv:1604.08685,https://arxiv.org/abs/1604.08685,"b'Abstract:  Understanding 3D object structure from a single image is an important but\ndifficult task in computer vision, mostly due to the lack of 3D object\nannotations in real images. Previous work tackles this problem by either\nsolving an optimization task given 2D keypoint positions, or training on\nsynthetic data with ground truth 3D information. In this work, we propose 3D\nINterpreter Network (3D-INN), an end-to-end framework which sequentially\nestimates 2D keypoint heatmaps and 3D object structure, trained on both real\n2D-annotated images and synthetic 3D data. This is made possible mainly by two\ntechnical innovations. First, we propose a Projection Layer, which projects\nestimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D\nstructural parameters supervised by 2D annotations on real images. Second,\nheatmaps of keypoints serve as an intermediate representation connecting real\nand synthetic data, enabling 3D-INN to benefit from the variation and abundance\nof synthetic 3D objects, without suffering from the difference between the\nstatistics of real and synthesized images due to imperfect rendering. The\nnetwork achieves state-of-the-art performance on both 2D keypoint estimation\nand 3D structure recovery. We also show that the recovered 3D information can\nbe used in other vision applications, such as 3D rendering and image retrieval.'"
William Freeman,Freeman_William,arXiv:1603.02284,https://arxiv.org/abs/1603.02284,"b'Abstract:  We present the first direct comparison between Balmer line and panchromatic\nSED-based SFRs for z~2 galaxies. For this comparison we used 17 star-forming\ngalaxies selected from the MOSFIRE Deep Evolution Field (MOSDEF) survey, with\n$3\\sigma$ detections for H$\\alpha$ and at least two IR bands (Spitzer/MIPS\n24$\\mu$m and Herschel/PACS 100 and 160$\\mu$m, and in some cases Herschel/SPIRE\n250, 350, and 500$\\mu$m). The galaxies have total IR (8-1000$\\mu$m)\nluminosities of $\\sim10^{11.4}-10^{12.4}\\,\\textrm{L}_\\odot$ and star-formation\nrates (SFRs) of $\\sim30-250\\,\\textrm{M}_\\odot\\,\\mathrm{yr^{-1}}$. We fit the\nUV-to-far-IR SEDs with flexible stellar population synthesis (FSPS) models -\nwhich include both stellar and dust emission - and compare the inferred SFRs\nwith the SFR(H$\\alpha$,H$\\beta$) values corrected for dust attenuation using\nBalmer decrements. The two SFRs agree with a scatter of 0.17 dex. Our results\nimply that the Balmer decrement accurately predicts the obscuration of the\nnebular lines and can be used to robustly calculate SFRs for star-forming\ngalaxies at z~2 with SFRs up to $\\sim200\\,\\textrm{M}_\\odot\\,\\mathrm{yr^{-1}}$.\nWe also use our data to assess SFR indicators based on modeling the\nUV-to-mid-IR SEDs or by adding SFR(UV) and SFR(IR), for which the latter is\nbased on the mid-IR only or on the full IR SED. All these SFRs show a poorer\nagreement with SFR(H$\\alpha$,H$\\beta$) and in some cases large systematic\nbiases are observed. Finally, we show that the SFR and dust attenuation derived\nfrom the UV-to-near-IR SED alone are unbiased when assuming a delayed\nexponentially declining star-formation history.'"
William Freeman,Freeman_William,arXiv:1512.08512,https://arxiv.org/abs/1512.08512,"b'Abstract:  Objects make distinctive sounds when they are hit or scratched. These sounds\nreveal aspects of an object\'s material properties, as well as the actions that\nproduced them. In this paper, we propose the task of predicting what sound an\nobject makes when struck as a way of studying physical interactions within a\nvisual scene. We present an algorithm that synthesizes sound from silent videos\nof people hitting and scratching objects with a drumstick. This algorithm uses\na recurrent neural network to predict sound features from videos and then\nproduces a waveform from these features with an example-based synthesis\nprocedure. We show that the sounds predicted by our model are realistic enough\nto fool participants in a ""real or fake"" psychophysical experiment, and that\nthey convey significant information about material properties and physical\ninteractions.'"
William Freeman,Freeman_William,arXiv:1512.01413,https://arxiv.org/abs/1512.01413,"b'Abstract:  Very long baseline interferometry (VLBI) is a technique for imaging celestial\nradio emissions by simultaneously observing a source from telescopes\ndistributed across Earth. The challenges in reconstructing images from fine\nangular resolution VLBI data are immense. The data is extremely sparse and\nnoisy, thus requiring statistical image models such as those designed in the\ncomputer vision community. In this paper we present a novel Bayesian approach\nfor VLBI image reconstruction. While other methods often require careful tuning\nand parameter selection for different types of data, our method (CHIRP)\nproduces good results under different settings such as low SNR or extended\nemission. The success of our method is demonstrated on realistic synthetic\nexperiments as well as publicly available real data. We present this problem in\na way that is accessible to members of the community, and provide a dataset\nwebsite (vlbiimaging.csail.mit.edu) that facilitates controlled comparisons\nacross algorithms.'"
William Freeman,Freeman_William,arXiv:1511.03272,https://arxiv.org/abs/1511.03272,"b'Abstract:  We present H$\\alpha$ gas kinematics for 178 star-forming galaxies at z~2 from\nthe MOSFIRE Deep Evolution Field survey. We have developed models to interpret\nthe kinematic measurements from fixed-angle multi-object spectroscopy, using\nstructural parameters derived from CANDELS HST/F160W imaging. For 35 galaxies\nwe measure resolved rotation with a median $(V/\\sigma_{V,0})_{R_E}=2.1$. We\nderive dynamical masses from the kinematics and sizes and compare them to\nbaryonic masses, with gas masses estimated from dust-corrected H$\\alpha$ star\nformation rates (SFRs) and the Kennicutt-Schmidt relation. When assuming that\ngalaxies with and without observed rotation have the same median\n$(V/\\sigma_{V,0})_{R_E}$, we find good agreement between the dynamical and\nbaryonic masses, with a scatter of $\\sigma_{RMS}=0.34$dex and a median offset\nof $\\Delta\\log_{10}M=0.04$dex. This comparison implies a low dark matter\nfraction (8% within an effective radius) for a Chabrier initial mass function\n(IMF), and disfavors a Salpeter IMF. Moreover, the requirement that\n$M_{dyn}/M_{baryon}$ should be independent of inclination yields a median value\nof $(V/\\sigma_{V,0})_{R_E}=2.1$ for galaxies without observed rotation. If\ninstead we treat the galaxies without detected rotation as early-type galaxies,\nthe masses are also in reasonable agreement ($\\Delta\\log_{10}M=-0.07$dex,\n$\\sigma_{RMS}=0.37$dex). The inclusion of gas masses is critical in this\ncomparison; if gas masses are excluded there is an increasing trend of\n$M_{dyn}/M_{*}$ with higher specific SFR (SSFR). Furthermore, we find\nindications that $V/\\sigma$ decreases with increasing H$\\alpha$ SSFR for our\nfull sample, which may reflect disk settling. We also study the Tully-Fisher\nrelation and find that at fixed stellar mass\n$S_{0.5}=(0.5V_{2.2}^2+\\sigma_{V,0}^2)^{1/2}$ was higher at earlier times. At\nfixed baryonic mass, we observe the opposite trend. [abridged]'"
William Freeman,Freeman_William,arXiv:1509.03636,https://arxiv.org/abs/1509.03636,"b'Abstract:  Using observations from the MOSFIRE Deep Evolution Field (MOSDEF) survey, we\ninvestigate the physical conditions of star-forming regions in $z\\sim2.3$\ngalaxies, specifically the electron density and ionization state. From\nmeasurements of the [O II]$\\lambda\\lambda$3726,3729 and [S\nII]$\\lambda\\lambda$6716,6731 doublets, we find a median electron density of\n$\\sim250$ cm$^{-3}$ at $z\\sim2.3$, an increase of an order of magnitude\ncompared to measurements of galaxies at $z\\sim0$. While $z\\sim2.3$ galaxies are\noffset towards significantly higher O$_{32}$ values relative to local galaxies\nat fixed stellar mass, we find that the high-redshift sample follows a similar\ndistribution to the low-metallicity tail of the local distribution in the\nO$_{32}$ vs. R$_{23}$ and O3N2 diagrams. Based on these results, we propose\nthat $z\\sim2.3$ star-forming galaxies have the same ionization parameter as\nlocal galaxies at fixed metallicity. In combination with simple photoionization\nmodels, the position of local and $z\\sim2.3$ galaxies in excitation diagrams\nsuggests that there is no significant change in the hardness of the ionizing\nspectrum at fixed metallicity from $z\\sim0$ to $z\\sim2.3$. We find that\n$z\\sim2.3$ galaxies show no offset compared to low-metallicity local galaxies\nin emission line ratio diagrams involving only lines of hydrogen, oxygen, and\nsulfur, but show a systematic offset in diagrams involving [N II]$\\lambda$6584.\nWe conclude that the offset of $z\\sim2.3$ galaxies from the local star-forming\nsequence in the [N II] BPT diagram is primarily driven by elevated N/O at fixed\nO/H compared to local galaxies. These results suggest that the local gas-phase\nand stellar metallicity sets the ionization state of star-forming regions at\n$z\\sim0$ and $z\\sim2$.'"
William Freeman,Freeman_William,arXiv:1507.03017,https://arxiv.org/abs/1507.03017,"b'Abstract:  We present results on the star-formation rate (SFR) versus stellar mass\n($M_*$) relation (i.e., the ""main sequence"") among star-forming galaxies at\n1.37$\\leq$$z$$\\leq$2.61 using the MOSFIRE Deep Evolution Field (MOSDEF) survey.\nBased on a sample of 261 galaxies with H$\\alpha$ and H$\\beta$ spectroscopy, we\nhave estimated robust dust-corrected instantaneous SFRs over a large range in\n$M_*$ ($\\sim10^{9.5}-10^{11.5}M_\\odot$). We find a correlation between\nlog(SFR(H$\\alpha$)) and log($M_*$) with a slope of 0.65$\\pm$0.08\n(0.58$\\pm$0.10) at 1.4<z<2.6 (2.1<z<2.6). We find that different assumptions\nfor the dust correction, such as using the color-excess of the stellar\ncontinuum to correct the nebular lines, sample selection biases against red\nstar-forming galaxies, and not accounting for Balmer absorption can yield\nsteeper slopes of the log(SFR)-log($M_*$) relation. Our sample is immune from\nthese biases as it is rest-frame optically selected, H$\\alpha$ and H$\\beta$ are\ncorrected for Balmer absorption, and the H$\\alpha$ luminosity is dust-corrected\nusing the nebular color-excess computed from the Balmer decrement. The scatter\nof the log(SFR(H$\\alpha$))-log($M_*$) relation, after accounting for the\nmeasurement uncertainties, is 0.31 dex at 2.1<z<2.6, which is 0.05 dex larger\nthan the scatter in log(SFR(UV))-log($M_*$). Based on comparisons to a\nsimulated SFR-$M_*$ relation with some intrinsic scatter, we argue that in the\nabsence of direct measurements of galaxy-to-galaxy variations in the\nattenuation/extinction curves and the IMF, one cannot use the difference in the\nscatter of the SFR(H$\\alpha$)- and SFR(UV)-$M_*$ relations to constrain the\nstochasticity of star formation in high-redshift galaxies.'"
William Freeman,Freeman_William,arXiv:1507.02379,https://arxiv.org/abs/1507.02379,"b'Abstract:  Convolutional Neural Network (CNN) has been successful in image recognition\ntasks, and recent works shed lights on how CNN separates different classes with\nthe learned inter-class knowledge through visualization. In this work, we\ninstead visualize the intra-class knowledge inside CNN to better understand how\nan object class is represented in the fully-connected layers.\nTo invert the intra-class knowledge into more interpretable images, we\npropose a non-parametric patch prior upon previous CNN visualization models.\nWith it, we show how different ""styles"" of templates for an object class are\norganized by CNN in terms of location and content, and represented in a\nhierarchical and ensemble way. Moreover, such intra-class knowledge can be used\nin many interesting applications, e.g. style-based image retrieval and\nstyle-based object completion.'"
William Freeman,Freeman_William,arXiv:1504.02782,https://arxiv.org/abs/1504.02782,"b'Abstract:  We present results on the dust attenuation curve of z~2 galaxies using early\nobservations from the MOSFIRE Deep Evolution Field (MOSDEF) survey. Our sample\nconsists of 224 star-forming galaxies with nebular spectroscopic redshifts in\nthe range z= 1.36-2.59 and high S/N measurements of, or upper limits on, the\nH-alpha and H-beta emission lines obtained with Keck/MOSFIRE. We construct\ncomposite SEDs of galaxies in bins of specific SFR and Balmer optical depth in\norder to directly constrain the dust attenuation curve from the UV through\nnear-IR for typical star-forming galaxies at high redshift. Our results imply\nan attenuation curve that is very similar to the SMC extinction curve at\nwavelengths redward of 2500 Angstroms. At shorter wavelengths, the shape of the\ncurve is identical to that of the Calzetti relation, but with a lower\nnormalization (R_V). Hence, the new attenuation curve results in SFRs that are\n~20% lower, and log stellar masses that are 0.16 dex lower, than those obtained\nwith the Calzetti attenuation curve. Moreover, we find that the difference in\nthe reddening---and the total attenuation---of the ionized gas and stellar\ncontinuum correlates strongly with SFR, such that for dust-corrected SFRs\nlarger than 20 Msun/yr assuming a Chabrier IMF, the nebular emission lines\nsuffer an increasing degree of obscuration relative to the continuum. A simple\nmodel that can account for these trends is one in which the UV through optical\nstellar continuum is dominated by a population of less reddened stars, while\nthe nebular line and bolometric luminosities become increasingly dominated by\ndustier stellar populations for galaxies with large SFRs, as a result of the\nincreased dust enrichment that accompanies such galaxies. Consequently, UV- and\nSED-based SFRs may underestimate the total SFR at even modest levels of ~20\nMsun/yr. [Abridged]'"
William Freeman,Freeman_William,arXiv:1412.1835,https://arxiv.org/abs/1412.1835,"b'Abstract:  In this paper we present the MOSFIRE Deep Evolution Field (MOSDEF) survey.\nThe MOSDEF survey aims to obtain moderate-resolution (R=3000-3650) rest-frame\noptical spectra (~3700-7000 Angstrom) for ~1500 galaxies at 1.37<z<3.80 in\nthree well-studied CANDELS fields: AEGIS, COSMOS, and GOODS-N. Targets are\nselected in three redshift intervals: 1.37<z<1.70, 2.09<z<2.61, and\n2.95<z<3.80, down to fixed H_AB (F160W) magnitudes of 24.0, 24.5 and 25.0,\nrespectively, using the photometric and spectroscopic catalogs from the 3D-HST\nsurvey. We target both strong nebular emission lines (e.g., [OII], Hbeta,\n[OIII], 5008, Halpha, [NII], and [SII]) and stellar continuum and absorption\nfeatures (e.g., Balmer lines, Ca-II H and K, Mgb, 4000 Angstrom break). Here we\npresent an overview of our survey, the observational strategy, the data\nreduction and analysis, and the sample characteristics based on spectra\nobtained during the first 24 nights. To date, we have completed 21 masks,\nobtaining spectra for 591 galaxies. For ~80% of the targets we derive a robust\nredshift from either emission or absorption lines. In addition, we confirm 55\nadditional galaxies, which were serendipitously detected. The MOSDEF galaxy\nsample includes unobscured star-forming, dusty star-forming, and quiescent\ngalaxies and spans a wide range in stellar mass (~10^9-10^11.5 Msol) and star\nformation rate (~10^0-10^3 Msol/yr). The spectroscopically confirmed sample is\nroughly representative of an H-band limited galaxy sample at these redshifts.\nWith its large sample size, broad diversity in galaxy properties, and wealth of\navailable ancillary data, MOSDEF will transform our understanding of the\nstellar, gaseous, metal, dust, and black hole content of galaxies during the\ntime when the universe was most active.'"
William Freeman,Freeman_William,arXiv:1409.7071,https://arxiv.org/abs/1409.7071,"b'Abstract:  We present results on the excitation properties of z~2.3 galaxies using early\nobservations from the MOSFIRE Deep Evolution Field (MOSDEF) Survey. With its\ncoverage of the full suite of strong rest-frame optical emission lines, MOSDEF\nprovides an unprecedented view of the rest-frame optical spectra of a\nrepresentative sample of distant star-forming galaxies. We investigate the\nlocations of z~2.3 MOSDEF galaxies in multiple emission-line diagnostic\ndiagrams. These include the [OIII]/Hb vs. [NII]/Ha and [OIII]/Hb vs. [SII]/Ha\n""BPT"" diagrams, as well as the O_32 vs. R_23 excitation diagram. We recover the\nwell-known offset in the star-forming sequence of high-redshift galaxies in the\n[OIII]/Hb vs. [NII]/Ha BPT diagram relative to SDSS star-forming galaxies.\nHowever, the shift for our rest-frame optically selected sample is less\nsignificant than for rest-frame-UV selected and emission-line selected galaxies\nat z~2. Furthermore, we find that the offset is mass-dependent, only appearing\nwithin the low-mass half of the z~2.3 MOSDEF sample, where galaxies are shifted\ntowards higher [NII]/Ha at fixed [OIII]/Hb. Within the [OIII]/Hb vs. [SII]/Ha\nand O_32 vs. R_23 diagrams, we find that z~2.3 galaxies are distributed like\nlocal ones, and therefore attribute the shift in the [OIII]/Hb vs. [NII]/Ha BPT\ndiagram to elevated N/O abundance ratios among lower-mass (M_*<10^10 M_sun)\nhigh-redshift galaxies. The variation in N/O ratios calls into question the use\nat high redshift of oxygen abundance indicators based on nitrogen lines, but\nthe apparent invariance with redshift of the excitation sequence in the O_32\nvs. R_23 diagram paves the way for using the combination of O_32 and R_23 as an\nunbiased metallicity indicator over a wide range in redshift. This indicator\nwill allow for an accurate characterization of the shape and normalization of\nthe mass-metallicity relationship over more than 10 Gyr.'"
William Freeman,Freeman_William,arXiv:1409.6522,https://arxiv.org/abs/1409.6522,"b'Abstract:  We present results from the MOSFIRE Deep Evolution Field (MOSDEF) survey on\nrest-frame optical AGN identification and completeness at z~2.3. With our\nsample of 50 galaxies and 10 X-ray and IR-selected AGN with measured H-beta,\n[OIII], H-alpha, and [NII] emission lines, we investigate the location of AGN\nin the BPT, MEx (mass-excitation), and CEx (color-excitation) diagrams. We find\nthat the BPT diagram works well to identify AGN at z~2.3 and that the z~0\nAGN/star-forming galaxy classifications do not need to shift substantially at\nz~2.3 to robustly separate these populations. However, the MEx diagram fails to\nidentify all of the AGN identified in the BPT diagram, and the CEx diagram is\nsubstantially contaminated at high redshift. We further show that AGN samples\nselected using the BPT diagram have selection biases in terms of both host\nstellar mass and stellar population, in that AGN in low mass and/or high\nspecific star formation rate galaxies are difficult to identify using the BPT\ndiagram. These selection biases become increasingly severe at high redshift,\nsuch that optically-selected AGN samples at high redshift will necessarily be\nincomplete. We also find that the gas in the narrow-line region appears to be\nmore enriched than gas in the host galaxy for at least some MOSDEF AGN.\nHowever, AGN at z~2 are generally less enriched than local AGN with the same\nhost stellar mass.'"
William Freeman,Freeman_William,arXiv:1409.4690,https://arxiv.org/abs/1409.4690,"b'Abstract:  The image of the emission surrounding the black hole in the center of the\nMilky Way is predicted to exhibit the imprint of general relativistic (GR)\neffects, including the existence of a shadow feature and a photon ring of\ndiameter ~50 microarcseconds. Structure on these scales can be resolved by\nmillimeter-wavelength very long baseline interferometry (VLBI). However,\nstrong-field GR features of interest will be blurred at lambda >= 1.3 mm due to\nscattering by interstellar electrons. The scattering properties are well\nunderstood over most of the relevant range of baseline lengths, suggesting that\nthe scattering may be (mostly) invertible. We simulate observations of a model\nimage of Sgr A* and demonstrate that the effects of scattering can indeed be\nmitigated by correcting the visibilities before reconstructing the image. This\ntechnique is also applicable to Sgr A* at longer wavelengths.'"
William Freeman,Freeman_William,arXiv:1408.2521,https://arxiv.org/abs/1408.2521,"b'Abstract:  We present results on the z~2.3 mass-metallicity relation (MZR) using early\nobservations from the MOSFIRE Deep Evolution Field (MOSDEF) survey. We use an\ninitial sample of 87 star-forming galaxies with spectroscopic coverage of\nH\\beta, [OIII]\\lambda 5007, H\\alpha, and [NII]\\lambda 6584 rest-frame optical\nemission lines, and estimate the gas-phase oxygen abundance based on the N2 and\nO3N2 strong-line indicators. We find a positive correlation between stellar\nmass and metallicity among individual z~2.3 galaxies using both the N2 and O3N2\nindicators. We also measure the emission-line ratios and corresponding oxygen\nabundances for composite spectra in bins of stellar mass. Among composite\nspectra, we find a monotonic increase in metallicity with increasing stellar\nmass, offset ~0.15-0.3 dex below the local MZR. When the sample is divided at\nthe median star-formation rate (SFR), we do not observe significant SFR\ndependence of the z~2.3 MZR among either individual galaxies or composite\nspectra. We furthermore find that z~2.3 galaxies have metallicities ~0.1 dex\nlower at a given stellar mass and SFR than is observed locally. This offset\nsuggests that high-redshift galaxies do not fall on the local ""fundamental\nmetallicity relation"" among stellar mass, metallicity, and SFR, and may provide\nevidence of a phase of galaxy growth in which the gas reservoir is built up due\nto inflow rates that are higher than star-formation and outflow rates. However,\nrobust conclusions regarding the gas-phase oxygen abundances of high-redshift\ngalaxies await a systematic reappraisal of the application of locally\ncalibrated metallicity indicators at high redshift.'"
William Freeman,Freeman_William,arXiv:1408.1420,https://arxiv.org/abs/1408.1420,"b'Abstract:  We present deep spectroscopy of 17 very low mass (M* ~ 2.0x10^6 Msun to\n1.4x10^9 Msun) and low luminosity (M_UV ~ -13.7 to -19.9) gravitationally\nlensed galaxies in the redshift range z~1.5-3.0. Deep rest-frame ultraviolet\nspectra reveal large equivalent width emission from numerous lines (NIV],\nOIII], CIV, Si III], CIII]) which are rarely seen in individual spectra of more\nmassive star forming galaxies. CIII] is detected in 16 of 17 low mass star\nforming systems with rest-frame equivalent widths as large as 13.5 Angstroms.\nNebular CIV emission is present in the most extreme CIII] emitters, requiring\nan ionizing source capable of producing a substantial component of photons with\nenergies in excess of 47.9 eV. Photoionization models support a picture whereby\nthe large equivalent widths are driven by the increased electron temperature\nand enhanced ionizing output arising from metal poor gas and stars, young\nstellar populations, and large ionization parameters. The young ages implied by\nthe emission lines and continuum SEDs indicate that the extreme line emitters\nin our sample are in the midst of a significant upturn in their star formation\nactivity. The low stellar masses, blue UV colors, and large sSFRs of our sample\nare similar to those of typical z>6 galaxies. Given the strong attenuation of\nLy-alpha in z>6 galaxies we suggest that CIII] is likely to provide our best\nprobe of early star forming galaxies with ground-based spectrographs and one of\nthe most efficient means of confirming z>10 galaxies with the James Webb Space\nTelescope.'"
William Freeman,Freeman_William,arXiv:1305.2413,https://arxiv.org/abs/1305.2413,"b'Abstract:  We identified the z~2 Lyman break galaxies using deep HST ultraviolet\n(F275W/F336W) imaging of Abell 1689. Because of the imaging depth and the large\nmagnification provided by the cluster, we detect galaxies 100x fainter (-19.5<\nM_1500 <-13) than previous surveys at this redshift. We are able to calculate\nthe intrinsic sensitivity of the observations as a function of source plane\nposition, allowing determinations of effective volume as a function of\nluminosity. We fit the faint-end slope of the luminosity function to be alpha =\n-1.74 +/-0.08, consistent with the values obtained for 2.5 < z < 6. There is no\nturnover in the luminosity function down to MUV = -13. The trend of\nincreasingly redder UV spectral slopes with luminosity at higher redshifts is\nobserved in our sample, but with redder slopes at all luminosities and average\nreddening of < E(B - V) >= 0.15. We assume the stars in these galaxies are\nmetal poor (0.2Z_sun) compared to their brighter counterparts (Z_sun),\nresulting in bluer assumed intrinsic UV slopes and larger derived dust\nextinction. The total UV luminosity density at z ~ 2 is 4.31x10^26\nerg/s/Hz/Mpc^3, more than 70% of which is emitted by galaxies in the luminosity\nrange of our sample. We determine the star formation rate density at z ~ 2\n(assuming constant dust extinction correction of 4.2 over all luminosities and\na Kroupa IMF) of 0.148 M/yr/Mpc^3, significantly higher than previous\ndeterminations because of the additional population of fainter galaxies and the\nlarger dust correction factors.[abridged]'"
William Freeman,Freeman_William,arXiv:1210.4856,https://arxiv.org/abs/1210.4856,"b'Abstract:  The recent proliferation of richly structured probabilistic models raises the\nquestion of how to automatically determine an appropriate model for a dataset.\nWe investigate this question for a space of matrix decomposition models which\ncan express a variety of widely used models from unsupervised learning. To\nenable model selection, we organize these models into a context-free grammar\nwhich generates a wide variety of structures through the compositional\napplication of a few simple rules. We use our grammar to generically and\nefficiently infer latent components and estimate predictive likelihood for\nnearly 2500 structures using a small toolbox of reusable algorithms. Using a\ngreedy search over our grammar, we automatically choose the decomposition\nstructure from raw data by evaluating only a small fraction of all models. The\nproposed method typically finds the correct structure for synthetic data and\nbacks off gracefully to simpler models under heavy noise. It learns sensible\nstructures for datasets as diverse as image patches, motion capture, 20\nQuestions, and U.S. Senate votes, all using exactly the same code.'"
William Freeman,Freeman_William,arXiv:0901.4275,https://arxiv.org/abs/0901.4275,"b'Abstract:  Compressed sensing is a recent set of mathematical results showing that\nsparse signals can be exactly reconstructed from a small number of linear\nmeasurements. Interestingly, for ideal sparse signals with no measurement\nnoise, random measurements allow perfect reconstruction while measurements\nbased on principal component analysis (PCA) or independent component analysis\n(ICA) do not. At the same time, for other signal and noise distributions, PCA\nand ICA can significantly outperform random projections in terms of enabling\nreconstruction from a small number of measurements. In this paper we ask: given\nthe distribution of signals we wish to measure, what are the optimal set of\nlinear projections for compressed sensing? We consider the problem of finding a\nsmall number of linear projections that are maximally informative about the\nsignal. Formally, we use the InfoMax criterion and seek to maximize the mutual\ninformation between the signal, x, and the (possibly noisy) projection y=Wx. We\nshow that in general the optimal projections are not the principal components\nof the data nor random projections, but rather a seemingly novel set of\nprojections that capture what is still uncertain about the signal, given the\nknowledge of distribution. We present analytic solutions for certain special\ncases including natural images. In particular, for natural images, the\nnear-optimal projections are bandwise random, i.e., incoherent to the sparse\nbases at a particular frequency band but with more weights on the\nlow-frequencies, which has a physical relation to the multi-resolution\nrepresentation of images.'"
James Fujimoto,Fujimoto_James,arXiv:1802.03943,https://arxiv.org/abs/1802.03943,"b'Abstract:  This paper introduces an universal and structure-preserving regularization\nterm, called quantile sparse image (QuaSI) prior. The prior is suitable for\ndenoising images from various medical imaging modalities. We demonstrate its\neffectiveness on volumetric optical coherence tomography (OCT) and computed\ntomography (CT) data, which show different noise and image characteristics. OCT\noffers high-resolution scans of the human retina but is inherently impaired by\nspeckle noise. CT on the other hand has a lower resolution and shows\nhigh-frequency noise. For the purpose of denoising, we propose a variational\nframework based on the QuaSI prior and a Huber data fidelity model that can\nhandle 3-D and 3-D+t data. Efficient optimization is facilitated through the\nuse of an alternating direction method of multipliers (ADMM) scheme and the\nlinearization of the quantile filter. Experiments on multiple datasets\nemphasize the excellent performance of the proposed method.'"
James Fujimoto,Fujimoto_James,arXiv:1703.02942,https://arxiv.org/abs/1703.02942,"b'Abstract:  Optical coherence tomography (OCT) enables high-resolution and non-invasive\n3D imaging of the human retina but is inherently impaired by speckle noise.\nThis paper introduces a spatio-temporal denoising algorithm for OCT data on a\nB-scan level using a novel quantile sparse image (QuaSI) prior. To remove\nspeckle noise while preserving image structures of diagnostic relevance, we\nimplement our QuaSI prior via median filter regularization coupled with a Huber\ndata fidelity model in a variational approach. For efficient energy\nminimization, we develop an alternating direction method of multipliers (ADMM)\nscheme using a linearization of median filtering. Our spatio-temporal method\ncan handle both, denoising of single B-scans and temporally consecutive\nB-scans, to gain volumetric OCT data with enhanced signal-to-noise ratio. Our\nalgorithm based on 4 B-scans only achieved comparable performance to averaging\n13 B-scans and outperformed other current denoising methods.'"
Robert Gallager,Gallager_Robert,arXiv:0812.2709,https://arxiv.org/abs/0812.2709,"b'Abstract:  Schalkwijk and Kailath (1966) developed a class of block codes for Gaussian\nchannels with ideal feedback for which the probability of decoding error\ndecreases as a second-order exponent in block length for rates below capacity.\nThis well-known but surprising result is explained and simply derived here in\nterms of a result by Elias (1956) concerning the minimum mean-square distortion\nachievable in transmitting a single Gaussian random variable over multiple uses\nof the same Gaussian channel. A simple modification of the Schalkwijk-Kailath\nscheme is then shown to have an error probability that decreases with an\nexponential order which is linearly increasing with block length. In the\ninfinite bandwidth limit, this scheme produces zero error probability using\nbounded expected energy at all rates below capacity. A lower bound on error\nprobability for the finite bandwidth case is then derived in which the error\nprobability decreases with an exponential order which is linearly increasing in\nblock length at the same rate as the upper bound.'"
David Gifford,Gifford_David,arXiv:1810.03805,https://arxiv.org/abs/1810.03805,"b""Abstract:  Local explanation frameworks aim to rationalize particular decisions made by\na black-box prediction model. Existing techniques are often restricted to a\nspecific type of predictor or based on input saliency, which may be undesirably\nsensitive to factors unrelated to the model's decision making process. We\ninstead propose sufficient input subsets that identify minimal subsets of\nfeatures whose observed values alone suffice for the same decision to be\nreached, even if all other input feature values are missing. General principles\nthat globally govern a model's decision-making can also be revealed by\nsearching for clusters of such input patterns across many data points. Our\napproach is conceptually straightforward, entirely model-agnostic, simply\nimplemented using instance-wise backward selection, and able to produce more\nconcise rationales than existing techniques. We demonstrate the utility of our\ninterpretation method on various neural network models trained on text, image,\nand genomic data."""
David Gifford,Gifford_David,arXiv:1511.04486,https://arxiv.org/abs/1511.04486,"b'Abstract:  We present a nonparametric framework to model a short sequence of probability\ndistributions that vary both due to underlying effects of sequential\nprogression and confounding noise. To distinguish between these two types of\nvariation and estimate the sequential-progression effects, our approach\nleverages an assumption that these effects follow a persistent trend. This work\nis motivated by the recent rise of single-cell RNA-sequencing experiments over\na brief time course, which aim to identify genes relevant to the progression of\na particular biological process across diverse cell populations. While\nclassical statistical tools focus on scalar-response regression or\norder-agnostic differences between distributions, it is desirable in this\nsetting to consider both the full distributions as well as the structure\nimposed by their ordering. We introduce a new regression model for ordinal\ncovariates where responses are univariate distributions and the underlying\nrelationship reflects consistent changes in the distributions over increasing\nlevels of the covariate. This concept is formalized as a ""trend"" in\ndistributions, which we define as an evolution that is linear under the\nWasserstein metric. Implemented via a fast alternating projections algorithm,\nour method exhibits numerous strengths in simulations and analyses of\nsingle-cell gene expression data.'"
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1805.02351,https://arxiv.org/abs/1805.02351,"b'Abstract:  In this paper we study the fine-grained complexity of finding exact and\napproximate solutions to problems in P. Our main contribution is showing\nreductions from exact to approximate solution for a host of such problems.\nAs one (notable) example, we show that the Closest-LCS-Pair problem (Given\ntwo sets of strings $A$ and $B$, compute exactly the maximum $\\textsf{LCS}(a,\nb)$ with $(a, b) \\in A \\times B$) is equivalent to its approximation version\n(under near-linear time reductions, and with a constant approximation factor).\nMore generally, we identify a class of problems, which we call BP-Pair-Class,\ncomprising both exact and approximate solutions, and show that they are all\nequivalent under near-linear time reductions.\nExploring this class and its properties, we also show:\n$\\bullet$ Under the NC-SETH assumption (a significantly more relaxed\nassumption than SETH), solving any of the problems in this class requires\nessentially quadratic time.\n$\\bullet$ Modest improvements on the running time of known algorithms\n(shaving log factors) would imply that NEXP is not in non-uniform\n$\\textsf{NC}^1$.\n$\\bullet$ Finally, we leverage our techniques to show new barriers for\ndeterministic approximation algorithms for LCS.\nAt the heart of these new results is a deep connection between interactive\nproof systems for bounded-space computations and the fine-grained complexity of\nexact and approximate solutions to problems in P. In particular, our results\nbuild on the proof techniques from the classical IP = PSPACE result.'"
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1803.02540,https://arxiv.org/abs/1803.02540,"b'Abstract:  We introduce a new coordination problem in distributed computing that we call\nthe population stability problem. A system of agents each with limited memory\nand communication, as well as the ability to replicate and self-destruct, is\nsubjected to attacks by a worst-case adversary that can at a bounded rate (1)\ndelete agents chosen arbitrarily and (2) insert additional agents with\narbitrary initial state into the system. The goal is perpetually to maintain a\npopulation whose size is within a constant factor of the target size $N$. The\nproblem is inspired by the ability of complex biological systems composed of a\nmultitude of memory-limited individual cells to maintain a stable population\nsize in an adverse environment. Such biological mechanisms allow organisms to\nheal after trauma or to recover from excessive cell proliferation caused by\ninflammation, disease, or normal development.\nWe present a population stability protocol in a communication model that is a\nsynchronous variant of the population model of Angluin et al. In each round,\npairs of agents selected at random meet and exchange messages, where at least a\nconstant fraction of agents is matched in each round. Our protocol uses\nthree-bit messages and $\\omega(\\log^2 N)$ states per agent. We emphasize that\nour protocol can handle an adversary that can both insert and delete agents, a\nsetting in which existing approximate counting techniques do not seem to apply.\nThe protocol relies on a novel coloring strategy in which the population size\nis encoded in the variance of the distribution of colors. Individual agents can\nlocally obtain a weak estimate of the population size by sampling from the\ndistribution, and make individual decisions that robustly maintain a stable\nglobal population size.'"
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1706.04641,https://arxiv.org/abs/1706.04641,"b'Abstract:  We introduce pseudo-deterministic interactive proofs (psdAM): interactive\nproof systems for search problems where the verifier is guaranteed with high\nprobability to output the same output on different executions. As in the case\nwith classical interactive proofs, the verifier is a probabilistic polynomial\ntime algorithm interacting with an untrusted powerful prover.\nWe view pseudo-deterministic interactive proofs as an extension of the study\nof pseudo-deterministic randomized polynomial time algorithms: the goal of the\nlatter is to find canonical solutions to search problems whereas the goal of\nthe former is to prove that a solution to a search problem is canonical to a\nprobabilistic polynomial time verifier. Alternatively, one may think of the\npowerful prover as aiding the probabilistic polynomial time verifier to find\ncanonical solutions to search problems, with high probability over the\nrandomness of the verifier. The challenge is that pseudo-determinism should\nhold not only with respect to the randomness, but also with respect to the\nprover: a malicious prover should not be able to cause the verifier to output a\nsolution other than the unique canonical one.'"
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1601.02298,https://arxiv.org/abs/1601.02298,"b'Abstract:  The availability of vast amounts of data is changing how we can make medical\ndiscoveries, predict global market trends, save energy, and develop educational\nstrategies. In some settings such as Genome Wide Association Studies or deep\nlearning, sheer size of data seems critical. When data is held distributedly by\nmany parties, they must share it to reap its full benefits.\nOne obstacle to this revolution is the lack of willingness of different\nparties to share data, due to reasons such as loss of privacy or competitive\nedge. Cryptographic works address privacy aspects, but shed no light on\nindividual parties\' losses/gains when access to data carries tangible rewards.\nEven if it is clear that better overall conclusions can be drawn from\ncollaboration, are individual collaborators better off by collaborating?\nAddressing this question is the topic of this paper.\n* We formalize a model of n-party collaboration for computing functions over\nprivate inputs in which participants receive their outputs in sequence, and the\norder depends on their private inputs. Each output ""improves"" on preceding\noutputs according to a score function.\n* We say a mechanism for collaboration achieves collaborative equilibrium if\nit ensures higher reward for all participants when collaborating (rather than\nworking alone). We show that in general, computing a collaborative equilibrium\nis NP-complete, yet we design efficient algorithms to compute it in a range of\nnatural model settings.\nOur collaboration mechanisms are in the standard model, and thus require a\ncentral trusted party; however, we show this assumption is unnecessary under\nstandard cryptographic assumptions. We show how to implement the mechanisms in\na decentralized way with new extensions of secure multiparty computation that\nimpose order/timing constraints on output delivery to different players, as\nwell as privacy and correctness.'"
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1503.01588,https://arxiv.org/abs/1503.01588,"b'Abstract:  The full-information model was introduced by Ben-Or and Linial in 1985 to\nstudy collective coin-flipping: the problem of generating a common bounded-bias\nbit in a network of $n$ players with $t=t(n)$ faults. They showed that the\nmajority protocol can tolerate $t=O(\\sqrt n)$ adaptive corruptions, and\nconjectured that this is optimal in the adaptive setting. Lichtenstein, Linial,\nand Saks proved that the conjecture holds for protocols in which each player\nsends a single bit. Their result has been the main progress on the conjecture\nin the last 30 years.\nIn this work we revisit this question and ask: what about protocols involving\nlonger messages? Can increased communication allow for a larger fraction of\nfaulty players?\nWe introduce a model of strong adaptive corruptions, where in each round, the\nadversary sees all messages sent by honest parties and, based on the message\ncontent, decides whether to corrupt a party (and intercept his message) or not.\nWe prove that any one-round coin-flipping protocol, regardless of message\nlength, is secure against at most $\\tilde{O}(\\sqrt n)$ strong adaptive\ncorruptions. Thus, increased message length does not help in this setting.\nWe then shed light on the connection between adaptive and strongly adaptive\nadversaries, by proving that for any symmetric one-round coin-flipping protocol\nsecure against $t$ adaptive corruptions, there is a symmetric one-round\ncoin-flipping protocol secure against $t$ strongly adaptive corruptions.\nReturning to the standard adaptive model, we can now prove that any symmetric\none-round protocol with arbitrarily long messages can tolerate at most\n$\\tilde{O}(\\sqrt n)$ adaptive corruptions.\nAt the heart of our results lies a novel use of the Minimax Theorem and a new\ntechnique for converting any one-round secure protocol into a protocol with\nmessages of $polylog(n)$ bits. This technique may be of independent interest.'"
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1401.0348,https://arxiv.org/abs/1401.0348,"b'Abstract:  In this paper we show that the existence of general indistinguishability\nobfuscators conjectured in a few recent works implies, somewhat\ncounterintuitively, strong impossibility results for virtual black box\nobfuscation. In particular, we show that indistinguishability obfuscation for\nall circuits implies:\n* The impossibility of average-case virtual black box obfuscation with\nauxiliary input for any circuit family with super-polynomial pseudo-entropy.\nSuch circuit families include all pseudo-random function families, and all\nfamilies of encryption algorithms and randomized digital signatures that\ngenerate their required coin flips pseudo-randomly. Impossibility holds even\nwhen the auxiliary input depends only on the public circuit family, and not the\nspecific circuit in the family being obfuscated.\n* The impossibility of average-case virtual black box obfuscation with a\nuniversal simulator (with or without any auxiliary input) for any circuit\nfamily with super-polynomial pseudo-entropy.\nThese bounds significantly strengthen the impossibility results of Goldwasser\nand Kalai (STOC 2005).'"
Shafi Goldwasser,Goldwasser_Shafi,arXiv:cs/0212056,https://arxiv.org/abs/cs/0212056,"b""Abstract:  Madhu Sudan's work spans many areas of computer science theory including\ncomputational complexity theory, the design of efficient algorithms,\nalgorithmic coding theory, and the theory of program checking and correcting.\nTwo results of Sudan stand out in the impact they have had on the mathematics\nof computation. The first work shows a probabilistic characterization of the\nclass NP -- those sets for which short and easily checkable proofs of\nmembership exist, and demonstrates consequences of this characterization to\nclassifying the complexity of approximation problems. The second work shows a\npolynomial time algorithm for list decoding the Reed Solomon error correcting\ncodes.\nThis short note will be devoted to describing Sudan's work on\nprobabilistically checkable proofs -- the so called {\\it PCP theorem} and its\nimplications."""
Shafi Goldwasser,Goldwasser_Shafi,arXiv:cs/0212055,https://arxiv.org/abs/cs/0212055,"b'Abstract:  Theoretical computer science has found fertile ground in many areas of\nmathematics. The approach has been to consider classical problems through the\nprism of computational complexity, where the number of basic computational\nsteps taken to solve a problem is the crucial qualitative parameter. This new\napproach has led to a sequence of advances, in setting and solving new\nmathematical challenges as well as in harnessing discrete mathematics to the\ntask of solving real-world problems.\nIn this talk, I will survey the development of modern cryptography -- the\nmathematics behind secret communications and protocols -- in this light. I will\ndescribe the complexity theoretic foundations underlying the cryptographic\ntasks of encryption, pseudo-randomness number generators and functions, zero\nknowledge interactive proofs, and multi-party secure protocols. I will attempt\nto highlight the paradigms and proof techniques which unify these foundations,\nand which have made their way into the mainstream of complexity theory.'"
Polina Golland,Golland_Polina,arXiv:1903.05044,https://arxiv.org/abs/1903.05044,b'Abstract:  We present a volumetric mesh-based algorithm for flattening the placenta to a\ncanonical template to enable effective visualization of local anatomy and\nfunction. Monitoring placental function in vivo promises to support pregnancy\nassessment and to improve care outcomes. We aim to alleviate visualization and\ninterpretation challenges presented by the shape of the placenta when it is\nattached to the curved uterine wall. We flatten the volumetric mesh that\ncaptures placental shape to resemble the well-studied ex vivo shape. We\nformulate our method as a map from the in vivo shape to a flattened template\nthat minimizes the symmetric Dirichlet energy density to control distortion\nthroughout the volume. Local injectivity is enforced via constrained line\nsearch during gradient descent. We evaluate the proposed method on 28 placenta\nshapes extracted from MRI images in a study of placental function. We achieve\nsub-voxel accuracy in mapping the boundary of the placenta to the template\nwhile successfully controlling distortion throughout the volume. We illustrate\nhow the resulting mapping of the placenta enhances visualization of the\nplacental anatomy and function. Our code is freely available at\nthis https URL .'
Polina Golland,Golland_Polina,arXiv:1903.04352,https://arxiv.org/abs/1903.04352,"b'Abstract:  Segmentation of structural and diffusion MRI (sMRI/dMRI) is usually performed\nindependently in neuroimaging pipelines. However, some brain structures (e.g.,\nglobus pallidus, thalamus and its nuclei) can be extracted more accurately by\nfusing the two modalities. Following the framework of Bayesian segmentation\nwith probabilistic atlases and unsupervised appearance modeling, we present\nhere a novel algorithm to jointly segment multi-modal sMRI/dMRI data. We\npropose a hierarchical likelihood term for the dMRI defined on the unit ball,\nwhich combines the Beta and Dimroth-Scheidegger-Watson distributions to model\nthe data at each voxel. This term is integrated with a mixture of Gaussians for\nthe sMRI data, such that the resulting joint unsupervised likelihood enables\nthe analysis of multi-modal scans acquired with any type of MRI contrast,\nb-values, or number of directions, which enables wide applicability. We also\npropose an inference algorithm to estimate the maximum-a-posteriori model\nparameters from input images, and to compute the most likely segmentation.\nUsing a recently published atlas derived from histology, we apply our method to\nthalamic nuclei segmentation on two datasets: HCP (state of the art) and ADNI\n(legacy) - producing lower sample sizes than Bayesian segmentation with sMRI\nalone.'"
Polina Golland,Golland_Polina,arXiv:1903.02959,https://arxiv.org/abs/1903.02959,"b'Abstract:  We present a robust method to correct for motion in volumetric in-utero MRI\ntime series. Time-course analysis for in-utero volumetric MRI time series often\nsuffers from substantial and unpredictable fetal motion. Registration provides\nvoxel correspondences between images and is commonly employed for motion\ncorrection. Current registration methods often fail when aligning images that\nare substantially different from a template (reference image). To achieve\naccurate and robust alignment, we make a Markov assumption on the nature of\nmotion and take advantage of the temporal smoothness in the image data. Forward\nmessage passing in the corresponding hidden Markov model (HMM) yields an\nestimation algorithm that only has to account for relatively small motion\nbetween consecutive frames. We evaluate the utility of the temporal model in\nthe context of in-utero MRI time series alignment by examining the accuracy of\npropagated segmentation label maps. Our results suggest that the proposed model\ncaptures accurately the temporal dynamics of transformations in in-utero MRI\ntime series.'"
Polina Golland,Golland_Polina,arXiv:1902.10785,https://arxiv.org/abs/1902.10785,"b'Abstract:  We propose and demonstrate machine learning algorithms to assess the severity\nof pulmonary edema in chest x-ray images of congestive heart failure patients.\nAccurate assessment of pulmonary edema in heart failure is critical when making\ntreatment and disposition decisions. Our work is grounded in a large-scale\nclinical image dataset of over 300,000 x-ray images with associated radiology\nreports. While edema severity labels can be extracted unambiguously from a\nsmall fraction of the radiology reports, accurate annotation is challenging in\nmost cases. To take advantage of the unlabeled images, we develop a generative\nmodel that includes an auto-encoder for learning a latent representation from\nthe entire image dataset and a classifier that employs this representation for\npredicting pulmonary edema severity. We use segmentation to focus the\nauto-encoder on the lungs, where most of pulmonary edema findings are observed.\nOur experimental results suggest that modeling the distribution of images and\nproviding anatomical information improve the accuracy of pulmonary edema\nscoring compared to a strictly supervised approach. To the best of our\nknowledge, this is the first attempt to employ machine learning algorithms to\nautomatically and quantitatively assess the severity of pulmonary edema in\nchest x-ray images.'"
Polina Golland,Golland_Polina,arXiv:1809.04182,https://arxiv.org/abs/1809.04182,"b'Abstract:  We propose a new iterative segmentation model which can be accurately learned\nfrom a small dataset. A common approach is to train a model to directly segment\nan image, requiring a large collection of manually annotated images to capture\nthe anatomical variability in a cohort. In contrast, we develop a segmentation\nmodel that recursively evolves a segmentation in several steps, and implement\nit as a recurrent neural network. We learn model parameters by optimizing the\ninterme- diate steps of the evolution in addition to the final segmentation. To\nthis end, we train our segmentation propagation model by presenting incom-\nplete and/or inaccurate input segmentations paired with a recommended next\nstep. Our work aims to alleviate challenges in segmenting heart structures from\ncardiac MRI for patients with congenital heart disease (CHD), which encompasses\na range of morphological deformations and topological changes. We demonstrate\nthe advantages of this approach on a dataset of 20 images from CHD patients,\nlearning a model that accurately segments individual heart chambers and great\nvessels. Com- pared to direct segmentation, the iterative method yields more\naccurate segmentation for patients with the most severe CHD malformations.'"
Polina Golland,Golland_Polina,arXiv:1808.05732,https://arxiv.org/abs/1808.05732,"b'Abstract:  We present an algorithm for creating high resolution anatomically plausible\nimages consistent with acquired clinical brain MRI scans with large inter-slice\nspacing. Although large data sets of clinical images contain a wealth of\ninformation, time constraints during acquisition result in sparse scans that\nfail to capture much of the anatomy. These characteristics often render\ncomputational analysis impractical as many image analysis algorithms tend to\nfail when applied to such images. Highly specialized algorithms that explicitly\nhandle sparse slice spacing do not generalize well across problem domains. In\ncontrast, we aim to enable application of existing algorithms that were\noriginally developed for high resolution research scans to significantly\nundersampled scans. We introduce a generative model that captures fine-scale\nanatomical structure across subjects in clinical image collections and derive\nan algorithm for filling in the missing data in scans with large inter-slice\nspacing. Our experimental results demonstrate that the resulting method\noutperforms state-of-the-art upsampling super-resolution techniques, and\npromises to facilitate subsequent analysis not previously possible with scans\nof this quality. Our implementation is freely available at\nthis https URL .'"
Polina Golland,Golland_Polina,arXiv:1806.08723,https://arxiv.org/abs/1806.08723,"b'Abstract:  We introduce an approach for image segmentation based on sparse\ncorrespondences between keypoints in testing and training images. Keypoints\nrepresent automatically identified distinctive image locations, where each\nkeypoint correspondence suggests a transformation between images. We use these\ncorrespondences to transfer label maps of entire organs from the training\nimages to the test image. The keypoint transfer algorithm includes three steps:\n(i) keypoint matching, (ii) voting-based keypoint labeling, and (iii)\nkeypoint-based probabilistic transfer of organ segmentations. We report\nsegmentation results for abdominal organs in whole-body CT and MRI, as well as\nin contrast-enhanced CT and MRI. Our method offers a speed-up of about three\norders of magnitude in comparison to common multi-atlas segmentation, while\nachieving an accuracy that compares favorably. Moreover, keypoint transfer does\nnot require the registration to an atlas or a training phase. Finally, the\nmethod allows for the segmentation of scans with highly variable field-of-view.'"
Polina Golland,Golland_Polina,arXiv:1803.07682,https://arxiv.org/abs/1803.07682,"b'Abstract:  A reliable Ultrasound (US)-to-US registration method to compensate for brain\nshift would substantially improve Image-Guided Neurological Surgery. Developing\nsuch a registration method is very challenging, due to factors such as missing\ncorrespondence in images, the complexity of brain pathology and the demand for\nfast computation. We propose a novel feature-driven active framework. Here,\nlandmarks and their displacement are first estimated from a pair of US images\nusing corresponding local image features. Subsequently, a Gaussian Process (GP)\nmodel is used to interpolate a dense deformation field from the sparse\nlandmarks. Kernels of the GP are estimated by using variograms and a discrete\ngrid search method. If necessary, the user can actively add new landmarks based\non the image context and visualization of the uncertainty measure provided by\nthe GP to further improve the result. We retrospectively demonstrate our\nregistration framework as a robust and accurate brain shift compensation\nsolution on clinical data acquired during neurosurgery.'"
Polina Golland,Golland_Polina,arXiv:1803.05266,https://arxiv.org/abs/1803.05266,"b'Abstract:  Estimating the uncertainty in image registration is an area of current\nresearch that is aimed at providing information that will enable surgeons to\nassess the operative risk based on registered image data and the estimated\nregistration uncertainty. If they receive inaccurately calculated registration\nuncertainty and misplace confidence in the alignment solutions, severe\nconsequences may result. For probabilistic image registration (PIR), most\nresearch quantifies the registration uncertainty using summary statistics of\nthe transformation distributions. In this paper, we study a rarely examined\ntopic: whether those summary statistics of the transformation distribution\ntruly represent the registration uncertainty. Using concrete examples, we show\nthat there are two types of uncertainties: the transformation uncertainty, Ut,\nand label uncertainty Ul. Ut indicates the doubt concerning transformation\nparameters and can be estimated by conventional uncertainty measures, while Ul\nis strongly linked to the goal of registration. Further, we show that using Ut\nto quantify Ul is inappropriate and can be misleading. In addition, we present\nsome potentially critical findings regarding PIR.'"
Polina Golland,Golland_Polina,arXiv:1608.03907,https://arxiv.org/abs/1608.03907,b'Abstract:  We present a robust method to correct for motion and deformations for\nin-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI\nrequires robust alignment across time in the presence of substantial and\nunpredictable motion. We make a Markov assumption on the nature of deformations\nto take advantage of the temporal structure in the image data. Forward message\npassing in the corresponding hidden Markov model (HMM) yields an estimation\nalgorithm that only has to account for relatively small motion between\nconsecutive frames. We demonstrate the utility of the temporal model by showing\nthat its use improves the accuracy of the segmentation propagation through\ntemporal registration. Our results suggest that the proposed model captures\naccurately the temporal dynamics of deformations in in-utero MRI time series.'
Polina Golland,Golland_Polina,arXiv:1510.01648,https://arxiv.org/abs/1510.01648,"b'Abstract:  Despite the popularity and empirical success of patch-based nearest-neighbor\nand weighted majority voting approaches to medical image segmentation, there\nhas been no theoretical development on when, why, and how well these\nnonparametric methods work. We bridge this gap by providing a theoretical\nperformance guarantee for nearest-neighbor and weighted majority voting\nsegmentation under a new probabilistic model for patch-based image\nsegmentation. Our analysis relies on a new local property for how similar\nnearby patches are, and fuses existing lines of work on modeling natural\nimagery patches and theory for nonparametric classification. We use the model\nto derive a new patch-based segmentation algorithm that iterates between\ninferring local label patches and merging these local segmentations to produce\na globally consistent image segmentation. Many existing patch-based algorithms\narise as special cases of the new algorithm.'"
Polina Golland,Golland_Polina,arXiv:1503.03506,https://arxiv.org/abs/1503.03506,"b'Abstract:  High computational costs of manifold learning prohibit its application for\nlarge point sets. A common strategy to overcome this problem is to perform\ndimensionality reduction on selected landmarks and to successively embed the\nentire dataset with the Nystr\xc3\xb6m method. The two main challenges that arise\nare: (i) the landmarks selected in non-Euclidean geometries must result in a\nlow reconstruction error, (ii) the graph constructed from sparsely sampled\nlandmarks must approximate the manifold well. We propose the sampling of\nlandmarks from determinantal distributions on non-Euclidean spaces. Since\ncurrent determinantal sampling algorithms have the same complexity as those for\nmanifold learning, we present an efficient approximation running in linear\ntime. Further, we recover the local geometry after the sparsification by\nassigning each landmark a local covariance matrix, estimated from the original\npoint set. The resulting neighborhood selection based on the Bhattacharyya\ndistance improves the embedding of sparsely sampled manifolds. Our experiments\nshow a significant performance improvement compared to state-of-the-art\nlandmark selection techniques.'"
Polina Golland,Golland_Polina,arXiv:1411.6307,https://arxiv.org/abs/1411.6307,"b'Abstract:  We propose a novel diverse feature selection method based on determinantal\npoint processes (DPPs). Our model enables one to flexibly define diversity\nbased on the covariance of features (similar to orthogonal matching pursuit) or\nalternatively based on side information. We introduce our approach in the\ncontext of Bayesian sparse regression, employing a DPP as a variational\napproximation to the true spike and slab posterior distribution. We\nsubsequently show how this variational DPP approximation generalizes and\nextends mean-field approximation, and can be learned efficiently by exploiting\nthe fast sampling properties of DPPs. Our motivating application comes from\nbioinformatics, where we aim to identify a diverse set of genes whose\nexpression profiles predict a tumor type where the diversity is defined with\nrespect to a gene-gene interaction network. We also explore an application in\nspatial statistics. In both cases, we demonstrate that the proposed method\nyields significantly more diverse feature sets than classic sparse methods,\nwithout compromising accuracy.'"
Polina Golland,Golland_Polina,arXiv:1303.5508,https://arxiv.org/abs/1303.5508,"b'Abstract:  Manifold learning has been successfully applied to a variety of medical\nimaging problems. Its use in real-time applications requires fast projection\nonto the low-dimensional space. To this end, out-of-sample extensions are\napplied by constructing an interpolation function that maps from the input\nspace to the low-dimensional manifold. Commonly used approaches such as the\nNystr\xc3\xb6m extension and kernel ridge regression require using all training\npoints. We propose an interpolation function that only depends on a small\nsubset of the input training data. Consequently, in the testing phase each new\npoint only needs to be compared against a small number of input training data\nin order to project the point onto the low-dimensional space. We interpret our\nmethod as an out-of-sample extension that approximates kernel ridge regression.\nOur method involves solving a simple convex optimization problem and has the\nattractive property of guaranteeing an upper bound on the approximation error,\nwhich is crucial for medical applications. Tuning this error bound controls the\nsparsity of the resulting interpolation function. We illustrate our method in\ntwo clinical applications that require fast mapping of input images onto a\nlow-dimensional space.'"
John Guttag,Guttag_John,arXiv:1903.03545,https://arxiv.org/abs/1903.03545,"b'Abstract:  Classical deformable registration techniques achieve impressive results and\noffer a rigorous theoretical treatment, but are computationally intensive since\nthey solve an optimization problem for each image pair. Recently,\nlearning-based methods have facilitated fast registration by learning spatial\ndeformation functions. However, these approaches use restricted deformation\nmodels, require supervised labels, or do not guarantee a diffeomorphic\n(topology-preserving) registration. Furthermore, learning-based registration\ntools have not been derived from a probabilistic framework that can offer\nuncertainty estimates.\nIn this paper, we build a connection between classical and learning-based\nmethods. We present a probabilistic generative model and derive an unsupervised\nlearning-based inference algorithm that uses insights from classical\nregistration methods and makes use of recent developments in convolutional\nneural networks (CNNs). We demonstrate our method on a 3D brain registration\ntask for both images and anatomical surfaces, and provide extensive empirical\nanalyses of the algorithm. Our principled approach results in state of the art\naccuracy and very fast runtimes, while providing diffeomorphic guarantees. Our\nimplementation is available online at this http URL.'"
John Guttag,Guttag_John,arXiv:1903.03503,https://arxiv.org/abs/1903.03503,"b'Abstract:  A wide range of systems exhibit high dimensional incomplete data. Accurate\nestimation of the missing data is often desired, and is crucial for many\ndownstream analyses. Many state-of-the-art recovery methods involve supervised\nlearning using datasets containing full observations. In contrast, we focus on\nunsupervised estimation of missing image data, where no full observations are\navailable - a common situation in practice. Unsupervised imputation methods for\nimages often employ a simple linear subspace to capture correlations between\ndata dimensions, omitting more complex relationships. In this work, we\nintroduce a general probabilistic model that describes sparse high dimensional\nimaging data as being generated by a deep non-linear embedding. We derive a\nlearning algorithm using a variational approximation based on convolutional\nneural networks and discuss its relationship to linear imputation models, the\nvariational auto encoder, and deep image priors. We introduce sparsity-aware\nnetwork building blocks that explicitly model observed and missing data. We\nanalyze proposed sparsity-aware network building blocks, evaluate our method on\npublic domain imaging datasets, and conclude by showing that our method enables\nimputation in an important real-world problem involving medical images. The\ncode is freely available as part of the \\verb|neuron| library at\nthis http URL.'"
John Guttag,Guttag_John,arXiv:1903.03148,https://arxiv.org/abs/1903.03148,"b'Abstract:  We consider the problem of segmenting a biomedical image into anatomical\nregions of interest. We specifically address the frequent scenario where we\nhave no paired training data that contains images and their manual\nsegmentations. Instead, we employ unpaired segmentation images to build an\nanatomical prior. Critically these segmentations can be derived from imaging\ndata from a different dataset and imaging modality than the current task. We\nintroduce a generative probabilistic model that employs the learned prior\nthrough a convolutional neural network to compute segmentations in an\nunsupervised setting. We conducted an empirical analysis of the proposed\napproach in the context of structural brain MRI segmentation, using a\nmulti-study dataset of more than 14,000 scans. Our results show that an\nanatomical prior can enable fast unsupervised segmentation which is typically\nnot possible using standard convolutional networks. The integration of\nanatomical priors can facilitate CNN-based anatomical segmentation in a range\nof novel clinical problems, where few or no annotations are available and thus\nstandard networks are not trainable. The code is freely available at\nthis http URL.'"
John Guttag,Guttag_John,arXiv:1902.09383,https://arxiv.org/abs/1902.09383,"b'Abstract:  Biomedical image segmentation is an important task in many medical\napplications. Segmentation methods based on convolutional neural networks\nattain state-of-the-art accuracy; however, they typically rely on supervised\ntraining with large labeled datasets. Labeling datasets of medical images\nrequires significant expertise and time, and is infeasible at large scales. To\ntackle the lack of labeled data, researchers use techniques such as\nhand-engineered preprocessing steps, hand-tuned architectures, and data\naugmentation. However, these techniques involve costly engineering efforts, and\nare typically dataset-specific.\nWe present an automated data augmentation method for medical images. We\ndemonstrate our method on the task of segmenting magnetic resonance imaging\n(MRI) brain scans, focusing on the one-shot segmentation scenario -- a\npractical challenge in many medical applications. Our method requires only a\nsingle segmented scan, and leverages other unlabeled scans in a semi-supervised\napproach. We learn a model of transforms from the images, and use the model\nalong with the labeled example to synthesize additional labeled training\nexamples for supervised segmentation. Each transform is comprised of a spatial\ndeformation field and an intensity change, enabling the synthesis of complex\neffects such as variations in anatomy and image acquisition procedures.\nAugmenting the training of a supervised segmenter with these new examples\nprovides significant improvements over state-of-the-art methods for one-shot\nbiomedical image segmentation. Our code is available at\nthis https URL.'"
John Guttag,Guttag_John,arXiv:1901.10002,https://arxiv.org/abs/1901.10002,"b'Abstract:  As machine learning increasingly affects people and society, it is important\nthat we strive for a comprehensive and unified understanding of how and why\nunwanted consequences arise. For instance, downstream harms to particular\ngroups are often blamed on ""biased data,"" but this concept encompass too many\nissues to be useful in developing solutions. In this paper, we provide a\nframework that partitions sources of downstream harm in machine learning into\nfive distinct categories spanning the data generation and machine learning\npipeline. We describe how these issues arise, how they are relevant to\nparticular applications, and how they motivate different solutions. In doing\nso, we aim to facilitate the development of solutions that stem from an\nunderstanding of application-specific populations and data generation\nprocesses, rather than relying on general claims about what may or may not be\n""fair.""'"
John Guttag,Guttag_John,arXiv:1812.06932,https://arxiv.org/abs/1812.06932,"b'Abstract:  Deformable registration of clinical scans is a fundamental task for many\napplications, such as population studies or the monitoring of long-term disease\nprogression in individual patients. This task is challenging because, in\ncontrast to high-resolution research-quality scans, clinical images are often\nsparse, missing up to 85% of the slices in comparison. Furthermore, the anatomy\nin the acquired slices is not consistent across scans because of variations in\npatient orientation with respect to the scanner. In this work, we introduce\nSparse VoxelMorph (SparseVM), which adapts a state-of-the-art learning-based\nregistration method to improve the registration of sparse clinical images.\nSparseVM is a fast, unsupervised method that weights voxel contributions to\nregistration in proportion to confidence in the voxels. This leads to improved\nregistration performance on volumes with voxels of varying reliability, such as\ninterpolated clinical scans. SparseVM registers 3D scans in under a second on\nthe GPU, which is orders of magnitudes faster than the best performing clinical\nregistration methods, while still achieving comparable accuracy. Because of its\nshort runtimes and accurate behavior, SparseVM can enable clinical analyses not\npreviously possible. The code is publicly available at voxelmorph.mit.edu.'"
John Guttag,Guttag_John,arXiv:1812.00475,https://arxiv.org/abs/1812.00475,"b'Abstract:  In this paper, we apply a multiple instance learning paradigm to signal-based\nrisk stratification for cardiovascular outcomes. In contrast to methods that\nrequire hand-crafted features or domain knowledge, our method learns a\nrepresentation with state-of-the-art predictive power from the raw ECG signal.\nWe accomplish this by leveraging the multiple instance learning framework. This\nframework is particularly valuable to learning from biometric signals, where\npatient-level labels are available but signal segments are rarely annotated. We\nmake two contributions in this paper: 1) reframing risk stratification for\ncardiovascular death (CVD) as a multiple instance learning problem, and 2)\nusing this framework to design a new risk score, for which patients in the\nhighest quartile are 15.9 times more likely to die of CVD within 90 days of\nhospital admission for an acute coronary syndrome.'"
John Guttag,Guttag_John,arXiv:1809.05231,https://arxiv.org/abs/1809.05231,"b""Abstract:  We present VoxelMorph, a fast learning-based framework for deformable,\npairwise medical image registration. Traditional registration methods optimize\nan objective function for each pair of images, which can be time-consuming for\nlarge datasets or rich deformation models. In contrast to this approach, and\nbuilding on recent learning-based methods, we formulate registration as a\nfunction that maps an input image pair to a deformation field that aligns these\nimages. We parameterize the function via a convolutional neural network (CNN),\nand optimize the parameters of the neural network on a set of images. Given a\nnew pair of scans, VoxelMorph rapidly computes a deformation field by directly\nevaluating the function. In this work, we explore two different training\nstrategies. In the first (unsupervised) setting, we train the model to maximize\nstandard image matching objective functions that are based on the image\nintensities. In the second setting, we leverage auxiliary segmentations\navailable in the training data. We demonstrate that the unsupervised model's\naccuracy is comparable to state-of-the-art methods, while operating orders of\nmagnitude faster. We also show that VoxelMorph trained with auxiliary data\nimproves registration accuracy at test time, and evaluate the effect of\ntraining set size on registration. Our method promises to speed up medical\nimage analysis and processing pipelines, while facilitating novel directions in\nlearning-based registration and its applications. Our code is freely available\nat voxelmorph.csail.mit.edu."""
John Guttag,Guttag_John,arXiv:1808.02515,https://arxiv.org/abs/1808.02515,"b""Abstract:  Thanks to the rapid proliferation of connected devices, sensor-generated time\nseries constitute a large and growing portion of the world's data. Often, this\ndata is collected from distributed, resource-constrained devices and\ncentralized at one or more servers. A key challenge in this setup is reducing\nthe size of the transmitted data without sacrificing its quality. Lower quality\nreduces the data's utility, but smaller size enables both reduced network and\nstorage costs at the servers and reduced power consumption in sensing devices.\nA natural solution is to compress the data at the sensing devices.\nUnfortunately, existing compression algorithms either violate the memory and\nlatency constraints common for these devices or, as we show experimentally,\nperform poorly on sensor-generated time series.\nWe introduce a time series compression algorithm that achieves\nstate-of-the-art compression ratios while requiring less than 1KB of memory and\nadding virtually no latency. This method is suitable not only for low-power\ndevices collecting data, but also for servers storing and querying data; in the\nlatter context, it can decompress at over 3GB/s in a single thread, even faster\nthan many algorithms with much lower compression ratios. A key component of our\nmethod is a high-speed forecasting algorithm that can be trained online and\nsignificantly outperforms alternatives such as delta coding.\nExtensive experiments on datasets from many domains show that these results\nhold not only for sensor data but also across a wide array of other time\nseries."""
John Guttag,Guttag_John,arXiv:1806.02878,https://arxiv.org/abs/1806.02878,"b'Abstract:  Machine learning approaches have been effective in predicting adverse\noutcomes in different clinical settings. These models are often developed and\nevaluated on datasets with heterogeneous patient populations. However, good\npredictive performance on the aggregate population does not imply good\nperformance for specific groups.\nIn this work, we present a two-step framework to 1) learn relevant patient\nsubgroups, and 2) predict an outcome for separate patient populations in a\nmulti-task framework, where each population is a separate task. We demonstrate\nhow to discover relevant groups in an unsupervised way with a\nsequence-to-sequence autoencoder. We show that using these groups in a\nmulti-task framework leads to better predictive performance of in-hospital\nmortality both across groups and overall. We also highlight the need for more\ngranular evaluation of performance when dealing with heterogeneous populations.'"
John Guttag,Guttag_John,arXiv:1806.00397,https://arxiv.org/abs/1806.00397,"b'Abstract:  Electronic Health Records (EHRs) contain a large volume of heterogeneous\npatient data, which are useful at the point of care and for retrospective\nresearch. These data are typically stored in relational databases. Gaining an\nintegrated view of these data for a single patient typically requires complex\nSQL queries joining multiple tables. In this work, we present a visualization\ntool that integrates heterogeneous health care data (e.g., clinical notes,\nlaboratory test values, vital signs) into a single timeline. We train risk\nmodels offline and dynamically generate and present their predictions alongside\npatient data. Our visualization is designed to enable users to understand the\nheterogeneous temporal data quickly and comprehensively, and to place the\noutput of analytic models in the context of the underlying data.'"
John Guttag,Guttag_John,arXiv:1805.04605,https://arxiv.org/abs/1805.04605,"b'Abstract:  Traditional deformable registration techniques achieve impressive results and\noffer a rigorous theoretical treatment, but are computationally intensive since\nthey solve an optimization problem for each image pair. Recently,\nlearning-based methods have facilitated fast registration by learning spatial\ndeformation functions. However, these approaches use restricted deformation\nmodels, require supervised labels, or do not guarantee a diffeomorphic\n(topology-preserving) registration. Furthermore, learning-based registration\ntools have not been derived from a probabilistic framework that can offer\nuncertainty estimates. In this paper, we present a probabilistic generative\nmodel and derive an unsupervised learning-based inference algorithm that makes\nuse of recent developments in convolutional neural networks (CNNs). We\ndemonstrate our method on a 3D brain registration task, and provide an\nempirical analysis of the algorithm. Our approach results in state of the art\naccuracy and very fast runtimes, while providing diffeomorphic guarantees and\nuncertainty estimates. Our implementation is available online at\nthis http URL .'"
John Guttag,Guttag_John,arXiv:1804.07739,https://arxiv.org/abs/1804.07739,"b'Abstract:  We address the computational problem of novel human pose synthesis. Given an\nimage of a person and a desired pose, we produce a depiction of that person in\nthat pose, retaining the appearance of both the person and background. We\npresent a modular generative neural network that synthesizes unseen poses using\ntraining pairs of images and poses taken from human action videos. Our network\nseparates a scene into different body part and background layers, moves body\nparts to new locations and refines their appearances, and composites the new\nforeground with a hole-filled background. These subtasks, implemented with\nseparate modules, are trained jointly using only a single target image as a\nsupervised label. We use an adversarial discriminator to force our network to\nsynthesize realistic details conditioned on pose. We demonstrate image\nsynthesis results on three action classes: golf, yoga/workouts and tennis, and\nshow that our method produces accurate results within action classes as well as\nacross action classes. Given a sequence of desired poses, we also produce\ncoherent videos of actions.'"
John Guttag,Guttag_John,arXiv:1802.02604,https://arxiv.org/abs/1802.02604,"b'Abstract:  We present a fast learning-based algorithm for deformable, pairwise 3D\nmedical image registration. Current registration methods optimize an objective\nfunction independently for each pair of images, which can be time-consuming for\nlarge data. We define registration as a parametric function, and optimize its\nparameters given a set of images from a collection of interest. Given a new\npair of scans, we can quickly compute a registration field by directly\nevaluating the function using the learned parameters. We model this function\nusing a convolutional neural network (CNN), and use a spatial transform layer\nto reconstruct one image from another while imposing smoothness constraints on\nthe registration field. The proposed method does not require supervised\ninformation such as ground truth registration fields or anatomical landmarks.\nWe demonstrate registration accuracy comparable to state-of-the-art 3D image\nregistration, while operating orders of magnitude faster in practice. Our\nmethod promises to significantly speed up medical image analysis and processing\npipelines, while facilitating novel directions in learning-based registration\nand its applications. Our code is available at\nthis https URL .'"
John Guttag,Guttag_John,arXiv:1712.00643,https://arxiv.org/abs/1712.00643,"b""Abstract:  When an infection spreads in a community, an individual's probability of\nbecoming infected depends on both her susceptibility and exposure to the\ncontagion through contact with others. While one often has knowledge regarding\nan individual's susceptibility, in many cases, whether or not an individual's\ncontacts are contagious is unknown. We study the problem of predicting if an\nindividual will adopt a contagion in the presence of multiple modes of\ninfection (exposure/susceptibility) and latent neighbor influence. We present a\ngenerative probabilistic model and a variational inference method to learn the\nparameters of our model. Through a series of experiments on synthetic data, we\nmeasure the ability of the proposed model to identify latent spreaders, and\npredict the risk of infection. Applied to a real dataset of 20,000 hospital\npatients, we demonstrate the utility of our model in predicting the onset of a\nhealthcare associated infection using patient room-sharing and nurse-sharing\nnetworks. Our model outperforms existing benchmarks and provides actionable\ninsights for the design and implementation of targeted interventions to curb\nthe spread of infection."""
John Guttag,Guttag_John,arXiv:1706.10283,https://arxiv.org/abs/1706.10283,"b""Abstract:  Vectors of data are at the heart of machine learning and data mining.\nRecently, vector quantization methods have shown great promise in reducing both\nthe time and space costs of operating on vectors. We introduce a vector\nquantization algorithm that can compress vectors over 12x faster than existing\ntechniques while also accelerating approximate vector operations such as\ndistance and dot product computations by up to 10x. Because it can encode over\n2GB of vectors per second, it makes vector quantization cheap enough to employ\nin many more circumstances. For example, using our technique to compute\napproximate dot products in a nested loop can multiply matrices faster than a\nstate-of-the-art BLAS implementation, even when our algorithm must first\ncompress the matrices.\nIn addition to showing the above speedups, we demonstrate that our approach\ncan accelerate nearest neighbor search and maximum inner product search by over\n100x compared to floating point operations and up to 10x compared to other\nvector quantization methods. Our approximate Euclidean distance and dot product\ncomputations are not only faster than those of related algorithms with slower\nencodings, but also faster than Hamming distance computations, which have\ndirect hardware support on the tested platforms. We also assess the errors of\nour algorithm's approximate distances and dot products, and find that it is\ncompetitive with existing, slower vector quantization algorithms."""
John Guttag,Guttag_John,arXiv:1612.04007,https://arxiv.org/abs/1612.04007,"b""Abstract:  For many movement disorders, such as Parkinson's disease and ataxia, disease\nprogression is visually assessed by a clinician using a numerical disease\nrating scale. These tests are subjective, time-consuming, and must be\nadministered by a professional. This can be problematic where specialists are\nnot available, or when a patient is not consistently evaluated by the same\nclinician. We present an automated method for quantifying the severity of\nmotion impairment in patients with ataxia, using only video recordings. We\nconsider videos of the finger-to-nose test, a common movement task used as part\nof the assessment of ataxia progression during the course of routine clinical\ncheckups.\nOur method uses neural network-based pose estimation and optical flow\ntechniques to track the motion of the patient's hand in a video recording. We\nextract features that describe qualities of the motion such as speed and\nvariation in performance. Using labels provided by an expert clinician, we\ntrain a supervised learning model that predicts severity according to the Brief\nAtaxia Rating Scale (BARS). The performance of our system is comparable to that\nof a group of ataxia specialists in terms of mean error and correlation, and\nour system's predictions were consistently within the range of inter-rater\nvariability. This work demonstrates the feasibility of using computer vision\nand machine learning to produce consistent and clinically useful measures of\nmotor impairment."""
John Guttag,Guttag_John,arXiv:1609.09196,https://arxiv.org/abs/1609.09196,"b""Abstract:  Thanks to the rise of wearable and connected devices, sensor-generated time\nseries comprise a large and growing fraction of the world's data.\nUnfortunately, extracting value from this data can be challenging, since\nsensors report low-level signals (e.g., acceleration), not the high-level\nevents that are typically of interest (e.g., gestures). We introduce a\ntechnique to bridge this gap by automatically extracting examples of real-world\nevents in low-level data, given only a rough estimate of when these events have\ntaken place.\nBy identifying sets of features that repeat in the same temporal arrangement,\nwe isolate examples of such diverse events as human actions, power consumption\npatterns, and spoken words with up to 96% precision and recall. Our method is\nfast enough to run in real time and assumes only minimal knowledge of which\nvariables are relevant or the lengths of events. Our evaluation uses numerous\npublicly available datasets and over 1 million samples of manually labeled\nsensor data."""
John Guttag,Guttag_John,arXiv:1608.02301,https://arxiv.org/abs/1608.02301,"b'Abstract:  Voice disorders affect an estimated 14 million working-aged Americans, and\nmany more worldwide. We present the first large scale study of vocal misuse\nbased on long-term ambulatory data collected by an accelerometer placed on the\nneck. We investigate an unsupervised data mining approach to uncovering latent\ninformation about voice misuse.\nWe segment signals from over 253 days of data from 22 subjects into over a\nhundred million single glottal pulses (closures of the vocal folds), cluster\nsegments into symbols, and use symbolic mismatch to uncover differences between\npatients and matched controls, and between patients pre- and post-treatment.\nOur results show significant behavioral differences between patients and\ncontrols, as well as between some pre- and post-treatment patients. Our\nproposed approach provides an objective basis for helping diagnose behavioral\nvoice disorders, and is a first step towards a more data-driven understanding\nof the impact of voice therapy.'"
John Guttag,Guttag_John,arXiv:1608.02071,https://arxiv.org/abs/1608.02071,"b""Abstract:  In many domains such as medicine, training data is in short supply. In such\ncases, external knowledge is often helpful in building predictive models. We\npropose a novel method to incorporate publicly available domain expertise to\nbuild accurate models. Specifically, we use word2vec models trained on a\ndomain-specific corpus to estimate the relevance of each feature's text\ndescription to the prediction problem. We use these relevance estimates to\nrescale the features, causing more important features to experience weaker\nregularization.\nWe apply our method to predict the onset of five chronic diseases in the next\nfive years in two genders and two age groups. Our rescaling approach improves\nthe accuracy of the model, particularly when there are few positive examples.\nFurthermore, our method selects 60% fewer features, easing interpretation by\nphysicians. Our method is applicable to other domains where feature and outcome\ndescriptions are available."""
Peter Hagelstein,Hagelstein_Peter,arXiv:1201.4377,https://arxiv.org/abs/1201.4377,"b'Abstract:  Motivated by many observations of anomalies in condensed matter systems, we\nconsider a new fundamental Hamiltonian in which condensed matter and nuclear\nsystems are described initially on the same footing. Since it may be possible\nthat the lattice will respond to the mass change associated with a excited\nnuclear state, we adopt a relativistic description throughout based on a\nmany-particle Dirac formalism. This approach has not been used in the past,\nperhaps due to the difficulty in separating the center of mass and relative\ndegrees of freedom of the nuclear system, or perhaps due to an absence of\napplications for such a model. In response to some recent ideas about how to\nthink about the center of mass and relative separation, we obtained from the\nDirac model a new fundamental Hamiltonian in which the lattice couples to\ndifferent states within the composite nuclei within the lattice. In this\ndescription the different nuclear states have different mass energies and\nkinetic energies, as we had expected. In addition there appear new terms which\nprovide for nuclear excitation as a result of coupling to the composite\nmomentum. This new effect comes about because of changes in the composite\nnuclear state as a result of the dynamical Lorentz boost in the lattice.'"
Peter Hagelstein,Hagelstein_Peter,arXiv:1201.1488,https://arxiv.org/abs/1201.1488,"b""Abstract:  We are interested in the energy-momentum relation for a moving composite in\nrelativistic quantum mechanics in many-particle Dirac models. For a manifestly\ncovariant model one can apply the Lorentz transform to go from the rest frame\nto a moving frame to establish an energy-momentum relation of the form\n$\\sqrt{(M^*c^2)^2+c^2|{\\bf P}|^2}$ where $M^*$ is the kinematic mass. However,\nthe many-particle Dirac model is not manifestly covariant, and some other\napproach is required. We have found a simple approach that allows for a\nseparation of relative and center of mass contributions to the energy. We are\nable to define the associated kinematic energy and determine the\nenergy-momentum relation. Our result can be expressed as a modified deBroglie\nrelation of the form\n$$ \\hbar \\omega ({\\bf P}) = <\\Phi' | \\sum_j {m_j \\over M} \\beta_j | \\Phi' >~\n\\sqrt{[M^*({\\bf P}) c^2]^2 + c^2 |{\\bf P}|^2} $$\nwhere the kinematic mass $M^*$ will depend on the total momentum ${\\bf P}$\nfor a general noncovariant potential. The prefactor that occurs we associate\nwith a time dilation effect, the existence of which has been discussed\npreviously in the literature."""
Peter Hagelstein,Hagelstein_Peter,arXiv:0803.1906,https://arxiv.org/abs/0803.1906,"b'Abstract:  We consider a generalization of the spin-boson model in which two different\ntwo-level systems are coupled to an oscillator, under conditions where the\noscillator energy is much less than the two-level system energies, and where\nthe oscillator is highly excited. We find that the two-level system transition\nenergy is shifted, producing a Bloch-Siegert shift in each two-level system\nsimilar to what would be obtained if the other were absent. At resonances\nassociated with energy exchange between a two-level system and the oscillator,\nthe level splitting is about the same as would be obtained in the spin-boson\nmodel at a Bloch-Siegert resonance. However, there occur resonances associated\nwith the transfer of excitation between one two-level system and the other, an\neffect not present in the spin-boson model. We use a unitary transformation\nleading to a rotated system in which terms responsible for the shift and\nsplittings can be identified. The level splittings at the anticrossings\nassociated with both energy exchange and excitation transfer resonances are\naccounted for with simple two-state models and degenerate perturbation theory\nusing operators that appear in the rotated Hamiltonian.'"
Peter Hagelstein,Hagelstein_Peter,arXiv:0802.2779,https://arxiv.org/abs/0802.2779,"b'Abstract:  In previous work we studied the spin-boson model in the multiphoton regime,\nusing a rotation that provides a separation between terms that contribute most\nof the level energies away from resonance, and terms responsible for the level\nsplittings at the anticrossing. Here, we consider a generalization of the\nspin-boson model consisting of a three-level system coupled to an oscillator.\nWe construct a similar rotation and apply it to the more complicated model. We\nfind that the rotation provides a useful approximation to the energy levels in\nthe multiphoton region of the new problem. We find that good results can be\nobtained for the level splittings at the anticrossings for resonances involving\nthe lower two levels in regions away from accidental or low-order resonances of\nthe upper two levels.'"
Peter Hagelstein,Hagelstein_Peter,arXiv:0801.3810,https://arxiv.org/abs/0801.3810,"b'Abstract:  The electron mass is known to be sensitive to local fluctuations in the\nelectromagnetic field, and undergoes a small shift in a thermal field. It was\nclaimed recently that a very large electron mass shift should be expected near\nthe surface of a metal hydride [{\\it Eur. Phys. J. C}, {\\bf 46} 107 (2006)]. We\nexamine the shift using a formulation based on the Coulomb gauge, which leads\nto a much smaller shift. The maximization of the electron mass shift under\nnonequilibrium conditions seems nonetheless to be an interesting problem. We\nconsider a scheme in which a current in a hollow wire produces a large vector\npotential in the wire center. Fluctuations in an LC circuit with nearly matched\nloss and gain can produce large current fluctuations; and these can increase\nthe electron mass shift by orders of magnitude over its room temperature value.'"
Peter Hagelstein,Hagelstein_Peter,arXiv:0709.3557,https://arxiv.org/abs/0709.3557,"b'Abstract:  We consider a spin-boson model in which a spin 1 system is coupled to an\noscillator. A unitary transformation is applied which allows a separation of\nterms responsible for the Bloch-Siegert shift, and terms responsible for the\nlevel splittings at anticrossings associated with Bloch-Siegert resonances.\nWhen the oscillator is highly excited, the system can maintain resonance for\nsequential multiphoton transitions. At lower levels of excitation, resonance\ncannot be maintained because energy exchange with the oscillator changes the\nlevel shift. An estimate for the critical excitation level of the oscillator is\ndeveloped.'"
Peter Hagelstein,Hagelstein_Peter,arXiv:0709.1961,https://arxiv.org/abs/0709.1961,"b'Abstract:  We present a unitary equivalent spin-boson Hamiltonian in which terms can be\nidentified which contribute to the Bloch-Siegert shift, and to the level\nsplittings at the anticrossings associated with the Bloch-Siegert resonances.\nFirst-order degenerate perturbation theory is used to develop approximate\nresults in the case of moderate coupling for the level splitting.'"
Peter Hagelstein,Hagelstein_Peter,arXiv:0709.1958,https://arxiv.org/abs/0709.1958,"b'Abstract:  Recently there has been theoretical and experimental interest in\nBloch-Siegert shifts in an intense photon field. A perturbative treatment\nbecomes difficult in this multiphoton regime. We present a unitary transform\nand rotated model, which allows us to get accurate results away from the level\nanticrossings. A simple variational energy estimate leads to a new expression\nfor the dressed two-level system energy which is accurate, and useful over a\nwide range of the dimensionless coupling constant.'"
Peter Hagelstein,Hagelstein_Peter,arXiv:cond-mat/0612306,https://arxiv.org/abs/cond-mat/0612306,"b'Abstract:  We consider models in which two sets of matched two-level systems are coupled\nto a common oscillator in the case where the oscillator energy is small\nrelative to the two-level transition energies. Since the two sets of two-level\nsystems are coupled indirectly through the oscillator, excitation transfer from\none set of two-level systems to the other is possible. In addition, the\nexcitation energy from the two-level systems may be exchanged with the\noscillator coherently, even though the oscillator energy may be orders of\nmagnitude smaller than the two-level system transition energy. In the lossless\ncase, we demonstrate these effects numerically, and also use an approximate\ndiagonalization to show that these effects are expected from the model\nHamiltonian.\nWe augment the model to include loss effects, and show that loss enhances the\nexcitation transfer effect by breaking the severe cancelation between different\npaths that occurs in the lossless case. We describe a simple approximate model\nwavefunction appropriate when the loss increases rapidly with energy. Within\nthis model approximation, we present numerical and analytical results for\nexcitation transfer and energy transfer rates, showing that they are greatly\nincreased.\nOur study of these models is motivated in part by claims of excess heat\nproduction in electrochemical experiments in heavy water. We examine the\nquestion of whether the rates associated with this kind of model are\nsufficiently large to be relevant to the experimental claims. We find that\nconsistency is possible given recent experimental results showing strong\nscreening effects in low energy deuteron-deuteron fusion experiments in metals.'"
Peter Hagelstein,Hagelstein_Peter,arXiv:cond-mat/0606585,https://arxiv.org/abs/cond-mat/0606585,"b'Abstract:  Phonon exchange with nuclei in the course of fusion reactions that occur in a\nsolid has not been analyzed previously. This problem has become of interest in\nconnection with claims of observations of anomalies in metal deuterides. If the\nstrong force interaction were dependent only on position (and not spin or\nisospin), then the coupling with phonons can be developed directly. Since a\nnuclear interaction can change the lattice constituents, the initial and final\nstate lattices can be different, and we must include this in the formulation.\nFor more realistic strong force models with spin and isospin dependence, we can\nuse correlated nuclear wavefunctions which are made up of products of space,\nspin and isospin components. In this case, the spin and isospin algebra can be\ndone analytically, producing channel-dependent potentials that are only space\ndependent. The formulation that results can be used for quantitative estimates\nof phonon exchange.'"
Song Han,Han_Song,arXiv:1902.02023,https://arxiv.org/abs/1902.02023,"b'Abstract:  Along with the rapid growth of Industrial Internet-of-Things (IIoT)\napplications and their penetration into many industry sectors, real-time\nwireless networks (RTWNs) have been playing a more critical role in providing\nreal-time, reliable and secure communication services for such applications. A\nkey challenge in RTWN management is how to ensure real-time Quality of Services\n(QoS) especially in the presence of unexpected disturbances and lossy wireless\nlinks. Most prior work takes centralized approaches for handling disturbances,\nwhich are slow and subject to single-point failure, and do not scale. To\novercome these drawbacks, this paper presents a fully distributed packet\nscheduling framework called FD-PaS. FD-PaS aims to provide guaranteed fast\nresponse to unexpected disturbances while achieving minimum performance\ndegradation for meeting the timing and reliability requirements of all critical\ntasks. To combat the scalability challenge, FD-PaS incorporates several key\nadvances in both algorithm design and data link layer protocol design to enable\nindividual nodes to make on-line decisions locally without any centralized\ncontrol. Our extensive simulation and testbed results have validated the\ncorrectness of the FD-PaS design and demonstrated its effectiveness in\nproviding fast response for handling disturbances while ensuring the designated\nQoS requirements.'"
Song Han,Han_Song,arXiv:1812.02734,https://arxiv.org/abs/1812.02734,"b'Abstract:  Analog IC design relies on human experts to search for parameters that\nsatisfy circuit specifications with their experience and intuitions, which is\nhighly labor intensive, time consuming and suboptimal. Machine learning is a\npromising tool to automate this process. However, supervised learning is\ndifficult for this task due to the low availability of training data: 1)\nCircuit simulation is slow, thus generating large-scale dataset is\ntime-consuming; 2) Most circuit designs are propitiatory IPs within individual\nIC companies, making it expensive to collect large-scale datasets. We propose\nLearning to Design Circuits (L2DC) to leverage reinforcement learning that\nlearns to efficiently generate new circuits data and to optimize circuits. We\nfix the schematic, and optimize the parameters of the transistors automatically\nby training an RL agent with no prior knowledge about optimizing circuits.\nAfter iteratively getting observations, generating a new set of transistor\nparameters, getting a reward, and adjusting the model, L2DC is able to optimize\ncircuits. We evaluate L2DC on two transimpedance amplifiers. Trained for a day,\nour RL agent can achieve comparable or better performance than human experts\ntrained for a quarter. It first learns to meet hard-constraints (eg. gain,\nbandwidth), and then learns to optimize good-to-have targets (eg. area, power).\nCompared with grid search-aided human design, L2DC can achieve\n$\\mathbf{250}\\boldsymbol{\\times}$ higher sample efficiency with comparable\nperformance. Under the same runtime constraint, the performance of L2DC is also\nbetter than Bayesian Optimization.'"
Song Han,Han_Song,arXiv:1812.00332,https://arxiv.org/abs/1812.00332,"b'Abstract:  Neural architecture search (NAS) has a great impact by automatically\ndesigning effective neural network architectures. However, the prohibitive\ncomputational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours)\nmakes it difficult to \\emph{directly} search the architectures on large-scale\ntasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via\na continuous representation of network architecture but suffers from the high\nGPU memory consumption issue (grow linearly w.r.t. candidate set size). As a\nresult, they need to utilize~\\emph{proxy} tasks, such as training on a smaller\ndataset, or learning with only a few blocks, or training just for a few epochs.\nThese architectures optimized on proxy tasks are not guaranteed to be optimal\non the target task. In this paper, we present \\emph{ProxylessNAS} that can\n\\emph{directly} learn the architectures for large-scale target tasks and target\nhardware platforms. We address the high memory consumption issue of\ndifferentiable NAS and reduce the computational cost (GPU hours and GPU memory)\nto the same level of regular training while still allowing a large candidate\nset. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of\ndirectness and specialization. On CIFAR-10, our model achieves 2.08\\% test\nerror with only 5.7M parameters, better than the previous state-of-the-art\narchitecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet,\nour model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being\n1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to\nspecialize neural architectures for hardware with direct hardware metrics (e.g.\nlatency) and provide insights for efficient CNN architecture design.'"
Song Han,Han_Song,arXiv:1811.08886,https://arxiv.org/abs/1811.08886,"b""Abstract:  Model quantization is a widely used technique to compress and accelerate deep\nneural network (DNN) inference. Emergent DNN hardware accelerators begin to\nsupport flexible bitwidth (1-8 bits) to further improve the computation\nefficiency, which raises a great challenge to find the optimal bitwidth for\neach layer: it requires domain experts to explore the vast design space trading\noff among accuracy, latency, energy, and model size, which is both\ntime-consuming and sub-optimal. Conventional quantization algorithm ignores the\ndifferent hardware architectures and quantizes all the layers in a uniform way.\nIn this paper, we introduce the Hardware-Aware Automated Quantization (HAQ)\nframework which leverages the reinforcement learning to automatically determine\nthe quantization policy, and we take the hardware accelerator's feedback in the\ndesign loop. Rather than relying on proxy signals such as FLOPs and model size,\nwe employ a hardware simulator to generate direct feedback signals to the RL\nagent. Compared with conventional methods, our framework is fully automated and\ncan specialize the quantization policy for different neural network\narchitectures and hardware architectures. Our framework effectively reduced the\nlatency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of\naccuracy compared with the fixed bitwidth (8 bits) quantization. Our framework\nreveals that the optimal policies on different hardware architectures (i.e.,\nedge and cloud architectures) under different resource constraints (i.e.,\nlatency, energy and model size) are drastically different. We interpreted the\nimplication of different quantization policies, which offer insights for both\nneural network architecture design and hardware architecture design."""
Song Han,Han_Song,arXiv:1811.08383,https://arxiv.org/abs/1811.08383,"b""Abstract:  The explosive growth in online video streaming gives rise to challenges on\nefficiently extracting the spatial-temporal information to perform video\nunderstanding. Conventional 2D CNNs are computationally cheap but cannot\ncapture long-term temporal relationships; 3D CNN based methods can achieve good\nperformance but are computationally intensive, making it expensive to deploy.\nIn this paper, we propose a generic and effective Temporal Shift Module (TSM)\nthat enjoys both high efficiency and high performance. Specifically, it can\nachieve the performance of 3D CNN but maintain 2D complexity. The central idea\nof TSM is to shift part of the channels along the temporal dimension, which\nfacilitates information exchange among neighboring frames. TSM can be inserted\ninto 2D CNNs to achieve temporal modeling at the cost of zero FLOPs and zero\nparameters. On the Something-Something-V1 dataset which focuses on temporal\nmodeling, we achieved better results than I3D family and ECO family using 6X\nand 2.7X fewer FLOPs respectively. Measured on P100 GPU, our single model\nachieved 1.8% higher accuracy at 8X lower latency and 12X higher throughput\ncompared to I3D. Remarkably, our framework ranks the first on both\nSomething-Something V1 and V2 leaderboards upon this paper's submission."""
Song Han,Han_Song,arXiv:1811.06072,https://arxiv.org/abs/1811.06072,"b'Abstract:  We consider the problem of clustering graph nodes over large-scale dynamic\ngraphs, such as citation networks, images and web networks, when graph updates\nsuch as node/edge insertions/deletions are observed distributively. We propose\ncommunication-efficient algorithms for two well-established communication\nmodels namely the message passing and the blackboard models. Given a graph with\n$n$ nodes that is observed at $s$ remote sites over time $[1,t]$, the two\nproposed algorithms have communication costs $\\tilde{O}(ns)$ and\n$\\tilde{O}(n+s)$ ($\\tilde{O}$ hides a polylogarithmic factor), almost matching\ntheir lower bounds, $\\Omega(ns)$ and $\\Omega(n+s)$, respectively, in the\nmessage passing and the blackboard models. More importantly, we prove that at\neach time point in $[1,t]$ our algorithms generate clustering quality nearly as\ngood as that of centralizing all updates up to that time and then applying a\nstandard centralized clustering algorithm. We conducted extensive experiments\non both synthetic and real-life datasets which confirmed the communication\nefficiency of our approach over baseline algorithms while achieving comparable\nclustering results.'"
Song Han,Han_Song,arXiv:1806.02639,https://arxiv.org/abs/1806.02639,"b'Abstract:  We introduce a new function-preserving transformation for efficient neural\narchitecture search. This network transformation allows reusing previously\ntrained networks and existing successful architectures that improves sample\nefficiency. We aim to address the limitation of current network transformation\noperations that can only perform layer-level architecture modifications, such\nas adding (pruning) filters or inserting (removing) a layer, which fails to\nchange the topology of connection paths. Our proposed path-level transformation\noperations enable the meta-controller to modify the path topology of the given\nnetwork while keeping the merits of reusing weights, and thus allow efficiently\ndesigning effective structures with complex path topologies like Inception\nmodels. We further propose a bidirectional tree-structured reinforcement\nlearning meta-controller to explore a simple yet highly expressive\ntree-structured architecture space that can be viewed as a generalization of\nmulti-branch architectures. We experimented on the image classification\ndatasets with limited computational resources (about 200 GPU-hours), where we\nobserved improved parameter efficiency and better test results (97.70% test\naccuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet\nin the mobile setting), demonstrating the effectiveness and transferability of\nour designed architectures.'"
Song Han,Han_Song,arXiv:1804.06913,https://arxiv.org/abs/1804.06913,"b'Abstract:  Recent results at the Large Hadron Collider (LHC) have pointed to enhanced\nphysics capabilities through the improvement of the real-time event processing\ntechniques. Machine learning methods are ubiquitous and have proven to be very\npowerful in LHC physics, and particle physics as a whole. However, exploration\nof the use of such techniques in low-latency, low-power FPGA hardware has only\njust begun. FPGA-based trigger and data acquisition (DAQ) systems have\nextremely low, sub-microsecond latency requirements that are unique to particle\nphysics. We present a case study for neural network inference in FPGAs focusing\non a classifier for jet substructure which would enable, among many other\nphysics scenarios, searches for new dark sector particles and novel\nmeasurements of the Higgs boson. While we focus on a specific example, the\nlessons are far-reaching. We develop a package based on High-Level Synthesis\n(HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS\nincreases accessibility across a broad user community and allows for a drastic\ndecrease in firmware development time. We map out FPGA resource usage and\nlatency versus neural network hyperparameters to identify the problems in\nparticle physics that would benefit from performing neural network inference\nwith FPGAs. For our example jet substructure model, we fit well within the\navailable resources of modern FPGAs with a latency on the scale of 100 ns.'"
Song Han,Han_Song,arXiv:1803.01992,https://arxiv.org/abs/1803.01992,"b'Abstract:  Recently emerged dielectric resonators and metasurfaces offer a low-loss\nplatform for efficient manipulation of electromagnetic waves from microwave to\nvisible. Such flat meta-optics can focus electromagnetic waves, generate\nstructured beams and vortices, enhance local fields for sensing as well as\nprovide additional functionalities for advanced MRI machinery. Recent advances\nare associated with exotic optical modes called bound states in the continuum,\nwhich can give rise to extremely large quality factors and supercavity lasing.\nHere, we experimentally demonstrate subwavelength active supercavities with\nextremely high-Q resonances that could be reconfigured at an ultrafast time\nscale. We reveal that such supercavities enable all-optical switching and\nmodulation of extremely sharp resonances, and thus could have numerous\napplications in lasing, mode multiplexing, and biosensing.'"
Song Han,Han_Song,arXiv:1802.07855,https://arxiv.org/abs/1802.07855,"b'Abstract:  In most process control systems nowadays, process measurements are\nperiodically collected and archived in historians. Analytics applications\nprocess the data, and provide results offline or in a time period that is\nconsiderably slow in comparison to the performance of the manufacturing\nprocess. Along with the proliferation of Internet-of-Things (IoT) and the\nintroduction of ""pervasive sensors"" technology in process industries,\nincreasing number of sensors and actuators are installed in process plants for\npervasive sensing and control, and the volume of produced process data is\ngrowing exponentially. To digest these data and meet the ever-growing\nrequirements to increase production efficiency and improve product quality,\nthere needs to be a way to both improve the performance of the analytics system\nand scale the system to closely monitor a much larger set of plant resources.\nIn this paper, we present a real-time data analytics platform, called RT-DAP,\nto support large-scale continuous data analytics in process industries. RT-DAP\nis designed to be able to stream, store, process and visualize a large volume\nof realtime data flows collected from heterogeneous plant resources, and\nfeedback to the control system and operators in a realtime manner. A prototype\nof the platform is implemented on Microsoft Azure. Our extensive experiments\nvalidate the design methodologies of RT-DAP and demonstrate its efficiency in\nboth component and system levels.'"
Song Han,Han_Song,arXiv:1802.06367,https://arxiv.org/abs/1802.06367,"b""Abstract:  Convolutional Neural Networks (CNNs) are computationally intensive, which\nlimits their application on mobile devices. Their energy is dominated by the\nnumber of multiplies needed to perform the convolutions. Winograd's minimal\nfiltering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can\nreduce the operation count, but these two methods cannot be directly combined\n$-$ applying the Winograd transform fills in the sparsity in both the weights\nand the activations. We propose two modifications to Winograd-based CNNs to\nenable these methods to exploit sparsity. First, we move the ReLU operation\ninto the Winograd domain to increase the sparsity of the transformed\nactivations. Second, we prune the weights in the Winograd domain to exploit\nstatic weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet\ndatasets, our method reduces the number of multiplications by $10.4\\times$,\n$6.8\\times$ and $10.8\\times$ respectively with loss of accuracy less than\n$0.1\\%$, outperforming previous baselines by $2.0\\times$-$3.0\\times$. We also\nshow that moving ReLU to the Winograd domain allows more aggressive pruning."""
Song Han,Han_Song,arXiv:1802.03740,https://arxiv.org/abs/1802.03740,"b'Abstract:  The interaction between microscopic particles has always been a fascinating\nand intriguing area of science. Direct interrogation of such interactions is\noften difficult or impossible. Structured electromagnetic systems offer a rich\ntoolkit for mimicking and reproducing the key dynamics that governs the\nmicroscopic interactions, and thus provide an avenue to explore and interpret\nthe microscopic phenomena. In particular, metamaterials offer the freedom to\nartificially tailor light-matter coupling and to control the interaction\nbetween unit cells in the metamaterial array. Here we demonstrate a terahertz\nmetamaterial that mimics spin-related interactions of microscopic particles in\na 2D lattice via complex electromagnetic multipole interactions within the\nmetamaterial array. Fano resonances featured by distinct mode properties due to\nstrong nearest-neighbor interactions are discussed that draw parallels with the\n2D Ising model. Interestingly, a hyperfine Fano splitting spectrum is observed\nby manipulating the 2D interactions without applying external magnetic or\nelectric fields, which provides a passive multispectral platform for\napplications in super-resolution imaging, biosensing, and selective thermal\nemission. The dynamic approach to reproduce the static interaction between\nmicroscopic particles would enable more profound significance in exploring the\nunknown physical world by the macroscopic analogues.'"
Song Han,Han_Song,arXiv:1802.03494,https://arxiv.org/abs/1802.03494,"b'Abstract:  Model compression is a critical technique to efficiently deploy neural\nnetwork models on mobile devices which have limited computation resources and\ntight power budgets. Conventional model compression techniques rely on\nhand-crafted heuristics and rule-based policies that require domain experts to\nexplore the large design space trading off among model size, speed, and\naccuracy, which is usually sub-optimal and time-consuming. In this paper, we\npropose AutoML for Model Compression (AMC) which leverage reinforcement\nlearning to provide the model compression policy. This learning-based\ncompression policy outperforms conventional rule-based compression policy by\nhaving higher compression ratio, better preserving the accuracy and freeing\nhuman labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than\nthe handcrafted model compression policy for VGG-16 on ImageNet. We applied\nthis automated, push-the-button compression pipeline to MobileNet and achieved\n1.81x speedup of measured inference latency on an Android phone and 1.43x\nspeedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy.'"
Song Han,Han_Song,arXiv:1712.01887,https://arxiv.org/abs/1712.01887,"b'Abstract:  Large-scale distributed training requires significant communication bandwidth\nfor gradient exchange that limits the scalability of multi-node training, and\nrequires expensive high-bandwidth network infrastructure. The situation gets\neven worse with distributed training on mobile devices (federated learning),\nwhich suffers from higher latency, lower throughput, and intermittent poor\nconnections. In this paper, we find 99.9% of the gradient exchange in\ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to\ngreatly reduce the communication bandwidth. To preserve accuracy during\ncompression, DGC employs four methods: momentum correction, local gradient\nclipping, momentum factor masking, and warm-up training. We have applied Deep\nGradient Compression to image classification, speech recognition, and language\nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and\nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a\ngradient compression ratio from 270x to 600x without losing accuracy, cutting\nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from\n488MB to 0.74MB. Deep gradient compression enables large-scale distributed\ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed\ntraining on mobile.'"
Song Han,Han_Song,arXiv:1706.00051,https://arxiv.org/abs/1706.00051,"b'Abstract:  Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear\ninverse task demanding time and resource intensive computations that can\nsubstantially trade off {\\it accuracy} for {\\it speed} in real-time imaging. In\naddition, state-of-the-art compressed sensing (CS) analytics are not cognizant\nof the image {\\it diagnostic quality}. To cope with these challenges we put\nforth a novel CS framework that permeates benefits from generative adversarial\nnetworks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR\nimages from historical patients. Leveraging a mixture of least-squares (LS)\nGANs and pixel-wise $\\ell_1$ cost, a deep residual network with skip\nconnections is trained as the generator that learns to remove the {\\it\naliasing} artifacts by projecting onto the manifold. LSGAN learns the texture\ndetails, while $\\ell_1$ controls the high-frequency noise. A multilayer\nconvolutional neural network is then jointly trained based on diagnostic\nquality images to discriminate the projection quality. The test phase performs\nfeed-forward propagation over the generator network that demands a very low\ncomputational overhead. Extensive evaluations are performed on a large\ncontrast-enhanced MR dataset of pediatric patients. In particular, images rated\nbased on expert radiologists corroborate that GANCS retrieves high contrast\nimages with detailed texture relative to conventional CS, and pixel-wise\nschemes. In addition, it offers reconstruction under a few milliseconds, two\norders of magnitude faster than state-of-the-art CS-MRI schemes.'"
Song Han,Han_Song,arXiv:1705.08922,https://arxiv.org/abs/1705.08922,"b'Abstract:  Sparsity helps reduce the computational complexity of deep neural networks by\nskipping zeros. Taking advantage of sparsity is listed as a high priority in\nnext generation DNN accelerators such as TPU. The structure of sparsity, i.e.,\nthe granularity of pruning, affects the efficiency of hardware accelerator\ndesign as well as the prediction accuracy. Coarse-grained pruning creates\nregular sparsity patterns, making it more amenable for hardware acceleration\nbut more challenging to maintain the same accuracy. In this paper we\nquantitatively measure the trade-off between sparsity regularity and prediction\naccuracy, providing insights in how to maintain accuracy while having more a\nmore structured sparsity pattern. Our experimental results show that\ncoarse-grained pruning can achieve a sparsity ratio similar to unstructured\npruning without loss of accuracy. Moreover, due to the index saving effect,\ncoarse-grained pruning is able to obtain a better compression ratio than\nfine-grained sparsity at the same accuracy threshold. Based on the recent\nsparse convolutional neural network accelerator (SCNN), our experiments further\ndemonstrate that coarse-grained sparsity saves about 2x the memory references\ncompared to fine-grained sparsity. Since memory reference is more than two\norders of magnitude more expensive than arithmetic operations, the regularity\nof sparse structure leads to more efficient hardware design.'"
Song Han,Han_Song,arXiv:1612.02562,https://arxiv.org/abs/1612.02562,"b""Abstract:  As our population ages, neurological impairments and degeneration of the\nmusculoskeletal system yield gait abnormalities, which can significantly reduce\nquality of life. Gait rehabilitative therapy has been widely adopted to help\npatients maximize community participation and living independence. To further\nimprove the precision and efficiency of rehabilitative therapy, more objective\nmethods need to be developed based on sensory data. In this paper, an\nalgorithmic framework is proposed to provide classification of gait disorders\ncaused by two common neurological diseases, stroke and Parkinson's Disease\n(PD), from ground contact force (GCF) data. An advanced machine learning\nmethod, multi-task feature learning (MTFL), is used to jointly train\nclassification models of a subject's gait in three classes, post-stroke, PD and\nhealthy gait. Gait parameters related to mobility, balance, strength and rhythm\nare used as features for the classification. Out of all the features used, the\nMTFL models capture the more important ones per disease, which will help\nprovide better objective assessment and therapy progress tracking. To evaluate\nthe proposed methodology we use data from a human participant study, which\nincludes five PD patients, three post-stroke patients, and three healthy\nsubjects. Despite the diversity of abnormalities, the evaluation shows that the\nproposed approach can successfully distinguish post-stroke and PD gait from\nhealthy gait, as well as post-stroke from PD gait, with Area Under the Curve\n(AUC) score of at least 0.96. Moreover, the methodology helps select important\ngait features to better understand the key characteristics that distinguish\nabnormal gaits and design personalized treatment."""
Song Han,Han_Song,arXiv:1612.01064,https://arxiv.org/abs/1612.01064,"b""Abstract:  Deep neural networks are widely used in machine learning applications.\nHowever, the deployment of large neural networks models can be difficult to\ndeploy on mobile devices with limited power budgets. To solve this problem, we\npropose Trained Ternary Quantization (TTQ), a method that can reduce the\nprecision of weights in neural networks to ternary values. This method has very\nlittle accuracy degradation and can even improve the accuracy of some models\n(32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet\nmodel is trained from scratch, which means it's as easy as to train normal full\nprecision model. We highlight our trained quantization method that can learn\nboth ternary values and ternary assignment. During inference, only ternary\nvalues (2-bit weights) and scaling factors are needed, therefore our models are\nnearly 16x smaller than full-precision models. Our ternary models can also be\nviewed as sparse binary weight networks, which can potentially be accelerated\nwith custom circuit. Experiments on CIFAR-10 show that the ternary models\nobtained by trained quantization method outperform full-precision models of\nResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model\noutperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and\noutperforms previous ternary models by 3%."""
Song Han,Han_Song,arXiv:1612.00694,https://arxiv.org/abs/1612.00694,"b'Abstract:  Long Short-Term Memory (LSTM) is widely used in speech recognition. In order\nto achieve higher prediction accuracy, machine learning scientists have built\nlarger and larger models. Such large model is both computation intensive and\nmemory intensive. Deploying such bulky model results in high power consumption\nand leads to high total cost of ownership (TCO) of a data center. In order to\nspeedup the prediction and make it energy efficient, we first propose a\nload-balance-aware pruning method that can compress the LSTM model size by 20x\n(10x from pruning and 2x from quantization) with negligible loss of the\nprediction accuracy. The pruned model is friendly for parallel processing.\nNext, we propose scheduler that encodes and partitions the compressed model to\neach PE for parallelism, and schedule the complicated LSTM data flow. Finally,\nwe design the hardware architecture, named Efficient Speech Recognition Engine\n(ESE) that works directly on the compressed model. Implemented on Xilinx\nXCKU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working\ndirectly on the compressed LSTM network, corresponding to 2.52 TOPS on the\nuncompressed one, and processes a full LSTM for speech recognition with a power\ndissipation of 41 Watts. Evaluated on the LSTM for speech recognition\nbenchmark, ESE is 43x and 3x faster than Core i7 5930k CPU and Pascal Titan X\nGPU implementations. It achieves 40x and 11.5x higher energy efficiency\ncompared with the CPU and GPU respectively.'"
Song Han,Han_Song,arXiv:1611.07519,https://arxiv.org/abs/1611.07519,"b""Abstract:  In this paper, we theoretically and experimentally demonstrate a three\ndimensional metamaterial that can motivate electromagnetic induced transparency\n(EIT) by using circular polarized wave as stimulations. The unit cell consists\nof a pair of metallic strips printed on both sides of the printed circuit board\n(PCB), where a conductive cylinder junction is used to connect the metal strips\nby drilling a hole inside the substrate. When a right circularly polarized wave\nis incident, destructive interference is excited between meta-atoms of the 3D\nstructure, the transmission spectrum demonstrates a sharp transparency window.\nA coupled oscillator model and an electrical equivalent circuit model are\napplied to quantitatively and qualitatively analyze the coupling mechanism in\nthe EIT-like metamaterial. Analysis in detail shows the EIT window's amplitude\nand frequency are modulated by changing the degree of symmetry breaking. The\nproposed metamaterial may achieve potential applications in developing chiral\nslow light devices."""
Song Han,Han_Song,arXiv:1607.04381,https://arxiv.org/abs/1607.04381,"b""Abstract:  Modern deep neural networks have a large number of parameters, making them\nvery hard to train. We propose DSD, a dense-sparse-dense training flow, for\nregularizing deep neural networks and achieving better optimization\nperformance. In the first D (Dense) step, we train a dense network to learn\nconnection weights and importance. In the S (Sparse) step, we regularize the\nnetwork by pruning the unimportant connections with small weights and\nretraining the network given the sparsity constraint. In the final D (re-Dense)\nstep, we increase the model capacity by removing the sparsity constraint,\nre-initialize the pruned parameters from zero and retrain the whole dense\nnetwork. Experiments show that DSD training can improve the performance for a\nwide range of CNNs, RNNs and LSTMs on the tasks of image classification,\ncaption generation and speech recognition. On ImageNet, DSD improved the Top1\naccuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50\nby 1.1%, respectively. On the WSJ'93 dataset, DSD improved DeepSpeech and\nDeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the\nNeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training\ntime, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S\nstep. At testing time, DSD doesn't change the network architecture or incur any\ninference overhead. The consistent and significant performance gain of DSD\nexperiments shows the inadequacy of the current training methods for finding\nthe best local optimum, while DSD effectively achieves superior optimization\nperformance for finding a better solution. DSD models are available to download\nat this https URL."""
Song Han,Han_Song,arXiv:1607.00099,https://arxiv.org/abs/1607.00099,"b'Abstract:  For any semiring, the concept of k-congruences is introduced, criteria for\nk-congruences are established, it is proved that there is an\ninclusion-preserving bijection between k-congruences and k-ideals, and an\nequivalent condition for the existence of a zero is presented with the help of\nk-congruences. It is shown that a semiring is k-simple iff it is\nk-congruence-simple, and that inclines are k-simple iff they have at most 2\nelements. Lemma 2.12(i) in [Glas. Mat. 42(62) (2007) 301] is pointed out being\nfalse.'"
Song Han,Han_Song,arXiv:1602.07360,https://arxiv.org/abs/1602.07360,"b'Abstract:  Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\nThe SqueezeNet architecture is available for download here:\nthis https URL'"
Song Han,Han_Song,arXiv:1602.01895,https://arxiv.org/abs/1602.01895,"b'Abstract:  Generating natural language descriptions for images is a challenging task.\nThe traditional way is to use the convolutional neural network (CNN) to extract\nimage features, followed by recurrent neural network (RNN) to generate\nsentences. In this paper, we present a new model that added memory cells to\ngate the feeding of image features to the deep neural network. The intuition is\nenabling our model to memorize how much information from images should be fed\nat each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed\nthat our model outperforms other state-of-the-art models with higher BLEU\nscores.'"
Song Han,Han_Song,arXiv:1602.01528,https://arxiv.org/abs/1602.01528,"b""Abstract:  State-of-the-art deep neural networks (DNNs) have hundreds of millions of\nconnections and are both computationally and memory intensive, making them\ndifficult to deploy on embedded systems with limited hardware resources and\npower budgets. While custom hardware helps the computation, fetching weights\nfrom DRAM is two orders of magnitude more expensive than ALU operations, and\ndominates the required power.\nPreviously proposed 'Deep Compression' makes it possible to fit large DNNs\n(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by\npruning the redundant connections and having multiple connections share the\nsame weight. We propose an energy efficient inference engine (EIE) that\nperforms inference on this compressed network model and accelerates the\nresulting sparse matrix-vector multiplication with weight sharing. Going from\nDRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;\nWeight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.\nEvaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to\nCPU and GPU implementations of the same DNN without compression. EIE has a\nprocessing power of 102GOPS/s working directly on a compressed network,\ncorresponding to 3TOPS/s on an uncompressed network, and processes FC layers of\nAlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is\n24,000x and 3,400x more energy efficient than a CPU and GPU respectively.\nCompared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy\nefficiency and area efficiency."""
Song Han,Han_Song,arXiv:1510.07383,https://arxiv.org/abs/1510.07383,"b'Abstract:  An incline is an additively idempotent semiring in which the product of two\nelements is always less than or equal to either factor. By making use of prime\nnumbers, this paper proves that A^{11} is less than or equal to A^5 for all 3x3\nmatrices A over an arbitrary commutative incline, thus giving an answer to an\nopen problem ""For 3x3 matrices over any incline (even noncommutative) is X^5\ngreater than or equal to X^{11}?"", proposed by Cao, Kim and Roush in a\nmonograph Incline Algebra and Applications, 1984.'"
Song Han,Han_Song,arXiv:1510.00149,https://arxiv.org/abs/1510.00149,"b'Abstract:  Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce ""deep compression"", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency.'"
Song Han,Han_Song,arXiv:1506.02626,https://arxiv.org/abs/1506.02626,"b'Abstract:  Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems. Also, conventional\nnetworks fix the architecture before training starts; as a result, training\ncannot improve the architecture. To address these limitations, we describe a\nmethod to reduce the storage and computation required by neural networks by an\norder of magnitude without affecting their accuracy by learning only the\nimportant connections. Our method prunes redundant connections using a\nthree-step method. First, we train the network to learn which connections are\nimportant. Next, we prune the unimportant connections. Finally, we retrain the\nnetwork to fine tune the weights of the remaining connections. On the ImageNet\ndataset, our method reduced the number of parameters of AlexNet by a factor of\n9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar\nexperiments with VGG-16 found that the number of parameters can be reduced by\n13x, from 138 million to 10.3 million, again with no loss of accuracy.'"
Song Han,Han_Song,arXiv:1308.6348,https://arxiv.org/abs/1308.6348,"b'Abstract:  An incline is an additively idempotent semiring in which the product of two\nelements is always less than or equal to either factor. This paper proves that\nthe only regular inclines are distributive lattices, which also implies that\nthere is no noncommutative regular incline.'"
Song Han,Han_Song,arXiv:1304.6241,https://arxiv.org/abs/1304.6241,"b'Abstract:  In this paper, we proposed an identification and data encrypt key manage\nprotocol that can be used in some security system based on such secure devices\nas secure USB memories or RFIDs, which are widely used for identifying persons\nor other objects recently. In general, the default functions of the security\nsystem using a mobile device are the authentication for the owner of the device\nand secure storage of data stored on the device. We proposed a security model\nthat consists of the server and mobile devices in order to realize these\nsecurity features. In this model we defined the secure communication protocol\nfor the authentication and management of data encryption keys using a private\nkey encryption algorithm with the public key between the server and mobile\ndevices. In addition, we was performed the analysis for the attack to the\ncommunication protocol between the mobile device and server. Using the\ncommunication protocol, the system will attempt to authenticate the mobile\ndevice. The data decrypt key is transmitted only if the authentication process\nis successful. The data in the mobile device can be decrypted using the key.\nOur analysis proved that this Protocol ensures anonymity, prevents replay\nattacks and realizes the interactive identification between the security\ndevices and the authentication server.'"
Song Han,Han_Song,arXiv:1304.0374,https://arxiv.org/abs/1304.0374,"b""Abstract:  Cognitive complexity measures quantify human difficulty in understanding the\nsource code based on cognitive informatics foundation. The discipline derives\ncognitive complexity on a basis of fundamental software factors i.e, inputs,\noutputs, and internal processing architecture. An approach to integrating\nGranular Computing into the new measure called Structured Cognitive Information\nMeasure or SCIM. The proposed measure unifies and re-organizes complexity\nfactors analogous to human cognitive process. However, according to the\nmethodology of software and the scope of the variables, Information Complexity\nNumber(ICN) of variables is depended on change of variable value and cognitive\ncomplexity is measured in several ways. In this paper, we define the Scope\nInformation Complexity Number (SICN) and present the cognitive complexity based\non functional decomposition of software, including theoretical validation\nthrough nine Weyuker's properties."""
Song Han,Han_Song,arXiv:1212.3838,https://arxiv.org/abs/1212.3838,"b'Abstract:  DC has proved to be a promising tool for the specification and verification\nof functional requirements on the design of hard real-time systems. Many works\nwere devoted to develop effective techniques for checking the models of hard\nreal-time systems against DC specifications. DC model checking theory is still\nevolving and yet there is no available tools supporting practical verifications\ndue to the high undecidability of calculus and the great complexity of model\nchecking. Present situation of PDC model checking is much worse than the one of\nDC model checking. In view of the results so far achieved, it is desirable to\ndevelop approximate model checking techniques for DC and PDC specifications.\nThis work was motivated to develop approximate techniques checking automata\nmodels of hard real-time systems for DC and PDC specifications. Unlike previous\nworks which only deal with decidable formulas, we want to develop approximate\ntechniques covering whole DC and PDC formulas. The first results of our work,\nnamely, approximate techniques checking real-time automata models of systems\nfor LDI and PLDI specifications, are described in this paper.'"
Song Han,Han_Song,arXiv:1212.2415,https://arxiv.org/abs/1212.2415,"b'Abstract:  Face recognition systems must be robust to the variation of various factors\nsuch as facial expression, illumination, head pose and aging. Especially, the\nrobustness against illumination variation is one of the most important problems\nto be solved for the practical use of face recognition systems. Gabor wavelet\nis widely used in face detection and recognition because it gives the\npossibility to simulate the function of human visual system. In this paper, we\npropose a method for extracting Gabor wavelet features which is stable under\nthe variation of local illumination and show experiment results demonstrating\nits effectiveness.'"
Song Han,Han_Song,arXiv:1009.3626,https://arxiv.org/abs/1009.3626,"b'Abstract:  This paper presents a survey on several RFID authentication protocols under\nlow cost restrictions. Low cost RFID are mainly addressed with limited security\nand privacy protections. In this study, we explore several protocols with\nvarious authentication mechanisms found in literature that satisfy low cost\nrestrictions. Assessments of these protocols are based on data protection,\ntracking protection, forward security. Finally, it is concluded that no single\nlow cost RFID protocol fully meets the requirement of the given assessments.\nWhile a protocol satisfies one or two assessments, it fails to fully meet the\nrequirement of the third assessment. This study provides a new insight in RFID\nliterature which can be used particularly by small and medium industries to\nchoose the appropriate RFID protocol for their needs.'"
Song Han,Han_Song,arXiv:1008.2452,https://arxiv.org/abs/1008.2452,"b'Abstract:  Security and privacy are the inherent problems in RFID communications. There\nare several protocols have been proposed to overcome those problems. Hash chain\nis commonly employed by the protocols to improve security and privacy for RFID\nauthentication. Although the protocols able to provide specific solution for\nRFID security and privacy problems, they fail to provide integrated solution.\nThis article is a survey to closely observe those protocols in terms of its\nfocus and limitations.'"
Ruonan Han,Han_Ruonan,arXiv:1810.01056,https://arxiv.org/abs/1810.01056,"b'Abstract:  The nitrogen vacancy (NV) center in diamond has emerged as a leading\nsolid-state quantum sensor for applications including magnetometry,\nelectrometry, thermometry, and chemical sensing. However, an outstanding\nchallenge for practical applications is that existing NV-based sensing\ntechniques require bulky and discrete instruments for spin control and\ndetection. Here, we address this challenge by integrating NV based quantum\nsensing with complementary metal-oxide-semiconductor (CMOS) technology. Through\ntailored CMOS-integrated microwave generation and photodetection, this work\ndramatically reduces the instrumentation footprint for quantum magnetometry and\nthermometry. This hybrid diamond-CMOS integration enables an ultra-compact and\nscalable platform for quantum sensing and quantum information processing.'"
Jongyoon Han,Han_Jongyoon,arXiv:1802.08565,https://arxiv.org/abs/1802.08565,b'Abstract:  Series of short contributions that are part of Nobel Symposium 162 -\nMicrofluidics arXiv:1712.08369.'
Jongyoon Han,Han_Jongyoon,arXiv:1801.01462,https://arxiv.org/abs/1801.01462,"b'Abstract:  We develop the first theoretical model for the analytical description of ion\nconcentration polarization (ICP)-based electrokinetic molecular concentration,\nwhich had not been possible due to the extraordinary complexity of the system.\nWe define the two separate limits for the enrichment factor achievable in a\ngiven system and derive the scaling laws for critical parameters, which are\nvalidated by numerical simulations and experiments. This work provides clear\ntheoretical explanations on the diverse experimental behaviors previously\nobserved yet unexplainable, while setting solid foundation for the engineering\nof ICP-based concentrators and other fluid-coupled electrokinetic systems.'"
Jongyoon Han,Han_Jongyoon,arXiv:1709.01859,https://arxiv.org/abs/1709.01859,"b'Abstract:  This paper studies mechanism of preconcentration of charged particles in a\nstraight micro-channel embedded with permselective membranes, by numerically\nsolving coupled transport equations of ions, charged particles and solvent\nfluid without any simplifying assumptions. It is demonstrated that trapping and\npreconcentration of charged particles are determined by the interplay between\ndrag force from the electroosmotic fluid flow and the electrophoretic force\napplied trough the electric field. Several insightful characteristics are\nrevealed, including the diverse dynamics of co-ions and counter ions,\nreplacement of co-ions by focused particles, lowered ion concentrations in\nparticle enriched zone, and enhanced electroosmotic pumping effect etc.\nConditions for particles that may be concentrated are identified in terms of\ncharges, sizes and electrophoretic mobilities of particles and co-ions.\nDependences of enrichment factor on cross-membrane voltage, initial particle\nconcentration and buffer ion concentrations are analyzed and the underlying\nreasons are elaborated. Finally, post priori a condition for validity of\ndecoupled simulation model is given based on charges carried by focused charge\nparticles and that by buffer co-ions. These results provide important guidance\nin the design and optimization of nanofluidic preconcentration and other\nrelated devices.'"
Thomas Heldt,Heldt_Thomas,arXiv:1806.09994,https://arxiv.org/abs/1806.09994,"b'Abstract:  A binary beat-by-beat classification algorithm for cerebral blood flow\nvelocity (CBFV) recordings based on amplitude, spectral and morphological\nfeatures is presented. The classification difference between 15 manually and\nalgorithmically annotated CBFV records is around 5%.'"
Berthold Horn,Horn_Berthold,arXiv:1810.06055,https://arxiv.org/abs/1810.06055,"b'Abstract:  For identification of change information in image sequences, most studies\nfocus on change detection in one image sequence, while few studies have\nconsidered the change level comparison between two different image sequences.\nMoreover, most studies require the detection of image information in details,\nfor example, object detection. Based on Uncertainty Coefficient(UC), this paper\nproposes an innovative method CCUC for change comparison between two image\nsequences. The proposed method is computationally efficient and simple to\nimplement. The change comparison stems from video monitoring system. The\nlimited number of provided screens and a large number of monitoring cameras\nrequire the videos or image sequences ordered by change level. We demonstrate\nthis new method by applying it on two publicly available image sequences. The\nresults are able to show the method can distinguish the different change level\nfor sequences.'"
Berthold Horn,Horn_Berthold,arXiv:1612.08825,https://arxiv.org/abs/1612.08825,"b'Abstract:  Convolutions have long been regarded as fundamental to applied mathematics,\nphysics and engineering. Their mathematical elegance allows for common tasks\nsuch as numerical differentiation to be computed efficiently on large data\nsets. Efficient computation of convolutions is critical to artificial\nintelligence in real-time applications, like machine vision, where convolutions\nmust be continuously and efficiently computed on tens to hundreds of kilobytes\nper second. In this paper, we explore how convolutions are used in fundamental\nmachine vision applications. We present an accelerated n-dimensional\nconvolution package in the high performance computing language, Julia, and\ndemonstrate its efficacy in solving the time to contact problem for machine\nvision. Results are measured against synthetically generated videos and\nquantitatively assessed according to their mean squared error from the ground\ntruth. We achieve over an order of magnitude decrease in compute time and\nallocated memory for comparable machine vision applications. All code is\npackaged and integrated into the official Julia Package Manager to be used in\nvarious other scenarios.'"
Qing Hu,Hu_Qing,arXiv:1812.03505,https://arxiv.org/abs/1812.03505,"b'Abstract:  Hyperspectral imaging is a technique that allows for the creation of\nmulti-color images. At terahertz wavelengths, it has emerged as a prominent\ntool for a number of applications, ranging from non-ionizing cancer diagnosis\nand pharmaceutical characterization to non-destructive artifact testing.\nContemporary terahertz imaging systems typically rely on non-linear optical\ndown-conversion of a fiber-based near-infrared femtosecond laser, requiring\ncomplex optical systems. Here, we demonstrate hyperspectral imaging with\nchip-scale frequency combs based on terahertz quantum cascade lasers. The dual\ncombs are free-running and emit coherent terahertz radiation that covers a\nbandwidth of 220 GHz at 3.4 THz with ~10 {\\mu}W per line. The combination of\nthe fast acquisition rate of dual-comb spectroscopy with the monolithic design,\nscalability, and chip-scale size of the combs is highly appealing for future\nimaging applications in biomedicine and in the pharmaceutical industry.'"
Qing Hu,Hu_Qing,arXiv:1807.10401,https://arxiv.org/abs/1807.10401,"b'Abstract:  For many applications Optical Frequency Combs (OFCs) require a high degree of\ntemporal coherence (narrow linewidth). Commonly OFCs are generated in nonlinear\nmedia from a monochromatic narrow linewidth laser sources or from a mode-locked\nlaser pulses but in the all-important mid-infrared (MIR) and terahertz (THz)\nregions of spectrum OFCs can be generated intrinsically by the free-running\nquantum cascade lasers (QCLs) with high efficiency. These combs do not look\nlike conventional OFCs as the phases of each mode are different and in temporal\ndomain the OFC is a seemingly random combination of amplitude- and\nphase-modulated signals rather than a short pulse. Despite this\npseudo-randomness, the experimental evidence suggests that the linewidth of the\nQCL OFC is just as narrow as that of a QCL operating in the single mode. While\nuniversally acknowledged, this seemingly observation is not fully understood.\nIn this work we rigorously prove this fact by deriving the expression for the\nSchawlow-Townes linewidth of QCL OFC and offer a transparent physical\ninterpretation based on orthogonality of laser modes, indicating that despite\ntheir very different temporal profiles MIR and THz QCL OFCs are just as good\nfor most applications as any other OFC.'"
Qing Hu,Hu_Qing,arXiv:1805.05759,https://arxiv.org/abs/1805.05759,"b'Abstract:  We present here a new type of cold atom interferometry gravimeter based on\nBragg diffraction, which is able to increase the gravity measurement\nsensitivity and stability of common Raman atom gravimeters significantly. By\ncomparing with Raman transition, the principles and advantages of Bragg\ndiffraction-based atom gravimeters have been introduced. The theoretical model\nfor a time-domain Bragg atom gravimeter with atomic incident direction\nparallels to the wave vector of Bragg lasers has been constructed. Some key\ntechnical requirements for an nth-order Bragg diffraction-based atom gravimeter\nhave been deduced, including the temperature of atom cloud, the diameter,\ncurvature radius, frequency, intensity, and timing sequence of Bragg lasers,\netc. The analysis results were verified by the existing experimental data in\ndiscussion. The present study provides a good reference for the understanding\nand construction of a Bragg atom gravimeter.'"
Qing Hu,Hu_Qing,arXiv:1805.05707,https://arxiv.org/abs/1805.05707,"b'Abstract:  The loss of contrast due to atom expansion induced non-perfect Raman pulse\narea in atom interferometers is investigated systematically. Based on the\ntheoretical simulation, we find that the expansion of the atomic cloud results\nin a decrease of the {\\pi} pulse fidelity and a change of the {\\pi} pulse\nduration, which lead to a significant reduction in fringe contrast. We propose\na mitigation strategy of increasing the intensities of the second and third\nRaman pulses. Simulation results show that the fringe contrast can be improved\nby 13.6% in a typical atom interferometer gravimeter using this intensity\ncompensation strategy. We also evaluate the effects of this mitigation strategy\nin the case of a lower atomic cloud temperature and a larger Raman beam size\nunder different Raman pulse time interval conditions. This mitigation strategy\nhas potential applications in increasing the sensitivity of atom\ninterferometer-based precision measuring, including precision measuring of the\ngravity, gravity gradient, rotation, and magnetic field gradient, as well as\ntesting of the Einstein equivalence principle.'"
Qing Hu,Hu_Qing,arXiv:1805.05704,https://arxiv.org/abs/1805.05704,"b'Abstract:  We present the derivation of the frequency dependent scalar, vector, and\ntensor dynamical polarizabilities for the two hyperfine levels of the 87Rb atom\n5s ground state. Based on the characterization of the dynamical\npolarizabilities, we analyze and measure the differential vector and tensor\nlight shift between the 5s ground state sub-levels with near-resonant,\nstimulated Raman transitions. These results clarify that the tensor\npolarizabilities for the ground states of alkali atoms are absent when the\nlight field is far-detuned from the atomic resonance and the total electronic\nangular momentum J is a good quantum number. In the near resonant case, the\nlight shifts are non-trivial and the determination of the frequency dependent\nvector and tensor dynamic polarizabilities will help to achieve higher\nfidelities for applications of neutral atoms in quantum information and\nprecision measurements.'"
Qing Hu,Hu_Qing,arXiv:1805.05159,https://arxiv.org/abs/1805.05159,"b'Abstract:  Precisely evaluating the systematic error induced by the quadratic Zeeman\neffect is important for developing atom interferometer gravimeters aiming at an\naccuracy in the regime ( ). This paper reports on the experimental\ninvestigation of Raman spectroscopy-based magnetic field measurements and the\nevaluation of the systematic error in the Gravimetric Atom Interferometer\n(GAIN) due to quadratic Zeeman effect. We discuss Raman duration and frequency\nstep size dependent magnetic field measurement uncertainty, present vector\nlight shift (VLS) and tensor light shift (TLS) induced magnetic field\nmeasurement offset, and map the absolute magnetic field inside the\ninterferometer chamber of GAIN with an uncertainty of 0.72 nT and a spatial\nresolution of 12.8 mm. We evaluate the quadratic Zeeman effect induced gravity\nmeasurement error in GAIN as . The methods shown in this paper are important\nfor precisely mapping the absolute magnetic field in vacuum and reducing the\nquadratic Zeeman effect induced systematic error in Raman transition-based\nprecision measurements, such as atomic interferometer gravimeters.'"
Qing Hu,Hu_Qing,arXiv:1803.02913,https://arxiv.org/abs/1803.02913,"b""Abstract:  Topological materials bear gapped excitations in bulk yet protected gapless\nexcitations at boundaries. Magnetoplasmons (MPs), as high-frequency density\nexcitations of two-dimensional electron gas (2DEG) in a perpendicular magnetic\nfield, embody a prototype of band topology for bosons. The\ntime-reversal-breaking magnetic field opens a topological gap for bulk MPs up\nto the cyclotron frequency; topologically-protected edge magnetoplasmons (EMPs)\nbridge the bulk gap and propagate unidirectionally along system's boundaries.\nHowever, all the EMPs known to date adhere to physical edges where the electron\ndensity terminates abruptly. This restriction has made device application\nextremely difficult. Here we demonstrate a new class of topological edge\nplasmons -- domain-boundary magnetoplasmons (DBMPs), within a uniform edgeless\n2DEG. Such DBMPs arise at the domain boundaries of an engineered sign-changing\nmagnetic field and are protected by the difference of gap Chern numbers (+/-1)\nacross the magnetic domains. They propagate unidirectionally along the domain\nboundaries and are immune to domain defects. Moreover, they exhibit wide\ntunability in the microwave frequency range under an applied magnetic field or\ngate voltage. Our study opens a new direction to realize high-speed\nreconfigurable topological devices."""
Qing Hu,Hu_Qing,arXiv:1708.01445,https://arxiv.org/abs/1708.01445,"b'Abstract:  Diffusion of dopants in rutile is the fundamental process that determines the\nperformance of many devices in which rutile is used. The diffusion behavior is\nknown to be highly sample-dependent, but the reasons for this are less well\nunderstood. Here, rutile is studied by using first-principles calculations, in\norder to unravel the microscopic origins of the diverse diffusion behaviors for\ndifferent doping elements. Anomalous diffusion behavior in the open channel\nalong [001] direction is found: larger atoms include Sc and Zr have lower\nenergy barrier for diffusion via interstitial mechanism, apparently\ncontradicting their known slow diffusion rate. To resolve this, we present an\nalternate model for the overall diffusion rate of the large-size dopants in\nrutile, showing that parallel to the [001] channel, it is limited by the\nformation of the interstitial states, whereas in the direction perpendicular to\n[001], it proceeds via a kick-out mechanism. By contrast, Co and Ni, prefer to\nstay in the interstitial site of rutile, and have conventional diffusion with a\nvery small migration barrier in the [001] channel. This leads to highly\nanisotropic and fast diffusion. The diffusion mechanisms found in the present\nstudy can explain the diffusion data measured by experiments, and these\nfindings provide novel understanding for the classic diffusion topic.'"
Qing Hu,Hu_Qing,arXiv:1605.09436,https://arxiv.org/abs/1605.09436,"b""Abstract:  Dual comb spectroscopy allows for high-resolution spectra to be measured over\nbroad bandwidths, but an essential requirement for coherent integration is the\navailability of a phase reference. Usually, this means that the combs' phase\nand timing errors must be measured and either minimized by stabilization or\nremoved by correction, limiting the technique's applicability. In this work, we\ndemonstrate that it is possible to extract the phase and timing signals of a\nmultiheterodyne spectrum completely computationally, without any extra\nmeasurements or optical elements. These techniques are viable even when the\nrelative linewidth exceeds the repetition rate difference, and can tremendously\nsimplify any dual comb system. By reconceptualizing frequency combs in terms of\nthe temporal structure of their phase noise, not their frequency stability, we\nare able to greatly expand the scope of multiheterodyne techniques."""
Qing Hu,Hu_Qing,arXiv:1604.01048,https://arxiv.org/abs/1604.01048,"b'Abstract:  Frequency combs based on terahertz quantum cascade lasers feature broadband\ncoverage and high output powers in a compact package, making them an attractive\noption for broadband spectroscopy. Here, we demonstrate the first\nmulti-heterodyne spectroscopy using two terahertz quantum cascade laser combs.\nWith just 100 $\\mu$s of integration time, we achieve peak signal-to-noise\nratios exceeding 60 dB and a spectral coverage greater than 250 GHz centered at\n2.8 THz. Even with room-temperature detectors we are able to achieve peak\nsignal-to-noise ratios of 50 dB, and as a proof-of-principle we use these combs\nto measure the broadband transmission spectrum of etalon samples. Finally, we\nshow that with proper signal processing, it is possible to extend the\nmulti-heterodyne spectroscopy to quantum cascade laser combs operating in\npulsed mode, greatly expanding the range of quantum cascade lasers that could\nbe suitable for these techniques.'"
Qing Hu,Hu_Qing,arXiv:1603.08266,https://arxiv.org/abs/1603.08266,"b'Abstract:  Two-dimensional molecular aggregate (2DMA), a thin sheet of strongly\ninteracting dipole molecules self-assembled at close distance on an ordered\nlattice, is a fascinating fluorescent material. It is distinctively different\nfrom the single or colloidal dye molecules or quantum dots in most previous\nresearch. In this paper, we verify for the first time that when a 2DMA is\nplaced at a nanometric distance from a metallic substrate, the strong and\ncoherent interaction between the dipoles inside the 2DMA dominates its\nfluorescent decay at picosecond timescale. Our streak-camera lifetime\nmeasurement and interacting lattice-dipole calculation reveal that the\nmetal-mediated dipole-dipole interaction shortens the fluorescent lifetime to\nabout one half and increases the energy dissipation rate by ten times than\nexpected from the noninteracting single-dipole picture. Our finding can enrich\nour understanding of nanoscale energy transfer in molecular excitonic systems\nand may designate a new direction for developing fast and efficient\noptoelectronic devices.'"
Qing Hu,Hu_Qing,arXiv:1511.08620,https://arxiv.org/abs/1511.08620,"b'Abstract:  Interstitials (carbon and nitrogen) are crucial alloying elements for\noptimizing the mechanical performance of the twinning-induced plasticity (TWIP)\nsteels in terms of the stacking fault energy (SFE). First-principles\ncalculations have been performed to study the effect of interstitial-induced\nlattice expansion on the SFE. Comparing the predictions with the SFEs measured\nfor alloys containing C and N, our results suggest that the dominant effect of\nthese interstitials on the SFE is due to the lattice expansion effect.'"
Qing Hu,Hu_Qing,arXiv:1505.05443,https://arxiv.org/abs/1505.05443,"b'Abstract:  First-principles alloy theory, formulated within the exact muffin-tin\norbitals method in combination with the coherent-potential approximation, is\nused to study the mechanical properties of ferromagnetic body-centered cubic\n(bcc) Fe$_{1-x}$M$_x$ alloys (M=Mn or Ni, $0\\le x \\le 0.1$). We consider\nseveral physical parameters accessible from \\emph{ab initio} calculations and\ntheir combinations in various phenomenological models to compare the effect of\nMn and Ni on the properties of Fe. Alloying is found to slightly alter the\nlattice parameters and produce noticeable influence on elastic moduli. Both Mn\nand Ni decrease the surface energy and the unstable stacking fault energy\nassociated with the $\\{110\\}$ surface facet and the $\\{110\\}\\langle111\\rangle$\nslip system, respectively. Nickel is found to produce larger effect on the\nplanar fault energies than Mn. The semi-empirical ductility criteria by Rice\nand Pugh consistently predict that Ni enhances the ductility of Fe but give\ncontradictory results in the case of Mn doping. The origin of the discrepancy\nbetween the two criteria is discussed and an alternative measure of the\nductile-brittle behavior based on the theoretical cleavage strength and\nsingle-crystal shear modulus $G\\{110\\}\\langle111\\rangle$ is proposed.'"
Qing Hu,Hu_Qing,arXiv:1504.07867,https://arxiv.org/abs/1504.07867,"b'Abstract:  We demonstrate an unexpectedly strong surface-plasmonic absorption at the\ninterface of silver and high-index dielectrics based on electron and photon\nspectroscopy. The measured bandwidth and intensity of absorption deviate\nsignificantly from the classical theory. Our density-functional calculation\nwell predicts the occurrence of this phenomenon. It reveals that due to the low\nmetal-to-dielectric work function at such interfaces, conduction electrons can\ndisplay a drastic quantum spillover, causing the interfacial electron-hole pair\nproduction to dominate the decay of surface plasmons. This finding can be of\nfundamental importance in understanding and designing quantum nano-plasmonic\ndevices that utilize noble metals and high-index dielectrics.'"
Qing Hu,Hu_Qing,arXiv:1311.0946,https://arxiv.org/abs/1311.0946,"b'Abstract:  On-chip nanophotonics serves as the foundation for the new generation of\ninformation technology, but it is challenged by the diffraction limit of light.\nWith the capabilities of confining light into (deep) subwavelength volumes,\nplasmonics makes it possible to dramatically miniaturize optical devices so as\nto integrate them into silicon chips. Here we demonstrate that by cascading\nnano-corrugation gratings with different periodicities on silver nanowires atop\nsilicon, different colors can be spatially separated and chronologically\nreleased at different grating junctions. The released light frequency depends\non the grating arrangement and corrugation periodicities. Hence the nanowire\nacts as a spectral splitter for sorting/demultiplexing photons at different\nnano-scale positions with a ten-femtosecond-level interval. Such nanowires can\nbe constructed further into compact 2D networks or circuits. We believe that\nthis study provides a new and promising approach for realizing\nspatiotemporal-sensitive spectral splitting and optical signal processing on\nnanoscales, and for general integration of nanophotonics with microelectronics.'"
Qing Hu,Hu_Qing,arXiv:1307.6905,https://arxiv.org/abs/1307.6905,"b'Abstract:  We present new formulae for the matrix elements of one-body and two-body\nphysical operators in compact forms, which are applicable to arbitrary\nHartree-Fock-Bogoliubov wave functions, including those for multi-quasiparticle\nexcitations. The test calculations show that our formulae may substantially\naccelerate the process of symmetry restoration when applied to the heavy\nnuclear system.'"
Qing Hu,Hu_Qing,arXiv:1306.3051,https://arxiv.org/abs/1306.3051,"b'Abstract:  Overlap between Hartree-Fock-Bogoliubov(HFB) vacua is very important in the\nbeyond mean-field calculations. However, in the HFB transformation, the $U,V$\nmatrices are sometimes singular due to the exact emptiness ($v_i=0$) or full\noccupation ($u_i=0$) of some single-particle orbits. This singularity may cause\nsome problem in evaluating the overlap between HFB vacua through Pfaffian. We\nfound that this problem can be well avoided by setting those zero occupation\nnumbers to some tiny values (e.g., $u_i,v_i=10^{-8}$). This treatment does not\nchange the HFB vacuum state because $u_i^2,v_i^2=10^{-16}$ are numerically zero\nrelative to 1. Therefore, for arbitrary HFB transformation, we say that the\n$U,V$ matrices can always be nonsingular. From this standpoint, we present a\nnew convenient Pfaffian formula for the overlap between arbitrary HFB vacua,\nwhich is especially suitable for symmetry restoration. Testing calculations\nhave been performed for this new formula. It turns out that our method is\nreliable and accurate in evaluating the overlap between arbitrary HFB vacua.'"
Qing Hu,Hu_Qing,arXiv:1208.5776,https://arxiv.org/abs/1208.5776,"b'Abstract:  We report on a heterodyne receiver designed to observe the astrophysically\nimportant neutral atomic oxygen [OI] line at 4.7448 THz. The local oscillator\nis a third-order distributed feedback Quantum Cascade Laser operating in\ncontinuous wave mode at 4.741 THz. A quasi-optical, superconducting NbN hot\nelectron bolometer is used as the mixer. We recorded a double sideband receiver\nnoise temperature (T^DSB_rec) of 815 K, which is ~7 times the quantum noise\nlimit (h{\\nu}/2k_B) and an Allan variance time of 15 s at an effective noise\nfluctuation bandwidth of 18 MHz. Heterodyne performance was confirmed by\nmeasuring a methanol line spectrum.'"
Qing Hu,Hu_Qing,arXiv:1006.5915,https://arxiv.org/abs/1006.5915,"b'Abstract:  We study a ""strongly-coupled"" (SC) polariton system formed between the\natom-like intersubband transitions in a semiconductor nanostructure and the THz\noptical modes that are localised at the edges of a gold aperture. The\npolaritons can be excited optically, by incoherent excitation with bandgap\nradiation, and we find that they also coherently scatter the same input laser,\nto give strikingly sharp ""sideband"" (SB) spectral peaks in the backscattered\nspectrum. The SB intensity is a sensitive track of the polariton density and\nthey can be detected down to a quantum noise floor that is more than 2500 times\nlower than the excitation thresholds of comparable quantum cascade laser\ndiodes. Compared with other coherent scattering mechanisms, higher order SB\nscattering events are readily observable, and we speculate that the effect may\nfind utility as a passive all-optical wavelength shifting mechanism in\ntelecommunications systems.'"
Qing Hu,Hu_Qing,arXiv:0910.2959,https://arxiv.org/abs/0910.2959,"b'Abstract:  We develop simple density-matrix models to describe the role of coherence in\nresonant-tunneling (RT) transport of quantum-cascade lasers (QCLs).\nSpecifically, we investigate the effects of coherent coupling between the\nlasing levels with other levels on the transport properties and gain spectra.\nIn the first part of the paper, we use a three-level density-matrix model to\nobtain useful analytical expressions for current transport through the injector\nbarrier in a QCL. An expression for the slope discontinuity in the\ncurrent-voltage characteristics at the lasing threshold is derived. This value\nis shown to be a direct measure of the population inversion at threshold, and\ncontradicts the previously held belief of it being indicative of ratio of the\nlaser level lifetimes. In the second part of the paper, we use density matrices\nto compute the gain spectrum for a resonant-phonon terahertz QCL design. The\nlarge anticrossing of the doublet of lower radiative levels is reflected in a\nbroad gain linewidth due to a coherent RT assisted depopulation process. At\ncertain bias conditions, the gain spectrum exhibits double peaks which is\nsupported by experimental observations.'"
Qing Hu,Hu_Qing,arXiv:cond-mat/0703354,https://arxiv.org/abs/cond-mat/0703354,"b'Abstract:  It is shown that the errors of present-day exchange-correlation (xc)\nfunctionals are rather short ranged. For extended systems the correction can\ntherefore be evaluated by analyzing properly chosen clusters and employing\nhighest-quality quantum chemistry methods. The xc correction rapidly approaches\na universal dependence with cluster size. The method is applicable to bulk\nsystems as well as to defects in the bulk and at surfaces. It is demonstrated\nhere for CO adsorption at transition-metal surfaces, where present-day xc\nfunctionals dramatically fail to predict the correct adsorption site, and for\nthe crystal bulk cohesive energy.'"
Piotr Indyk,Indyk_Piotr,arXiv:1902.03534,https://arxiv.org/abs/1902.03534,"b'Abstract:  We study the classic set cover problem from the perspective of sub-linear\nalgorithms. Given access to a collection of $m$ sets over $n$ elements in the\nquery model, we show that sub-linear algorithms derived from existing\ntechniques have almost tight query complexities.\nOn one hand, first we show an adaptation of the streaming algorithm presented\nin Har-Peled et al. [2016] to the sub-linear query model, that returns an\n$\\alpha$-approximate cover using $\\tilde{O}(m(n/k)^{1/(\\alpha-1)} + nk)$\nqueries to the input, where $k$ denotes the value of a minimum set cover. We\nthen complement this upper bound by proving that for lower values of $k$, the\nrequired number of queries is $\\tilde{\\Omega}(m(n/k)^{1/(2\\alpha)})$, even for\nestimating the optimal cover size. Moreover, we prove that even checking\nwhether a given collection of sets covers all the elements would require\n$\\Omega(nk)$ queries. These two lower bounds provide strong evidence that the\nupper bound is almost tight for certain values of the parameter $k$.\nOn the other hand, we show that this bound is not optimal for larger values\nof the parameter $k$, as there exists a $(1+\\varepsilon)$-approximation\nalgorithm with $\\tilde{O}(mn/k\\varepsilon^2)$ queries. We show that this bound\nis essentially tight for sufficiently small constant $\\varepsilon$, by\nestablishing a lower bound of $\\tilde{\\Omega}(mn/k)$ query complexity.'"
Piotr Indyk,Indyk_Piotr,arXiv:1902.03519,https://arxiv.org/abs/1902.03519,"b'Abstract:  We study the fair variant of the classic $k$-median problem introduced by\nChierichetti et al. [2017]. In the standard $k$-median problem, given an input\npointset $P$, the goal is to find $k$ centers $C$ and assign each input point\nto one of the centers in $C$ such that the average distance of points to their\ncluster center is minimized.\nIn the fair variant of $k$-median, the points are colored, and the goal is to\nminimize the same average distance objective while ensuring that all clusters\nhave an ""approximately equal"" number of points of each color.\nChierichetti et al. proposed a two-phase algorithm for fair $k$-clustering.\nIn the first step, the pointset is partitioned into subsets called fairlets\nthat satisfy the fairness requirement and approximately preserve the $k$-median\nobjective. In the second step, fairlets are merged into $k$ clusters by one of\nthe existing $k$-median algorithms. The running time of this algorithm is\ndominated by the first step, which takes super-quadratic time.\nIn this paper, we present a practical approximate fairlet decomposition\nalgorithm that runs in nearly linear time. Our algorithm additionally allows\nfor finer control over the balance of resulting clusters than the original\nwork. We complement our theoretical bounds with empirical evaluation.'"
Piotr Indyk,Indyk_Piotr,arXiv:1901.08544,https://arxiv.org/abs/1901.08544,"b'Abstract:  Most of the efficient sublinear-time indexing algorithms for the\nhigh-dimensional nearest neighbor search problem (NNS) are based on space\npartitions of the ambient space $\\mathbb{R}^d$. Inspired by recent theoretical\nwork on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,\nWaingarten STOC 2018, FOCS 2018], we develop a new framework for constructing\nsuch partitions that reduces the problem to balanced graph partitioning\nfollowed by supervised classification. We instantiate this general approach\nwith the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural\nnetworks, respectively, to obtain a new partitioning procedure called Neural\nLocality-Sensitive Hashing (Neural LSH). On several standard benchmarks for\nNNS, our experiments show that the partitions found by Neural LSH consistently\noutperform partitions found by quantization- and tree-based methods.'"
Piotr Indyk,Indyk_Piotr,arXiv:1807.11648,https://arxiv.org/abs/1807.11648,"b""Abstract:  We study a spectral generalization of classical combinatorial graph spanners\nto the spectral setting. Given a set of vectors $V\\subseteq \\Re^d$, we say a\nset $U\\subseteq V$ is an $\\alpha$-spectral spanner if for all $v\\in V$ there is\na probability distribution $\\mu_v$ supported on $U$ such that $$vv^\\intercal\n\\preceq \\alpha\\cdot\\mathbb{E}_{u\\sim\\mu_v} uu^\\intercal.$$ We show that any set\n$V$ has an $\\tilde{O}(d)$-spectral spanner of size $\\tilde{O}(d)$ and this\nbound is almost optimal in the worst case.\nWe use spectral spanners to study composable core-sets for spectral problems.\nWe show that for many objective functions one can use a spectral spanner,\nindependent of the underlying functions, as a core-set and obtain almost\noptimal composable core-sets. For example, for the determinant maximization\nproblem we obtain an $\\tilde{O}(k)^k$-composable core-set and we show that this\nis almost optimal in the worst case.\nOur algorithm is a spectral analogue of the classical greedy algorithm for\nfinding (combinatorial) spanners in graphs. We expect that our spanners find\nmany other applications in distributed or parallel models of computation. Our\nproof is spectral. As a side result of our techniques, we show that the rank of\ndiagonally dominant lower-triangular matrices are robust under `small\nperturbations' which could be of independent interests."""
Piotr Indyk,Indyk_Piotr,arXiv:1807.00112,https://arxiv.org/abs/1807.00112,"b'Abstract:  We consider the $(1+\\epsilon)$-approximate nearest neighbor search problem:\ngiven a set $X$ of $n$ points in a $d$-dimensional space, build a data\nstructure that, given any query point $y$, finds a point $x \\in X$ whose\ndistance to $y$ is at most $(1+\\epsilon) \\min_{x \\in X} \\|x-y\\|$ for an\naccuracy parameter $\\epsilon \\in (0,1)$. Our main result is a data structure\nthat occupies only $O(\\epsilon^{-2} n \\log(n) \\log(1/\\epsilon))$ bits of space,\nassuming all point coordinates are integers in the range $\\{-n^{O(1)} \\ldots\nn^{O(1)}\\}$, i.e., the coordinates have $O(\\log n)$ bits of precision. This\nimproves over the best previously known space bound of $O(\\epsilon^{-2} n\n\\log(n)^2)$, obtained via the randomized dimensionality reduction method of\nJohnson and Lindenstrauss (1984). We also consider the more general problem of\nestimating all distances from a collection of query points to all data points\n$X$, and provide almost tight upper and lower bounds for the space complexity\nof this problem.'"
Piotr Indyk,Indyk_Piotr,arXiv:1806.09823,https://arxiv.org/abs/1806.09823,"b'Abstract:  The nearest neighbor problem is defined as follows: Given a set $P$ of $n$\npoints in some metric space $(X,D)$, build a data structure that, given any\npoint $q$, returns a point in $P$ that is closest to $q$ (its ""nearest\nneighbor"" in $P$). The data structure stores additional information about the\nset $P$, which is then used to find the nearest neighbor without computing all\ndistances between $q$ and $P$. The problem has a wide range of applications in\nmachine learning, computer vision, databases and other fields.\nTo reduce the time needed to find nearest neighbors and the amount of memory\nused by the data structure, one can formulate the {\\em approximate} nearest\nneighbor problem, where the the goal is to return any point $p\' \\in P$ such\nthat the distance from $q$ to $p\'$ is at most $c \\cdot \\min_{p \\in P} D(q,p)$,\nfor some $c \\geq 1$. Over the last two decades, many efficient solutions to\nthis problem were developed. In this article we survey these developments, as\nwell as their connections to questions in geometric functional analysis and\ncombinatorial geometry.'"
Piotr Indyk,Indyk_Piotr,arXiv:1711.01520,https://arxiv.org/abs/1711.01520,"b'Abstract:  We introduce a new distance-preserving compact representation of\nmulti-dimensional point-sets. Given $n$ points in a $d$-dimensional space where\neach coordinate is represented using $B$ bits (i.e., $dB$ bits per point), it\nproduces a representation of size $O( d \\log(d B/\\epsilon) + \\log n)$ bits per\npoint from which one can approximate the distances up to a factor of $1 \\pm\n\\epsilon$. Our algorithm almost matches the recent bound\nof~\\cite{indyk2017near} while being much simpler. We compare our algorithm to\nProduct Quantization (PQ)~\\cite{jegou2011product}, a state of the art heuristic\nmetric compression method. We evaluate both algorithms on several data sets:\nSIFT (used in \\cite{jegou2011product}), MNIST~\\cite{lecun1998mnist}, New York\nCity taxi time series~\\cite{guha2016robust} and a synthetic one-dimensional\ndata set embedded in a high-dimensional space. With appropriately tuned\nparameters, our algorithm produces representations that are comparable to or\nbetter than those produced by PQ, while having provable guarantees on its\nperformance.'"
Piotr Indyk,Indyk_Piotr,arXiv:1706.06935,https://arxiv.org/abs/1706.06935,"b'Abstract:  There is much interest in integrating millimeter wave radios (mmWave) into\nwireless LANs and 5G cellular networks to benefit from their multiple GHz of\navailable spectrum. Yet unlike existing technologies, e.g., WiFi, mmWave radios\nrequire highly directional antennas. Since the antennas have pencil-beams, the\ntransmitter and receiver need to align their antenna beams before they can\ncommunicate. Existing solutions scan the entire space to find the best\nalignment. Such a process has been shown to introduce up to seconds of delay,\nand is unsuitable for wireless networks where an access point has to quickly\nswitch between users and accommodate mobile clients.\nThis paper presents Rapid-Link, a new protocol that can find the best mmWave\nbeam alignment without scanning the space. Given all possible directions for\nsetting the antenna beam, Rapid-Link provably finds the optimal direction in\nlogarithmic number of measurements. Further, Rapid-Link works within the\nexisting 802.11ad standard for mmWave LAN, and can support both clients and\naccess points. We have implemented Rapid-Link in a mmWave radio and evaluated\nit empirically. Our results show that it reduces beam alignment delay by orders\nof magnitude. In particular, for highly directional mmWave devices operating\nunder 802.11ad, the delay drops from over a second to 2.5 ms.'"
Piotr Indyk,Indyk_Piotr,arXiv:1704.02958,https://arxiv.org/abs/1704.02958,"b'Abstract:  Empirical risk minimization (ERM) is ubiquitous in machine learning and\nunderlies most supervised learning methods. While there has been a large body\nof work on algorithms for various ERM problems, the exact computational\ncomplexity of ERM is still not understood. We address this issue for multiple\npopular ERM problems including kernel SVMs, kernel ridge regression, and\ntraining the final layer of a neural network. In particular, we give\nconditional hardness results for these problems based on complexity-theoretic\nassumptions such as the Strong Exponential Time Hypothesis. Under these\nassumptions, we show that there are no algorithms that solve the aforementioned\nERM problems to high accuracy in sub-quadratic time. We also give similar\nhardness results for computing the gradient of the empirical loss, which is the\nmain computational burden in many non-convex learning tasks.'"
Piotr Indyk,Indyk_Piotr,arXiv:1609.08739,https://arxiv.org/abs/1609.08739,"b'Abstract:  In the Sparse Linear Regression (SLR) problem, given a $d \\times n$ matrix\n$M$ and a $d$-dimensional query $q$, the goal is to compute a $k$-sparse\n$n$-dimensional vector $\\tau$ such that the error $||M \\tau-q||$ is minimized.\nThis problem is equivalent to the following geometric problem: given a set $P$\nof $n$ points and a query point $q$ in $d$ dimensions, find the closest\n$k$-dimensional subspace to $q$, that is spanned by a subset of $k$ points in\n$P$. In this paper, we present data-structures/algorithms and conditional lower\nbounds for several variants of this problem (such as finding the closest\ninduced $k$ dimensional flat/simplex instead of a subspace).\nIn particular, we present approximation algorithms for the online variants of\nthe above problems with query time $\\tilde O(n^{k-1})$, which are of interest\nin the ""low sparsity regime"" where $k$ is small, e.g., $2$ or $3$. For $k=d$,\nthis matches, up to polylogarithmic factors, the lower bound that relies on the\naffinely degenerate conjecture (i.e., deciding if $n$ points in $\\mathbb{R}^d$\ncontains $d+1$ points contained in a hyperplane takes $\\Omega(n^d)$ time).\nMoreover, our algorithms involve formulating and solving several geometric\nsubproblems, which we believe to be of independent interest.'"
Piotr Indyk,Indyk_Piotr,arXiv:1609.06295,https://arxiv.org/abs/1609.06295,"b'Abstract:  The metric sketching problem is defined as follows. Given a metric on $n$\npoints, and $\\epsilon>0$, we wish to produce a small size data structure\n(sketch) that, given any pair of point indices, recovers the distance between\nthe points up to a $1+\\epsilon$ distortion. In this paper we consider metrics\ninduced by $\\ell_2$ and $\\ell_1$ norms whose spread (the ratio of the diameter\nto the closest pair distance) is bounded by $\\Phi>0$. A well-known\ndimensionality reduction theorem due to Johnson and Lindenstrauss yields a\nsketch of size $O(\\epsilon^{-2} \\log (\\Phi n) n\\log n)$, i.e., $O(\\epsilon^{-2}\n\\log (\\Phi n) \\log n)$ bits per point. We show that this bound is not optimal,\nand can be substantially improved to $O(\\epsilon^{-2}\\log(1/\\epsilon) \\cdot\n\\log n + \\log\\log \\Phi)$ bits per point. Furthermore, we show that our bound is\ntight up to a factor of $\\log(1/\\epsilon)$.\nWe also consider sketching of general metrics and provide a sketch of size\n$O(n\\log(1/\\epsilon)+ \\log\\log \\Phi)$ bits per point, which we show is optimal.'"
Piotr Indyk,Indyk_Piotr,arXiv:1604.02188,https://arxiv.org/abs/1604.02188,"b'Abstract:  Motivated by applications in computer vision and databases, we introduce and\nstudy the Simultaneous Nearest Neighbor Search (SNN) problem. Given a set of\ndata points, the goal of SNN is to design a data structure that, given a\ncollection of queries, finds a collection of close points that are compatible\nwith each other. Formally, we are given $k$ query points $Q=q_1,\\cdots,q_k$,\nand a compatibility graph $G$ with vertices in $Q$, and the goal is to return\ndata points $p_1,\\cdots,p_k$ that minimize (i) the weighted sum of the\ndistances from $q_i$ to $p_i$ and (ii) the weighted sum, over all edges $(i,j)$\nin the compatibility graph $G$, of the distances between $p_i$ and $p_j$. The\nproblem has several applications, where one wants to return a set of consistent\nanswers to multiple related queries. This generalizes well-studied\ncomputational problems, including NN, Aggregate NN and the 0-extension problem.\nIn this paper we propose and analyze the following general two-step method\nfor designing efficient data structures for SNN. In the first step, for each\nquery point $q_i$ we find its (approximate) nearest neighbor point $\\hat{p}_i$;\nthis can be done efficiently using existing approximate nearest neighbor\nstructures. In the second step, we solve an off-line optimization problem over\nsets $q_1,\\cdots,q_k$ and $\\hat{p}_1,\\cdots,\\hat{p}_k$; this can be done\nefficiently given that $k$ is much smaller than $n$. Even though\n$\\hat{p}_1,\\cdots,\\hat{p}_k$ might not constitute the optimal answers to\nqueries $q_1,\\cdots,q_k$, we show that, for the unweighted case, the resulting\nalgorithm is $O(\\log k/\\log \\log k)$-approximation. Also, we show that the\napproximation factor can be in fact reduced to a constant for compatibility\ngraphs frequently occurring in practice.\nFinally, we show that the ""empirical approximation factor"" provided by the\nabove approach is very close to 1.'"
Piotr Indyk,Indyk_Piotr,arXiv:1511.07070,https://arxiv.org/abs/1511.07070,"b'Abstract:  Regular expressions constitute a fundamental notion in formal language theory\nand are frequently used in computer science to define search patterns. A\nclassic algorithm for these problems constructs and simulates a\nnon-deterministic finite automaton corresponding to the expression, resulting\nin an $O(mn)$ running time (where $m$ is the length of the pattern and $n$ is\nthe length of the text). This running time can be improved slightly (by a\npolylogarithmic factor), but no significantly faster solutions are known. At\nthe same time, much faster algorithms exist for various special cases of\nregular expressions, including dictionary matching, wildcard matching, subset\nmatching, word break problem etc.\nIn this paper, we show that the complexity of regular expression matching can\nbe characterized based on its {\\em depth} (when interpreted as a formula). Our\nresults hold for expressions involving concatenation, OR, Kleene star and\nKleene plus. For regular expressions of depth two (involving any combination of\nthe above operators), we show the following dichotomy: matching and membership\ntesting can be solved in near-linear time, except for ""concatenations of\nstars"", which cannot be solved in strongly sub-quadratic time assuming the\nStrong Exponential Time Hypothesis (SETH). For regular expressions of depth\nthree the picture is more complex. Nevertheless, we show that all problems can\neither be solved in strongly sub-quadratic time, or cannot be solved in\nstrongly sub-quadratic time assuming SETH.\nAn intriguing special case of membership testing involves regular expressions\nof the form ""a star of an OR of concatenations"", e.g., $[a|ab|bc]^*$. This\ncorresponds to the so-called {\\em word break} problem, for which a dynamic\nprogramming algorithm with a runtime of (roughly) $O(n\\sqrt{m})$ is known. We\nshow that the latter bound is not tight and improve the runtime to\n$O(nm^{0.44\\ldots})$.'"
Piotr Indyk,Indyk_Piotr,arXiv:1509.02897,https://arxiv.org/abs/1509.02897,"b'Abstract:  We show the existence of a Locality-Sensitive Hashing (LSH) family for the\nangular distance that yields an approximate Near Neighbor Search algorithm with\nthe asymptotically optimal running time exponent. Unlike earlier algorithms\nwith this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn\n2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also\nintroduce a multiprobe version of this algorithm, and conduct experimental\nevaluation on real and synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for\nthe quality of any LSH family for angular distance. Our lower bound implies\nthat the above LSH family exhibits a trade-off between evaluation time and\nquality that is close to optimal for a natural class of LSH functions.'"
Piotr Indyk,Indyk_Piotr,arXiv:1509.00118,https://arxiv.org/abs/1509.00118,"b'Abstract:  We consider the classic Set Cover problem in the data stream model. For $n$\nelements and $m$ sets ($m\\geq n$) we give a $O(1/\\delta)$-pass algorithm with a\nstrongly sub-linear $\\tilde{O}(mn^{\\delta})$ space and logarithmic\napproximation factor. This yields a significant improvement over the earlier\nalgorithm of Demaine et al. [DIMV14] that uses exponentially larger number of\npasses. We complement this result by showing that the tradeoff between the\nnumber of passes and space exhibited by our algorithm is tight, at least when\nthe approximation factor is equal to $1$. Specifically, we show that any\nalgorithm that computes set cover exactly using $({1 \\over 2\\delta}-1)$ passes\nmust use $\\tilde{\\Omega}(mn^{\\delta})$ space in the regime of $m=O(n)$.\nFurthermore, we consider the problem in the geometric setting where the\nelements are points in $\\mathbb{R}^2$ and sets are either discs, axis-parallel\nrectangles, or fat triangles in the plane, and show that our algorithm (with a\nslight modification) uses the optimal $\\tilde{O}(n)$ space to find a\nlogarithmic approximation in $O(1/\\delta)$ passes.\nFinally, we show that any randomized one-pass algorithm that distinguishes\nbetween covers of size 2 and 3 must use a linear (i.e., $\\Omega(mn)$) amount of\nspace. This is the first result showing that a randomized, approximate\nalgorithm cannot achieve a space bound that is sublinear in the input size.\nThis indicates that using multiple passes might be necessary in order to\nachieve sub-linear space bounds for this problem while guaranteeing small\napproximation factors.'"
Piotr Indyk,Indyk_Piotr,arXiv:1504.07648,https://arxiv.org/abs/1504.07648,"b'Abstract:  For every fixed constant $\\alpha > 0$, we design an algorithm for computing\nthe $k$-sparse Walsh-Hadamard transform of an $N$-dimensional vector $x \\in\n\\mathbb{R}^N$ in time $k^{1+\\alpha} (\\log N)^{O(1)}$. Specifically, the\nalgorithm is given query access to $x$ and computes a $k$-sparse $\\tilde{x} \\in\n\\mathbb{R}^N$ satisfying $\\|\\tilde{x} - \\hat{x}\\|_1 \\leq c \\|\\hat{x} -\nH_k(\\hat{x})\\|_1$, for an absolute constant $c > 0$, where $\\hat{x}$ is the\ntransform of $x$ and $H_k(\\hat{x})$ is its best $k$-sparse approximation. Our\nalgorithm is fully deterministic and only uses non-adaptive queries to $x$\n(i.e., all queries are determined and performed in parallel when the algorithm\nstarts).\nAn important technical tool that we use is a construction of nearly optimal\nand linear lossless condensers which is a careful instantiation of the GUV\ncondenser (Guruswami, Umans, Vadhan, JACM 2009). Moreover, we design a\ndeterministic and non-adaptive $\\ell_1/\\ell_1$ compressed sensing scheme based\non general lossless condensers that is equipped with a fast reconstruction\nalgorithm running in time $k^{1+\\alpha} (\\log N)^{O(1)}$ (for the GUV-based\ncondenser) and is of independent interest. Our scheme significantly simplifies\nand improves an earlier expander-based construction due to Berinde, Gilbert,\nIndyk, Karloff, Strauss (Allerton 2008).\nOur methods use linear lossless condensers in a black box fashion; therefore,\nany future improvement on explicit constructions of such condensers would\nimmediately translate to improved parameters in our framework (potentially\nleading to $k (\\log N)^{O(1)}$ reconstruction time with a reduced exponent in\nthe poly-logarithmic factor, and eliminating the extra parameter $\\alpha$).\nFinally, by allowing the algorithm to use randomness, while still using\nnon-adaptive queries, the running time of the algorithm can be improved to\n$\\tilde{O}(k \\log^3 N)$.'"
Piotr Indyk,Indyk_Piotr,arXiv:1504.01076,https://arxiv.org/abs/1504.01076,"b""Abstract:  We initiate the study of trade-offs between sparsity and the number of\nmeasurements in sparse recovery schemes for generic norms. Specifically, for a\nnorm $\\|\\cdot\\|$, sparsity parameter $k$, approximation factor $K>0$, and\nprobability of failure $P>0$, we ask: what is the minimal value of $m$ so that\nthere is a distribution over $m \\times n$ matrices $A$ with the property that\nfor any $x$, given $Ax$, we can recover a $k$-sparse approximation to $x$ in\nthe given norm with probability at least $1-P$? We give a partial answer to\nthis problem, by showing that for norms that admit efficient linear sketches,\nthe optimal number of measurements $m$ is closely related to the doubling\ndimension of the metric induced by the norm $\\|\\cdot\\|$ on the set of all\n$k$-sparse vectors. By applying our result to specific norms, we cast known\nmeasurement bounds in our general framework (for the $\\ell_p$ norms, $p \\in\n[1,2]$) as well as provide new, measurement-efficient schemes (for the\nEarth-Mover Distance norm). The latter result directly implies more succinct\nlinear sketches for the well-studied planar $k$-median clustering problem.\nFinally, our lower bound for the doubling dimension of the EMD norm enables us\nto address the open question of [Frahling-Sohler, STOC'05] about the space\ncomplexity of clustering problems in the dynamic streaming model."""
Piotr Indyk,Indyk_Piotr,arXiv:1412.3040,https://arxiv.org/abs/1412.3040,"b'Abstract:  Visualizations are frequently used as a means to understand trends and gather\ninsights from datasets, but often take a long time to generate. In this paper,\nwe focus on the problem of rapidly generating approximate visualizations while\npreserving crucial visual proper- ties of interest to analysts. Our primary\nfocus will be on sampling algorithms that preserve the visual property of\nordering; our techniques will also apply to some other visual properties. For\ninstance, our algorithms can be used to generate an approximate visualization\nof a bar chart very rapidly, where the comparisons between any two bars are\ncorrect. We formally show that our sampling algorithms are generally applicable\nand provably optimal in theory, in that they do not take more samples than\nnecessary to generate the visualizations with ordering guarantees. They also\nwork well in practice, correctly ordering output groups while taking orders of\nmagnitude fewer samples and much less time than conventional sampling schemes.'"
Piotr Indyk,Indyk_Piotr,arXiv:1412.0348,https://arxiv.org/abs/1412.0348,"b'Abstract:  The edit distance (a.k.a. the Levenshtein distance) between two strings is\ndefined as the minimum number of insertions, deletions or substitutions of\nsymbols needed to transform one string into another. The problem of computing\nthe edit distance between two strings is a classical computational task, with a\nwell-known algorithm based on dynamic programming. Unfortunately, all known\nalgorithms for this problem run in nearly quadratic time.\nIn this paper we provide evidence that the near-quadratic running time bounds\nknown for the problem of computing edit distance might be tight. Specifically,\nwe show that, if the edit distance can be computed in time $O(n^{2-\\delta})$\nfor some constant $\\delta>0$, then the satisfiability of conjunctive normal\nform formulas with $N$ variables and $M$ clauses can be solved in time\n$M^{O(1)} 2^{(1-\\epsilon)N}$ for a constant $\\epsilon>0$. The latter result\nwould violate the Strong Exponential Time Hypothesis, which postulates that\nsuch algorithms do not exist.'"
Piotr Indyk,Indyk_Piotr,arXiv:1406.1579,https://arxiv.org/abs/1406.1579,"b'Abstract:  Compressive Sensing (CS) stipulates that a sparse signal can be recovered\nfrom a small number of linear measurements, and that this recovery can be\nperformed efficiently in polynomial time. The framework of model-based\ncompressive sensing (model-CS) leverages additional structure in the signal and\nprescribes new recovery schemes that can reduce the number of measurements even\nfurther. However, model-CS requires an algorithm that solves the\nmodel-projection problem: given a query signal, produce the signal in the model\nthat is also closest to the query signal. Often, this optimization can be\ncomputationally very expensive. Moreover, an approximation algorithm is not\nsufficient for this optimization task. As a result, the model-projection\nproblem poses a fundamental obstacle for extending model-CS to many interesting\nmodels.\nIn this paper, we introduce a new framework that we call\napproximation-tolerant model-based compressive sensing. This framework includes\na range of algorithms for sparse recovery that require only approximate\nsolutions for the model-projection problem. In essence, our work removes the\naforementioned obstacle to model-based compressive sensing, thereby extending\nthe applicability of model-CS to a much wider class of models. We instantiate\nthis new framework for the Constrained Earth Mover Distance (CEMD) model, which\nis particularly useful for signal ensembles where the positions of the nonzero\ncoefficients do not change significantly as a function of spatial (or temporal)\nlocation. We develop novel approximation algorithms for both the maximization\nand the minimization versions of the model-projection problem via graph\noptimization techniques. Leveraging these algorithms into our framework results\nin a nearly sample-optimal sparse recovery scheme for the CEMD model.'"
Piotr Indyk,Indyk_Piotr,arXiv:1403.5804,https://arxiv.org/abs/1403.5804,"b'Abstract:  We give an algorithm for $\\ell_2/\\ell_2$ sparse recovery from Fourier\nmeasurements using $O(k\\log N)$ samples, matching the lower bound of\n\\cite{DIPW} for non-adaptive algorithms up to constant factors for any $k\\leq\nN^{1-\\delta}$. The algorithm runs in $\\tilde O(N)$ time. Our algorithm extends\nto higher dimensions, leading to sample complexity of $O_d(k\\log N)$, which is\noptimal up to constant factors for any $d=O(1)$. These are the first sample\noptimal algorithms for these problems.\nA preliminary experimental evaluation indicates that our algorithm has\nempirical sampling complexity comparable to that of other recovery methods\nknown in the literature, while providing strong provable guarantees on the\nrecovery quality.'"
Piotr Indyk,Indyk_Piotr,arXiv:1306.1547,https://arxiv.org/abs/1306.1547,"b""Abstract:  We present a new data structure for the c-approximate near neighbor problem\n(ANN) in the Euclidean space. For n points in R^d, our algorithm achieves\nO(n^{\\rho} + d log n) query time and O(n^{1 + \\rho} + d log n) space, where\n\\rho <= 7/(8c^2) + O(1 / c^3) + o(1). This is the first improvement over the\nresult by Andoni and Indyk (FOCS 2006) and the first data structure that\nbypasses a locality-sensitive hashing lower bound proved by O'Donnell, Wu and\nZhou (ICS 2011). By a standard reduction we obtain a data structure for the\nHamming space and \\ell_1 norm with \\rho <= 7/(8c) + O(1/c^{3/2}) + o(1), which\nis the first improvement over the result of Indyk and Motwani (STOC 1998)."""
Piotr Indyk,Indyk_Piotr,arXiv:1304.3604,https://arxiv.org/abs/1304.3604,"b'Abstract:  The Restricted Isometry Property (RIP) is a fundamental property of a matrix\nenabling sparse recovery. Informally, an m x n matrix satisfies RIP of order k\nin the l_p norm if ||Ax||_p \\approx ||x||_p for any vector x that is k-sparse,\ni.e., that has at most k non-zeros. The minimal number of rows m necessary for\nthe property to hold has been extensively investigated, and tight bounds are\nknown. Motivated by signal processing models, a recent work of Baraniuk et al\nhas generalized this notion to the case where the support of x must belong to a\ngiven model, i.e., a given family of supports. This more general notion is much\nless understood, especially for norms other than l_2. In this paper we present\ntight bounds for the model-based RIP property in the l_1 norm. Our bounds hold\nfor the two most frequently investigated models: tree-sparsity and\nblock-sparsity. We also show implications of our results to sparse recovery\nproblems.'"
Piotr Indyk,Indyk_Piotr,arXiv:1303.1209,https://arxiv.org/abs/1303.1209,"b'Abstract:  We present the first sample-optimal sublinear time algorithms for the sparse\nDiscrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our\nalgorithms are analyzed for /average case/ signals. For signals whose spectrum\nis exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,\nwhere k is the expected sparsity of the signal. For signals whose spectrum is\napproximately sparse, our algorithm uses O(k log n) samples and runs in O(k\nlog^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of\nsamples used by our algorithms matches the known lower bounds for the\nrespective signal models.\nBy a known reduction, our algorithms give similar results for the\none-dimensional sparse Discrete Fourier Transform when n is a power of a small\ncomposite number (e.g., n = 6^t).'"
Piotr Indyk,Indyk_Piotr,arXiv:1208.2447,https://arxiv.org/abs/1208.2447,"b""Abstract:  We propose a framework for compressive sensing of images with local\ndistinguishable objects, such as stars, and apply it to solve a problem in\ncelestial navigation. Specifically, let x be an N-pixel real-valued image,\nconsisting of a small number of local distinguishable objects plus noise. Our\ngoal is to design an m-by-N measurement matrix A with m << N, such that we can\nrecover an approximation to x from the measurements Ax.\nWe construct a matrix A and recovery algorithm with the following properties:\n(i) if there are k objects, the number of measurements m is O((k log N)/(log\nk)), undercutting the best known bound of O(k log(N/k)) (ii) the matrix A is\nvery sparse, which is important for hardware implementations of compressive\nsensing algorithms, and (iii) the recovery algorithm is empirically fast and\nruns in time polynomial in k and log(N).\nWe also present a comprehensive study of the application of our algorithm to\nattitude determination, or finding one's orientation in space. Spacecraft\ntypically use cameras to acquire an image of the sky, and then identify stars\nin the image to compute their orientation. Taking pictures is very expensive\nfor small spacecraft, since camera sensors use a lot of power. Our algorithm\noptically compresses the image before it reaches the camera's array of pixels,\nreducing the number of sensors that are required."""
Piotr Indyk,Indyk_Piotr,arXiv:1201.2501,https://arxiv.org/abs/1201.2501,"b'Abstract:  We consider the problem of computing the k-sparse approximation to the\ndiscrete Fourier transform of an n-dimensional signal. We show:\n* An O(k log n)-time randomized algorithm for the case where the input signal\nhas at most k non-zero Fourier coefficients, and\n* An O(k log n log(n/k))-time randomized algorithm for general input signals.\nBoth algorithms achieve o(n log n) time, and thus improve over the Fast\nFourier Transform, for any k = o(n). They are the first known algorithms that\nsatisfy this property. Also, if one assumes that the Fast Fourier Transform is\noptimal, the algorithm for the exactly k-sparse case is optimal for any k =\nn^{\\Omega(1)}.\nWe complement our algorithmic results by showing that any algorithm for\ncomputing the sparse Fourier transform of a general signal must use at least\n\\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to perform\nadaptive sampling.'"
Piotr Indyk,Indyk_Piotr,arXiv:1110.3850,https://arxiv.org/abs/1110.3850,"b""Abstract:  The goal of (stable) sparse recovery is to recover a $k$-sparse approximation\n$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is\nto recover $x*$ such that ||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q for some\nconstant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or\n$p=q=2$, this task can be accomplished using $m=O(k \\log (n/k))$ non-adaptive\nmeasurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].\nIn this paper we show that if one is allowed to perform measurements that are\nadaptive, then the number of measurements can be considerably reduced.\nSpecifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)k\nlog log (n eps/k))$ measurements that uses $O(log* k \\log \\log (n eps/k))$\nrounds. This is a significant improvement over the best possible non-adaptive\nbound. - A scheme with $m=O((1/eps) k log (k/eps) + k \\log (n/k))$ measurements\nthat uses /two/ rounds. This improves over the best possible non-adaptive\nbound. To the best of our knowledge, these are the first results of this type.\nAs an independent application, we show how to solve the problem of finding a\nduplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using\n$O(log n)$ bits of space and $O(log log n)$ passes, improving over the best\npossible space complexity achievable using a single pass."""
Piotr Indyk,Indyk_Piotr,arXiv:1106.0365,https://arxiv.org/abs/1106.0365,"b'Abstract:  We consider the following k-sparse recovery problem: design an m x n matrix\nA, such that for any signal x, given Ax we can efficiently recover x\'\nsatisfying\n||x-x\'||_1 <= C min_{k-sparse} x""} ||x-x""||_1.\nIt is known that there exist matrices A with this property that have only O(k\nlog (n/k)) rows.\nIn this paper we show that this bound is tight. Our bound holds even for the\nmore general /randomized/ version of the problem, where A is a random variable\nand the recovery algorithm is required to work for any fixed x with constant\nprobability (over A).'"
Piotr Indyk,Indyk_Piotr,arXiv:1104.4674,https://arxiv.org/abs/1104.4674,"b'Abstract:  We initiate the study of sparse recovery problems under the Earth-Mover\nDistance (EMD). Specifically, we design a distribution over m x n matrices A\nsuch that for any x, given Ax, we can recover a k-sparse approximation to x\nunder the EMD distance. One construction yields m = O(k log(n/k)) and a 1 +\nepsilon approximation factor, which matches the best achievable bound for other\nerror measures, such as the L_1 norm. Our algorithms are obtained by exploiting\nnovel connections to other problems and areas, such as streaming algorithms for\nk-median clustering and model-based compressive sensing. We also provide novel\nalgorithms and results for the latter problems.'"
Piotr Indyk,Indyk_Piotr,arXiv:1001.0041,https://arxiv.org/abs/1001.0041,"b'Abstract:  It has been known since 1970\'s that the N-dimensional $\\ell_1$-space contains\nnearly Euclidean subspaces whose dimension is $\\Omega(N)$. However, proofs of\nexistence of such subspaces were probabilistic, hence non-constructive, which\nmade the results not-quite-suitable for subsequently discovered applications to\nhigh-dimensional nearest neighbor search, error-correcting codes over the\nreals, compressive sensing and other computational problems. In this paper we\npresent a ""low-tech"" scheme which, for any $a > 0$, allows to exhibit nearly\nEuclidean $\\Omega(N)$-dimensional subspaces of $\\ell_1^N$ while using only\n$N^a$ random bits. Our results extend and complement (particularly) recent work\nby Guruswami-Lee-Wigderson. Characteristic features of our approach include (1)\nsimplicity (we use only tensor products) and (2) yielding ""almost Euclidean""\nsubspaces with arbitrarily small distortions.'"
Piotr Indyk,Indyk_Piotr,arXiv:cs/0303001,https://arxiv.org/abs/cs/0303001,"b'Abstract:  In the first part of the paper, we present an (1+\\mu)-approximation algorithm\nto the minimum-spanning tree of points in a planar arrangement of lines, where\nthe metric is the number of crossings between the spanning tree and the lines.\nThe expected running time is O((n/\\mu^5) alpha^3(n) log^5 n), where \\mu > 0 is\na prescribed constant.\nIn the second part of our paper, we show how to embed such a crossing metric,\ninto high-dimensions, so that the distances are preserved. As a result, we can\ndeploy a large collection of subquadratic approximations algorithms \\cite\nim-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric\nas a distance function. Applications include matching, clustering,\nnearest-neighbor, and furthest-neighbor.'"
Piotr Indyk,Indyk_Piotr,arXiv:cs/0009013,https://arxiv.org/abs/cs/0009013,"b'Abstract:  In this paper we present algorithms for a number of problems in geometric\npattern matching where the input consist of a collections of segments in the\nplane. Our work consists of two main parts. In the first, we address problems\nand measures that relate to collections of orthogonal line segments in the\nplane. Such collections arise naturally from problems in mapping buildings and\nrobot exploration.\nWe propose a new measure of segment similarity called a \\emph{coverage\nmeasure}, and present efficient algorithms for maximising this measure between\nsets of axis-parallel segments under translations. Our algorithms run in time\n$O(n^3\\polylog n)$ in the general case, and run in time $O(n^2\\polylog n)$ for\nthe case when all segments are horizontal. In addition, we show that when\nrestricted to translations that are only vertical, the Hausdorff distance\nbetween two sets of horizontal segments can be computed in time roughly\n$O(n^{3/2}{\\sl polylog}n)$. These algorithms form significant improvements over\nthe general algorithm of Chew et al. that takes time $O(n^4 \\log^2 n)$. In the\nsecond part of this paper we address the problem of matching polygonal chains.\nWe study the well known \\Frd, and present the first algorithm for computing the\n\\Frd under general translations. Our methods also yield algorithms for\ncomputing a generalization of the \\Fr distance, and we also present a simple\napproximation algorithm for the \\Frd that runs in time $O(n^2\\polylog n)$.'"
Erich Ippen,Ippen_Erich,arXiv:1410.8120,https://arxiv.org/abs/1410.8120,"b'Abstract:  A model for THz generation by optical rectification using tilted-pulse-fronts\nis developed. It simultaneously accounts for (i) the spatio-temporal\ndistortions of the optical pump pulse, (ii) the nonlinear coupled interaction\nof THz and optical radiation in two spatial dimensions (2-D), (iii) self-phase\nmodulation and (iv) stimulated Raman scattering. The model is validated by\nquantitative agreement with experiments and analytic calculations. We show that\nthe optical pump beam is significantly broadened in the transverse-momentum\n(kx) domain as a consequence of the spectral broadening caused by THz\ngeneration. In the presence of this large frequency and transverse-momentum (or\nangular) spread, group velocity dispersion causes a spatio-temporal break-up of\nthe optical pump pulse which inhibits further THz generation. The implications\nof these effects on energy scaling and optimization of optical-to-THz\nconversion efficiency are discussed. This suggests the use of optical pump\npulses with elliptical beam profiles for large optical pump energies. It is\nseen that optimization of the setup is highly dependent on optical pump\nconditions. Trade-offs of optimizing the optical-to-THz conversion efficiency\non the spatial and spectral properties of THz radiation is discussed to guide\nthe development of such sources.'"
Phillip Isola,Isola_Phillip,arXiv:1903.00784,https://arxiv.org/abs/1903.00784,"b'Abstract:  The emergence of complex life on Earth is often attributed to the arms race\nthat ensued from a huge number of organisms all competing for finite resources.\nWe present an artificial intelligence research environment, inspired by the\nhuman game genre of MMORPGs (Massively Multiplayer Online Role-Playing Games,\na.k.a. MMOs), that aims to simulate this setting in microcosm. As with MMORPGs\nand the real world alike, our environment is persistent and supports a large\nand variable number of agents. Our environment is well suited to the study of\nlarge-scale multiagent interaction: it requires that agents learn robust combat\nand navigation policies in the presence of large populations attempting to do\nthe same. Baseline experiments reveal that population size magnifies and\nincentivizes the development of skillful behaviors and results in agents that\noutcompete agents trained in smaller populations. We further show that the\npolicies of agents with unshared weights naturally diverge to fill different\nniches in order to avoid competition.'"
Phillip Isola,Isola_Phillip,arXiv:1902.05546,https://arxiv.org/abs/1902.05546,"b""Abstract:  Contemporary sensorimotor learning approaches typically start with an\nexisting complex agent (e.g., a robotic arm), which they learn to control. In\ncontrast, this paper investigates a modular co-evolution strategy: a collection\nof primitive agents learns to dynamically self-assemble into composite bodies\nwhile also learning to coordinate their behavior to control these bodies. Each\nprimitive agent consists of a limb with a motor attached at one end. Limbs may\nchoose to link up to form collectives. When a limb initiates a link-up action\nand there is another limb nearby, the latter is magnetically connected to the\n'parent' limb's motor. This forms a new single agent, which may further link\nwith other agents. In this way, complex morphologies can emerge, controlled by\na policy whose architecture is in explicit correspondence with the morphology.\nWe evaluate the performance of these 'dynamic' and 'modular' agents in\nsimulated environments. We demonstrate better generalization to test-time\nchanges both in the environment, as well as in the agent morphology, compared\nto static and monolithic baselines. Project videos and code are available at\nthis https URL"""
Phillip Isola,Isola_Phillip,arXiv:1812.00231,https://arxiv.org/abs/1812.00231,"b'Abstract:  Good Visual Retargeting changes the global size and aspect ratio of a natural\nimage, while preserving the size and aspect ratio of all its local elements. We\npropose formulating this principle by requiring that the distribution of\npatches in the input matches the distribution of patches in the output. We\nintroduce a Deep-Learning approach for retargeting, based on an ""Internal GAN""\n(InGAN). InGAN is an image-specific GAN. It incorporates the Internal\nstatistics of a single natural image in a GAN. It is trained on a single input\nimage and learns the distribution of its patches. It is then able to synthesize\nnatural looking target images composed from the input image patch-distribution.\nInGAN is totally unsupervised, and requires no additional data other than the\ninput image itself. Moreover, once trained on the input image, it can generate\ntarget images of any specified size or aspect ratio in real-time.'"
Phillip Isola,Isola_Phillip,arXiv:1802.04821,https://arxiv.org/abs/1802.04821,"b""Abstract:  We propose a metalearning approach for learning gradient-based reinforcement\nlearning (RL) algorithms. The idea is to evolve a differentiable loss function,\nsuch that an agent, which optimizes its policy to minimize this loss, will\nachieve high rewards. The loss is parametrized via temporal convolutions over\nthe agent's experience. Because this loss is highly flexible in its ability to\ntake into account the agent's history, it enables fast task learning. Empirical\nresults show that our evolved policy gradient algorithm (EPG) achieves faster\nlearning on several randomized environments compared to an off-the-shelf policy\ngradient method. We also demonstrate that EPG's learned loss can generalize to\nout-of-distribution test time tasks, and exhibits qualitatively different\nbehavior from other popular metalearning algorithms."""
Phillip Isola,Isola_Phillip,arXiv:1801.03924,https://arxiv.org/abs/1801.03924,"b'Abstract:  While it is nearly effortless for humans to quickly assess the perceptual\nsimilarity between two images, the underlying processes are thought to be quite\ncomplex. Despite this, the most widely used perceptual metrics today, such as\nPSNR and SSIM, are simple, shallow functions, and fail to account for many\nnuances of human perception. Recently, the deep learning community has found\nthat features of the VGG network trained on ImageNet classification has been\nremarkably useful as a training loss for image synthesis. But how perceptual\nare these so-called ""perceptual losses""? What elements are critical for their\nsuccess? To answer these questions, we introduce a new dataset of human\nperceptual similarity judgments. We systematically evaluate deep features\nacross different architectures and tasks and compare them with classic metrics.\nWe find that deep features outperform all previous metrics by large margins on\nour dataset. More surprisingly, this result is not restricted to\nImageNet-trained VGG features, but holds across different deep architectures\nand levels of supervision (supervised, self-supervised, or even unsupervised).\nOur results suggest that perceptual similarity is an emergent property shared\nacross deep visual representations.'"
Phillip Isola,Isola_Phillip,arXiv:1711.03213,https://arxiv.org/abs/1711.03213,"b'Abstract:  Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain\ninvariant representations, but are difficult to visualize and sometimes fail to\ncapture pixel-level and low-level domain shifts. Recent work has shown that\ngenerative adversarial networks combined with cycle-consistency constraints are\nsurprisingly effective at mapping images between domains, even without the use\nof aligned image pairs. We propose a novel discriminatively-trained\nCycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts\nrepresentations at both the pixel-level and feature-level, enforces\ncycle-consistency while leveraging a task loss, and does not require aligned\npairs. Our model can be applied in a variety of visual recognition and\nprediction settings. We show new state-of-the-art results across multiple\nadaptation tasks, including digit classification and semantic segmentation of\nroad scenes demonstrating transfer from synthetic to real world domains.'"
Phillip Isola,Isola_Phillip,arXiv:1707.08390,https://arxiv.org/abs/1707.08390,"b'Abstract:  Sketch-based modeling strives to bring the ease and immediacy of drawing to\nthe 3D world. However, while drawings are easy for humans to create, they are\nvery challenging for computers to interpret due to their sparsity and\nambiguity. We propose a data-driven approach that tackles this challenge by\nlearning to reconstruct 3D shapes from one or more drawings. At the core of our\napproach is a deep convolutional neural network (CNN) that predicts occupancy\nof a voxel grid from a line drawing. This CNN provides us with an initial 3D\nreconstruction as soon as the user completes a single drawing of the desired\nshape. We complement this single-view network with an updater CNN that refines\nan existing prediction given a new drawing of the shape created from a novel\nviewpoint. A key advantage of our approach is that we can apply the updater\niteratively to fuse information from an arbitrary number of viewpoints, without\nrequiring explicit stroke correspondences between the drawings. We train both\nCNNs by rendering synthetic contour drawings from hand-modeled shape\ncollections as well as from procedurally-generated abstract shapes. Finally, we\nintegrate our CNNs in a minimal modeling interface that allows users to\nseamlessly draw an object, rotate it to see its 3D reconstruction, and refine\nit by re-drawing from another vantage point using the 3D reconstruction as\nguidance. The main strengths of our approach are its robustness to freehand\nbitmap drawings, its ability to adapt to different object categories, and the\ncontinuum it offers between single-view and multi-view sketch-based modeling.'"
Phillip Isola,Isola_Phillip,arXiv:1705.02999,https://arxiv.org/abs/1705.02999,"b'Abstract:  We propose a deep learning approach for user-guided image colorization. The\nsystem directly maps a grayscale image, along with sparse, local user ""hints""\nto an output colorization with a Convolutional Neural Network (CNN). Rather\nthan using hand-defined rules, the network propagates user edits by fusing\nlow-level cues along with high-level semantic information, learned from\nlarge-scale data. We train on a million images, with simulated user inputs. To\nguide the user towards efficient input selection, the system recommends likely\ncolors based on the input image and current user inputs. The colorization is\nperformed in a single feed-forward pass, enabling real-time use. Even with\nrandomly simulated user inputs, we show that the proposed system helps novice\nusers quickly create realistic colorizations, and offers large improvements in\ncolorization quality with just a minute of use. In addition, we demonstrate\nthat the framework can incorporate other user ""hints"" to the desired\ncolorization, showing an application to color histogram transfer. Our code and\nmodels are available at this https URL.'"
Phillip Isola,Isola_Phillip,arXiv:1703.10593,https://arxiv.org/abs/1703.10593,"b'Abstract:  Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.'"
Phillip Isola,Isola_Phillip,arXiv:1703.02018,https://arxiv.org/abs/1703.02018,"b'Abstract:  Manipulation of deformable objects, such as ropes and cloth, is an important\nbut challenging problem in robotics. We present a learning-based system where a\nrobot takes as input a sequence of images of a human manipulating a rope from\nan initial to goal configuration, and outputs a sequence of actions that can\nreproduce the human demonstration, using only monocular images as input. To\nperform this task, the robot learns a pixel-level inverse dynamics model of\nrope manipulation directly from images in a self-supervised manner, using about\n60K interactions with the rope collected autonomously by the robot. The human\ndemonstration provides a high-level plan of what to do and the low-level\ninverse model is used to execute the plan. We show that by combining the high\nand low-level plans, the robot can successfully manipulate a rope into a\nvariety of target shapes using only a sequence of human-provided images for\ndirection.'"
Phillip Isola,Isola_Phillip,arXiv:1611.09842,https://arxiv.org/abs/1611.09842,"b'Abstract:  We propose split-brain autoencoders, a straightforward modification of the\ntraditional autoencoder architecture, for unsupervised representation learning.\nThe method adds a split to the network, resulting in two disjoint sub-networks.\nEach sub-network is trained to perform a difficult task -- predicting one\nsubset of the data channels from another. Together, the sub-networks extract\nfeatures from the entire input signal. By forcing the network to solve\ncross-channel prediction tasks, we induce a representation within the network\nwhich transfers well to other, unseen tasks. This method achieves\nstate-of-the-art performance on several large-scale transfer learning\nbenchmarks.'"
Phillip Isola,Isola_Phillip,arXiv:1611.07004,https://arxiv.org/abs/1611.07004,"b'Abstract:  We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either.'"
Phillip Isola,Isola_Phillip,arXiv:1603.08511,https://arxiv.org/abs/1603.08511,"b'Abstract:  Given a grayscale photograph as input, this paper attacks the problem of\nhallucinating a plausible color version of the photograph. This problem is\nclearly underconstrained, so previous approaches have either relied on\nsignificant user interaction or resulted in desaturated colorizations. We\npropose a fully automatic approach that produces vibrant and realistic\ncolorizations. We embrace the underlying uncertainty of the problem by posing\nit as a classification task and use class-rebalancing at training time to\nincrease the diversity of colors in the result. The system is implemented as a\nfeed-forward pass in a CNN at test time and is trained on over a million color\nimages. We evaluate our algorithm using a ""colorization Turing test,"" asking\nhuman participants to choose between a generated and ground truth color image.\nOur method successfully fools humans on 32% of the trials, significantly higher\nthan previous methods. Moreover, we show that colorization can be a powerful\npretext task for self-supervised feature learning, acting as a cross-channel\nencoder. This approach results in state-of-the-art performance on several\nfeature learning benchmarks.'"
Phillip Isola,Isola_Phillip,arXiv:1512.08512,https://arxiv.org/abs/1512.08512,"b'Abstract:  Objects make distinctive sounds when they are hit or scratched. These sounds\nreveal aspects of an object\'s material properties, as well as the actions that\nproduced them. In this paper, we propose the task of predicting what sound an\nobject makes when struck as a way of studying physical interactions within a\nvisual scene. We present an algorithm that synthesizes sound from silent videos\nof people hitting and scratching objects with a drumstick. This algorithm uses\na recurrent neural network to predict sound features from videos and then\nproduces a waveform from these features with an example-based synthesis\nprocedure. We show that the sounds predicted by our model are realistic enough\nto fool participants in a ""real or fake"" psychophysical experiment, and that\nthey convey significant information about material properties and physical\ninteractions.'"
Phillip Isola,Isola_Phillip,arXiv:1511.06811,https://arxiv.org/abs/1511.06811,"b'Abstract:  We propose a self-supervised framework that learns to group visual entities\nbased on their rate of co-occurrence in space and time. To model statistical\ndependencies between the entities, we set up a simple binary classification\nproblem in which the goal is to predict if two visual primitives occur in the\nsame spatial or temporal context. We apply this framework to three domains:\nlearning patch affinities from spatial adjacency in images, learning frame\naffinities from temporal adjacency in videos, and learning photo affinities\nfrom geospatial proximity in image collections. We demonstrate that in each\ncase the learned affinities uncover meaningful semantic groupings. From patch\naffinities we generate object proposals that are competitive with\nstate-of-the-art supervised methods. From frame affinities we generate movie\nscene segmentations that correlate well with DVD chapter structure. Finally,\nfrom geospatial affinities we learn groups that relate well to semantic place\ncategories.'"
Phillip Isola,Isola_Phillip,arXiv:1412.7884,https://arxiv.org/abs/1412.7884,"b'Abstract:  In this paper, we study the problem of reproducing the world lighting from a\nsingle image of an object covered with random specular microfacets on the\nsurface. We show that such reflectors can be interpreted as a randomized\nmapping from the lighting to the image. Such specular objects have very\ndifferent optical properties from both diffuse surfaces and smooth specular\nobjects like metals, so we design special imaging system to robustly and\neffectively photograph them. We present simple yet reliable algorithms to\ncalibrate the proposed system and do the inference. We conduct experiments to\nverify the correctness of our model assumptions and prove the effectiveness of\nour pipeline.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1902.09737,https://arxiv.org/abs/1902.09737,"b'Abstract:  We provide a new approach to training neural models to exhibit transparency\nin a well-defined, functional manner. Our approach naturally operates over\nstructured data and tailors the predictor, functionally, towards a chosen\nfamily of (local) witnesses. The estimation problem is setup as a co-operative\ngame between an unrestricted predictor such as a neural network, and a set of\nwitnesses chosen from the desired transparent family. The goal of the witnesses\nis to highlight, locally, how well the predictor conforms to the chosen family\nof functions, while the predictor is trained to minimize the highlighted\ndiscrepancy. We emphasize that the predictor remains globally powerful as it is\nonly encouraged to agree locally with locally adapted witnesses. We analyze the\neffect of the proposed approach, provide example formulations in the context of\ndeep graph and sequence models, and empirically illustrate the idea in chemical\nproperty prediction, temporal modeling, and molecule representation learning.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1902.02037,https://arxiv.org/abs/1902.02037,"b'Abstract:  We consider the problem of inferring the values of an arbitrary set of\nvariables (e.g., risk of diseases) given other observed variables (e.g.,\nsymptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images\nor EEG). This is a common problem in healthcare since variables of interest\noften differ for different patients. Existing methods including Bayesian\nnetworks and structured prediction either do not incorporate high-dimensional\nsignals or fail to model conditional dependencies among variables. To address\nthese issues, we propose bidirectional inference networks (BIN), which stich\ntogether multiple probabilistic neural networks, each modeling a conditional\ndependency. Predictions are then made via iteratively updating variables using\nbackpropagation (BP) to maximize corresponding posterior probability.\nFurthermore, we extend BIN to composite BIN (CBIN), which involves the\niterative prediction process in the training stage and improves both accuracy\nand computational efficiency by adaptively smoothing the optimization\nlandscape. Experiments on synthetic and real-world datasets (a sleep study and\na dermatology dataset) show that CBIN is a single model that can achieve\nstate-of-the-art performance and obtain better accuracy in most inference tasks\nthan multiple models each specifically trained for a different task.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1812.01070,https://arxiv.org/abs/1812.01070,"b'Abstract:  We view molecular optimization as a graph-to-graph translation problem. The\ngoal is to learn to map from one molecular graph to another with better\nproperties based on an available corpus of paired molecules. Since molecules\ncan be optimized in different ways, there are multiple viable translations for\neach input graph. A key challenge is therefore to model diverse translation\noutputs. Our primary contributions include a junction tree encoder-decoder for\nlearning diverse graph translations along with a novel adversarial training\nmethod for aligning distributions of molecules. Diverse output distributions in\nour model are explicitly realized by low-dimensional latent vectors that\nmodulate the translation process. We evaluate our model on multiple molecular\noptimization tasks and show that our model outperforms previous\nstate-of-the-art baselines.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1809.00013,https://arxiv.org/abs/1809.00013,"b'Abstract:  Cross-lingual or cross-domain correspondences play key roles in tasks ranging\nfrom machine translation to transfer learning. Recently, purely unsupervised\nmethods operating on monolingual embeddings have become effective alignment\ntools. Current state-of-the-art methods, however, involve multiple steps,\nincluding heuristic post-hoc refinement strategies. In this paper, we cast the\ncorrespondence problem directly as an optimal transport (OT) problem, building\non the idea that word embeddings arise from metric recovery algorithms. Indeed,\nwe exploit the Gromov-Wasserstein distance that measures how similarities\nbetween pairs of words relate across languages. We show that our OT objective\ncan be estimated efficiently, requires little or no tuning, and results in\nperformance comparable with the state-of-the-art in various unsupervised word\ntranslation tasks.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1807.08919,https://arxiv.org/abs/1807.08919,"b'Abstract:  Hierarchical Bayesian methods can unify many related tasks (e.g. k-shot\nclassification, conditional and unconditional generation) as inference within a\nsingle generative model. However, when this generative model is expressed as a\npowerful neural network such as a PixelCNN, we show that existing learning\ntechniques typically fail to effectively use latent variables. To address this,\nwe develop a modification of the Variational Autoencoder in which encoded\nobservations are decoded to new elements from the same class. This technique,\nwhich we call a Variational Homoencoder (VHE), produces a hierarchical latent\nvariable model which better utilises latent variables. We use the VHE framework\nto learn a hierarchical PixelCNN on the Omniglot dataset, which outperforms all\nexisting models on test set likelihood and achieves strong performance on\none-shot generation and classification tasks. We additionally validate the VHE\non natural images from the YouTube Faces database. Finally, we develop\nextensions of the model that apply to richer dataset structures such as\nfactorial and hierarchical categories.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1807.00130,https://arxiv.org/abs/1807.00130,"b'Abstract:  Interpretability has arisen as a key desideratum of machine learning models\nalongside performance. Approaches so far have been primarily concerned with\nfixed dimensional inputs emphasizing feature relevance or selection. In\ncontrast, we focus on temporal modeling and the problem of tailoring the\npredictor, functionally, towards an interpretable family. To this end, we\npropose a co-operative game between the predictor and an explainer without any\na priori restrictions on the functional class of the predictor. The goal of the\nexplainer is to highlight, locally, how well the predictor conforms to the\nchosen interpretable family of temporal models. Our co-operative game is setup\nasymmetrically in terms of information sets for efficiency reasons. We develop\nand illustrate the framework in the context of temporal sequence models with\nexamples.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1806.09277,https://arxiv.org/abs/1806.09277,"b'Abstract:  Many problems in machine learning involve calculating correspondences between\nsets of objects, such as point clouds or images. Discrete optimal transport\nprovides a natural and successful approach to such tasks whenever the two sets\nof objects can be represented in the same space, or at least distances between\nthem can be directly evaluated. Unfortunately neither requirement is likely to\nhold when object representations are learned from data. Indeed, automatically\nderived representations such as word embeddings are typically fixed only up to\nsome global transformations, for example, reflection or rotation. As a result,\npairwise distances across two such instances are ill-defined without specifying\ntheir relative transformation. In this work, we propose a general framework for\noptimal transport in the presence of latent global transformations. We cast the\nproblem as a joint optimization over transport couplings and transformations\nchosen from a flexible class of invariances, propose algorithms to solve it,\nand show promising results in various tasks, including a popular unsupervised\nword translation benchmark.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1806.08049,https://arxiv.org/abs/1806.08049,"b'Abstract:  We argue that robustness of explanations---i.e., that similar inputs should\ngive rise to similar explanations---is a key desideratum for interpretability.\nWe introduce metrics to quantify robustness and demonstrate that current\nmethods do not perform well according to these metrics. Finally, we propose\nways that robustness can be enforced on existing interpretability approaches.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1806.07538,https://arxiv.org/abs/1806.07538,"b'Abstract:  Most recent work on interpretability of complex machine learning models has\nfocused on estimating $\\textit{a posteriori}$ explanations for previously\ntrained models around specific predictions. $\\textit{Self-explaining}$ models\nwhere interpretability plays a key role already during learning have received\nmuch less attention. We propose three desiderata for explanations in general --\nexplicitness, faithfulness, and stability -- and show that existing methods do\nnot satisfy them. In response, we design self-explaining models in stages,\nprogressively generalizing linear classifiers to complex yet architecturally\nexplicit models. Faithfulness and stability are enforced via regularization\nspecifically tailored to such models. Experimental results across various\nbenchmark datasets show that our framework offers a promising direction for\nreconciling model complexity and interpretability.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1806.02867,https://arxiv.org/abs/1806.02867,b'Abstract:  Reparameterization of variational auto-encoders with continuous random\nvariables is an effective method for reducing the variance of their gradient\nestimates. In this work we reparameterize discrete variational auto-encoders\nusing the Gumbel-Max perturbation model that represents the Gibbs distribution\nusing the $\\arg \\max$ of randomly perturbed encoder. We subsequently apply the\ndirect loss minimization technique to propagate gradients through the\nreparameterized $\\arg \\max$. The resulting gradient is estimated by the\ndifference of the encoder gradients that are evaluated in two $\\arg \\max$\npredictions.'
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1802.04364,https://arxiv.org/abs/1802.04364,"b'Abstract:  We seek to automate the design of molecules based on specific chemical\nproperties. In computational terms, this task involves continuous embedding and\ngeneration of molecular graphs. Our primary contribution is the direct\nrealization of molecular graphs, a task previously approached by generating\nlinear SMILES strings instead of graphs. Our junction tree variational\nautoencoder generates molecular graphs in two phases, by first generating a\ntree-structured scaffold over chemical substructures, and then combining them\ninto a molecule with a graph message passing network. This approach allows us\nto incrementally expand molecules while maintaining chemical validity at every\nstep. We evaluate our model on multiple tasks ranging from molecular generation\nto optimization. Across these tasks, our model outperforms previous\nstate-of-the-art baselines by a significant margin.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1712.06199,https://arxiv.org/abs/1712.06199,"b'Abstract:  Optimal Transport has recently gained interest in machine learning for\napplications ranging from domain adaptation, sentence similarities to deep\nlearning. Yet, its ability to capture frequently occurring structure beyond the\n""ground metric"" is limited. In this work, we develop a nonlinear generalization\nof (discrete) optimal transport that is able to reflect much additional\nstructure. We demonstrate how to leverage the geometry of this new model for\nfast algorithms, and explore connections and properties. Illustrative\nexperiments highlight the benefit of the induced structured couplings for tasks\nin domain adaptation and natural language processing.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1709.04555,https://arxiv.org/abs/1709.04555,"b'Abstract:  The prediction of organic reaction outcomes is a fundamental problem in\ncomputational chemistry. Since a reaction may involve hundreds of atoms, fully\nexploring the space of possible transformations is intractable. The current\nsolution utilizes reaction templates to limit the space, but it suffers from\ncoverage and efficiency issues. In this paper, we propose a template-free\napproach to efficiently explore the space of product molecules by first\npinpointing the reaction center -- the set of nodes and edges where graph edits\noccur. Since only a small number of atoms contribute to reaction center, we can\ndirectly enumerate candidate products. The generated candidates are scored by a\nWeisfeiler-Lehman Difference Network that models high-order interactions\nbetween changes occurring at nodes across the molecule. Our framework\noutperforms the top-performing template-based approach with a 10\\% margin,\nwhile running orders of magnitude faster. Finally, we demonstrate that the\nmodel accuracy rivals the performance of domain experts.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1708.00133,https://arxiv.org/abs/1708.00133,"b'Abstract:  In this paper, we explore the utilization of natural language to drive\ntransfer for reinforcement learning (RL). Despite the wide-spread application\nof deep RL techniques, learning generalized policy representations that work\nacross domains remains a challenging problem. We demonstrate that textual\ndescriptions of environments provide a compact intermediate channel to\nfacilitate effective policy transfer. Specifically, by learning to ground the\nmeaning of text to the dynamics of the environment such as transitions and\nrewards, an autonomous agent can effectively bootstrap policy learning on a new\ndomain given its description. We employ a model-based RL approach consisting of\na differentiable planning module, a model-free component and a factorized state\nrepresentation to effectively use entity descriptions. Our model outperforms\nprior work on both transfer and multi-task scenarios in a variety of different\nenvironments. For instance, we achieve up to 14% and 11.5% absolute improvement\nover previously existing models in terms of average and initial rewards,\nrespectively.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1707.01943,https://arxiv.org/abs/1707.01943,"b'Abstract:  We interpret the predictions of any black-box structured input-structured\noutput model around a specific input-output pair. Our method returns an\n""explanation"" consisting of groups of input-output tokens that are causally\nrelated. These dependencies are inferred by querying the black-box model with\nperturbed inputs, generating a graph over tokens from the responses, and\nsolving a partitioning problem to select the most relevant components. We focus\nthe general approach on sequence-to-sequence problems, adopting a variational\nautoencoder to yield meaningful input perturbations. We test our method across\nseveral NLP sequence generation tasks.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1705.09655,https://arxiv.org/abs/1705.09655,"b'Abstract:  This paper focuses on style transfer on the basis of non-parallel text. This\nis an instance of a broad family of problems including machine translation,\ndecipherment, and sentiment modification. The key challenge is to separate the\ncontent from other aspects such as style. We assume a shared latent content\ndistribution across different text corpora, and propose a method that leverages\nrefined alignment of latent representations to perform style transfer. The\ntransferred sentences from one style should match example sentences from the\nother style as a population. We demonstrate the effectiveness of this\ncross-alignment method on three tasks: sentiment modification, decipherment of\nword substitution ciphers, and recovery of word order.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1705.09037,https://arxiv.org/abs/1705.09037,"b'Abstract:  The design of neural architectures for structured objects is typically guided\nby experimental insights rather than a formal process. In this work, we appeal\nto kernels over combinatorial structures, such as sequences and graphs, to\nderive appropriate neural operations. We introduce a class of deep recurrent\nneural operations and formally characterize their associated kernel spaces. Our\nrecurrent modules compare the input to virtual reference objects (cf. filters\nin CNN) via the kernels. Similar to traditional neural operations, these\nreference objects are parameterized and directly optimized in end-to-end\ntraining. We empirically evaluate the proposed class of neural architectures on\nstandard applications such as language modeling and molecular graph regression,\nachieving state-of-the-art results across these applications.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1701.00188,https://arxiv.org/abs/1701.00188,"b'Abstract:  We introduce a neural method for transfer learning between two (source and\ntarget) classification tasks or aspects over the same domain. Rather than\ntraining on target labels, we use a few keywords pertaining to source and\ntarget aspects indicating sentence relevance instead of document class labels.\nDocuments are encoded by learning to embed and softly select relevant sentences\nin an aspect-dependent manner. A shared classifier is trained on the source\nencoded documents and labels, and applied to target encoded documents. We\nensure transfer through aspect-adversarial training so that encoded documents\nare, as sets, aspect-invariant. Experimental results demonstrate that our\napproach outperforms different baselines and model variants on two datasets,\nyielding an improvement of 27% on a pathology dataset and 5% on a review\ndataset.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1606.05027,https://arxiv.org/abs/1606.05027,"b'Abstract:  Our goal is to identify beneficial interventions from observational data. We\nconsider interventions that are narrowly focused (impacting few covariates) and\nmay be tailored to each individual or globally enacted over a population. For\napplications where harmful intervention is drastically worse than proposing no\nchange, we propose a conservative definition of the optimal intervention.\nAssuming the underlying relationship remains invariant under intervention, we\ndevelop efficient algorithms to identify the optimal intervention policy from\nlimited data and provide theoretical guarantees for our approach in a Gaussian\nProcess setting. Although our methods assume covariates can be precisely\nadjusted, they remain capable of improving outcomes in misspecified settings\nwhere interventions incur unintentional downstream effects. Empirically, our\napproach identifies good interventions in two practical applications: gene\nperturbation and writing improvement.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1606.04155,https://arxiv.org/abs/1606.04155,"b'Abstract:  Prediction without justification has limited applicability. As a remedy, we\nlearn to extract pieces of input text as justifications -- rationales -- that\nare tailored to be short and coherent, yet sufficient for making the same\nprediction. Our approach combines two modular components, generator and\nencoder, which are trained to operate well together. The generator specifies a\ndistribution over text fragments as candidate rationales and these are passed\nthrough the encoder for prediction. Rationales are never given during training.\nInstead, the model is regularized by desiderata for rationales. We evaluate the\napproach on multi-aspect sentiment analysis against manually annotated test\ncases. Our approach outperforms attention-based baseline by a significant\nmargin. We also successfully illustrate the method on the question retrieval\ntask.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1602.03571,https://arxiv.org/abs/1602.03571,"b'Abstract:  This paper presents a new approach, called perturb-max, for high-dimensional\nstatistical inference that is based on applying random perturbations followed\nby optimization. This framework injects randomness to maximum a-posteriori\n(MAP) predictors by randomly perturbing the potential function for the input. A\nclassic result from extreme value statistics asserts that perturb-max\noperations generate unbiased samples from the Gibbs distribution using\nhigh-dimensional perturbations. Unfortunately, the computational cost of\ngenerating so many high-dimensional random variables can be prohibitive.\nHowever, when the perturbations are of low dimension, sampling the perturb-max\nprediction is as efficient as MAP optimization. This paper shows that the\nexpected value of perturb-max inference with low dimensional perturbations can\nbe used sequentially to generate unbiased samples from the Gibbs distribution.\nFurthermore the expected value of the maximal perturbations is a natural bound\non the entropy of such perturb-max models. A measure concentration result for\nperturb-max values shows that the deviation of their sampled average from its\nexpectation decays exponentially in the number of samples, allowing effective\napproximation of the expectation.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1512.05726,https://arxiv.org/abs/1512.05726,"b'Abstract:  Question answering forums are rapidly growing in size with no effective\nautomated ability to refer to and reuse answers already available for previous\nposted questions. In this paper, we develop a methodology for finding\nsemantically related questions. The task is difficult since 1) key pieces of\ninformation are often buried in extraneous details in the question body and 2)\navailable annotations on similar questions are scarce and fragmented. We design\na recurrent and convolutional model (gated convolution) to effectively map\nquestions to their semantic representations. The models are pre-trained within\nan encoder-decoder framework (from body to title) on the basis of the entire\nraw corpus, and fine-tuned discriminatively from limited annotations. Our\nevaluation demonstrates that our model yields substantial gains over a standard\nIR baseline and various neural network architectures (including CNNs, LSTMs and\nGRUs).'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1511.04486,https://arxiv.org/abs/1511.04486,"b'Abstract:  We present a nonparametric framework to model a short sequence of probability\ndistributions that vary both due to underlying effects of sequential\nprogression and confounding noise. To distinguish between these two types of\nvariation and estimate the sequential-progression effects, our approach\nleverages an assumption that these effects follow a persistent trend. This work\nis motivated by the recent rise of single-cell RNA-sequencing experiments over\na brief time course, which aim to identify genes relevant to the progression of\na particular biological process across diverse cell populations. While\nclassical statistical tools focus on scalar-response regression or\norder-agnostic differences between distributions, it is desirable in this\nsetting to consider both the full distributions as well as the structure\nimposed by their ordering. We introduce a new regression model for ordinal\ncovariates where responses are univariate distributions and the underlying\nrelationship reflects consistent changes in the distributions over increasing\nlevels of the covariate. This concept is formalized as a ""trend"" in\ndistributions, which we define as an evolution that is linear under the\nWasserstein metric. Implemented via a fast alternating projections algorithm,\nour method exhibits numerous strengths in simulations and analyses of\nsingle-cell gene expression data.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1511.00573,https://arxiv.org/abs/1511.00573,"b'Abstract:  Large unweighted directed graphs are commonly used to capture relations\nbetween entities. A fundamental problem in the analysis of such networks is to\nproperly define the similarity or dissimilarity between any two vertices.\nDespite the significance of this problem, statistical characterization of the\nproposed metrics has been limited. We introduce and develop a class of\ntechniques for analyzing random walks on graphs using stochastic calculus.\nUsing these techniques we generalize results on the degeneracy of hitting times\nand analyze a metric based on the Laplace transformed hitting time (LTHT). The\nmetric serves as a natural, provably well-behaved alternative to the expected\nhitting time. We establish a general correspondence between hitting times of\nthe Brownian motion and analogous hitting times on the graph. We show that the\nLTHT is consistent with respect to the underlying metric of a geometric graph,\npreserves clustering tendency, and remains robust against random addition of\nnon-geometric edges. Tests on simulated and real-world data show that the LTHT\nmatches theoretical predictions and outperforms alternatives.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1510.08956,https://arxiv.org/abs/1510.08956,"b'Abstract:  We introduce principal differences analysis (PDA) for analyzing differences\nbetween high-dimensional distributions. The method operates by finding the\nprojection that maximizes the Wasserstein divergence between the resulting\nunivariate populations. Relying on the Cramer-Wold device, it requires no\nassumptions about the form of the underlying distributions, nor the nature of\ntheir inter-class differences. A sparse variant of the method is introduced to\nidentify features responsible for the differences. We provide algorithms for\nboth the original minimax formulation as well as its semidefinite relaxation.\nIn addition to deriving some convergence results, we illustrate how the\napproach may be applied to identify differences between cell populations in the\nsomatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our\nbroader framework extends beyond the specific choice of Wasserstein divergence.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1509.05808,https://arxiv.org/abs/1509.05808,"b'Abstract:  Continuous vector representations of words and objects appear to carry\nsurprisingly rich semantic content. In this paper, we advance both the\nconceptual and theoretical understanding of word embeddings in three ways.\nFirst, we ground embeddings in semantic spaces studied in\ncognitive-psychometric literature and introduce new evaluation tasks. Second,\nin contrast to prior work, we take metric recovery as the key object of study,\nunify existing algorithms as consistent metric recovery methods based on\nco-occurrence counts from simple Markov random walks, and propose a new\nrecovery algorithm. Third, we generalize metric recovery to graphs and\nmanifolds, relating co-occurence counts on random walks in graphs and random\nprocesses on manifolds to the underlying metric to be recovered, thereby\nreconciling manifold estimation and embedding algorithms. We compare embedding\nalgorithms across a range of tasks, from nonlinear dimensionality reduction to\nthree semantic language tasks, including analogies, sequence completion, and\nclassification.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1508.05133,https://arxiv.org/abs/1508.05133,"b'Abstract:  Contemporary deep neural networks exhibit impressive results on practical\nproblems. These networks generalize well although their inherent capacity may\nextend significantly beyond the number of training examples. We analyze this\nbehavior in the context of deep, infinite neural networks. We show that deep\ninfinite layers are naturally aligned with Gaussian processes and kernel\nmethods, and devise stochastic kernels that encode the information of these\nnetworks. We show that stability results apply despite the size, offering an\nexplanation for their empirical success.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1508.04112,https://arxiv.org/abs/1508.04112,"b'Abstract:  The success of deep learning often derives from well-chosen operational\nbuilding blocks. In this work, we revise the temporal convolution operation in\nCNNs to better adapt it to text processing. Instead of concatenating word\nrepresentations, we appeal to tensor algebra and use low-rank n-gram tensors to\ndirectly exploit interactions between words already at the convolution stage.\nMoreover, we extend the n-gram convolution to non-consecutive words to\nrecognize patterns with intervening words. Through a combination of low-rank\ntensors, and pattern weighting, we can efficiently evaluate the resulting\nconvolution operation via dynamic programming. We test the resulting\narchitecture on standard sentiment classification and news categorization\ntasks. Our model achieves state-of-the-art performance both in terms of\naccuracy and training speed. For instance, we obtain 51.2% accuracy on the\nfine-grained sentiment classification task.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1508.00945,https://arxiv.org/abs/1508.00945,"b'Abstract:  Margin-based structured prediction commonly uses a maximum loss over all\npossible structured outputs \\cite{Altun03,Collins04b,Taskar03}. In natural\nlanguage processing, recent work \\cite{Zhang14,Zhang15} has proposed the use of\nthe maximum loss over random structured outputs sampled independently from some\nproposal distribution. This method is linear-time in the number of random\nstructured outputs and trivially parallelizable. We study this family of loss\nfunctions in the PAC-Bayes framework under Gaussian perturbations\n\\cite{McAllester07}. Under some technical conditions and up to statistical\naccuracy, we show that this family of loss functions produces a tighter upper\nbound of the Gibbs decoder distortion than commonly used methods. Thus, using\nthe maximum loss over random structured outputs is a principled way of learning\nthe parameter of structured prediction models. Besides explaining the\nexperimental success of \\cite{Zhang14,Zhang15}, our theoretical results show\nthat more general techniques are possible.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1506.07609,https://arxiv.org/abs/1506.07609,"b'Abstract:  We present a framework for clustering with cluster-specific feature\nselection. The framework, CRAFT, is derived from asymptotic log posterior\nformulations of nonparametric MAP-based clustering models. CRAFT handles\nassorted data, i.e., both numeric and categorical data, and the underlying\nobjective functions are intuitively appealing. The resulting algorithm is\nsimple to implement and scales nicely, requires minimal parameter tuning,\nobviates the need to specify the number of clusters a priori, and compares\nfavorably with other methods on real datasets.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1503.02335,https://arxiv.org/abs/1503.02335,"b'Abstract:  Most state-of-the-art systems today produce morphological analysis based only\non orthographic patterns. In contrast, we propose a model for unsupervised\nmorphological analysis that integrates orthographic and semantic views of\nwords. We model word formation in terms of morphological chains, from base\nwords to the observed words, breaking the chains into parent-child relations.\nWe use log-linear models with morpheme and word-level features to predict\npossible parents, including their modifications, for each word. The limited set\nof candidate parents for each word render contrastive estimation feasible. Our\nmodel consistently matches or outperforms five state-of-the-art systems on\nArabic, English and Turkish.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1411.5720,https://arxiv.org/abs/1411.5720,"b'Abstract:  We analyze directed, unweighted graphs obtained from $x_i\\in \\mathbb{R}^d$ by\nconnecting vertex $i$ to $j$ iff $|x_i - x_j| < \\epsilon(x_i)$. Examples of\nsuch graphs include $k$-nearest neighbor graphs, where $\\epsilon(x_i)$ varies\nfrom point to point, and, arguably, many real world graphs such as\nco-purchasing graphs. We ask whether we can recover the underlying Euclidean\nmetric $\\epsilon(x_i)$ and the associated density $p(x_i)$ given only the\ndirected graph and $d$.\nWe show that consistent recovery is possible up to isometric scaling when the\nvertex degree is at least $\\omega(n^{2/(2+d)}\\log(n)^{d/(d+2)})$. Our estimator\nis based on a careful characterization of a random walk over the directed graph\nand the associated continuum limit. As an algorithm, it resembles the PageRank\ncentrality metric. We demonstrate empirically that the estimator performs well\non simulated examples as well as on real-world co-purchasing graphs even with a\nsmall number of points and degree scaling as low as $\\log(n)$.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1310.4227,https://arxiv.org/abs/1310.4227,"b'Abstract:  The maximum a-posteriori (MAP) perturbation framework has emerged as a useful\napproach for inference and learning in high dimensional complex models. By\nmaximizing a randomly perturbed potential function, MAP perturbations generate\nunbiased samples from the Gibbs distribution. Unfortunately, the computational\ncost of generating so many high-dimensional random variables can be\nprohibitive. More efficient algorithms use sequential sampling strategies based\non the expected value of low dimensional MAP perturbations. This paper develops\nnew measure concentration inequalities that bound the number of samples needed\nto estimate such expected values. Applying the general result to MAP\nperturbations can yield a more efficient algorithm to approximate sampling from\nthe Gibbs distribution. The measure concentration result is of general interest\nand may be applicable to other areas involving expected estimations.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1309.7598,https://arxiv.org/abs/1309.7598,"b'Abstract:  In this paper we describe how MAP inference can be used to sample efficiently\nfrom Gibbs distributions. Specifically, we provide means for drawing either\napproximate or unbiased samples from Gibbs\' distributions by introducing low\ndimensional perturbations and solving the corresponding MAP assignments. Our\napproach also leads to new ways to derive lower bounds on partition functions.\nWe demonstrate empirically that our method excels in the typical ""high signal -\nhigh coupling"" regime. The setting results in ragged energy landscapes that are\nchallenging for alternative approaches to sampling and/or lower bounds.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1309.6838,https://arxiv.org/abs/1309.6838,"b'Abstract:  We propose maximum likelihood estimation for learning Gaussian graphical\nmodels with a Gaussian (ell_2^2) prior on the parameters. This is in contrast\nto the commonly used Laplace (ell_1) prior for encouraging sparseness. We show\nthat our optimization problem leads to a Riccati matrix equation, which has a\nclosed form solution. We propose an efficient algorithm that performs a\nsingular value decomposition of the training data. Our algorithm is\nO(NT^2)-time and O(NT)-space for N variables and T samples. Our method is\ntailored to high-dimensional problems (N gg T), in which sparseness promoting\nmethods become intractable. Furthermore, instead of obtaining a single solution\nfor a specific regularization parameter, our algorithm finds the whole solution\npath. We show that the method has logarithmic sample complexity under the\nspiked covariance model. We also propose sparsification of the dense solution\nwith provable performance guarantees. We provide techniques for using our\nlearnt models, such as removing unimportant variables, computing likelihoods\nand conditional distributions. Finally, we show promising results in several\ngene expressions datasets.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1302.3586,https://arxiv.org/abs/1302.3586,b'Abstract:  We present deterministic techniques for computing upper and lower bounds on\nmarginal probabilities in sigmoid and noisy-OR networks. These techniques\nbecome useful when the size of the network (or clique size) precludes exact\ncomputations. We illustrate the tightness of the bounds by numerical\nexperiments.'
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.3875,https://arxiv.org/abs/1301.3875,"b'Abstract:  In this paper we present decomposable priors, a family of priors over\nstructure and parameters of tree belief nets for which Bayesian learning with\ncomplete observations is tractable, in the sense that the posterior is also\ndecomposable and can be completely determined analytically in polynomial time.\nThis follows from two main results: First, we show that factored distributions\nover spanning trees in a graph can be integrated in closed form. Second, we\nexamine priors over tree parameters and show that a set of assumptions similar\nto (Heckerman and al. 1995) constrain the tree parameter priors to be a\ncompactly parameterized product of Dirichlet distributions. Beside allowing for\nexact Bayesian learning, these results permit us to formulate a new class of\ntractable latent variable models in which the likelihood of a data point is\ncomputed through an ensemble average over tree structures.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.3865,https://arxiv.org/abs/1301.3865,"b'Abstract:  Incorporating feature selection into a classification or regression method\noften carries a number of advantages. In this paper we formalize feature\nselection specifically from a discriminative perspective of improving\nclassification/regression accuracy. The feature selection method is developed\nas an extension to the recently proposed maximum entropy discrimination (MED)\nframework. We describe MED as a flexible (Bayesian) regularization approach\nthat subsumes, e.g., support vector classification, regression and exponential\nfamily models. For brevity, we restrict ourselves primarily to feature\nselection in the context of linear classification/regression methods and\ndemonstrate that the proposed approach indeed carries substantial improvements\nin practice. Moreover, we discuss and develop various extensions of feature\nselection, including the problem of dealing with example specific but\nunobserved degrees of freedom -- alignments or invariants.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.0610,https://arxiv.org/abs/1301.0610,"b'Abstract:  Bounds on the log partition function are important in a variety of contexts,\nincluding approximate inference, model fitting, decision theory, and large\ndeviations analysis. We introduce a new class of upper bounds on the log\npartition function, based on convex combinations of distributions in the\nexponential domain, that is applicable to an arbitrary undirected graphical\nmodel. In the special case of convex combinations of tree-structured\ndistributions, we obtain a family of variational problems, similar to the Bethe\nfree energy, but distinguished by the following desirable properties: i. they\nare cnvex, and have a unique global minimum; and ii. the global minimum gives\nan upper bound on the log partition function. The global minimum is defined by\nstationary conditions very similar to those defining fixed points of belief\npropagation or tree-based reparameterization Wainwright et al., 2001. As with\nBP fixed points, the elements of the minimizing argument can be used as\napproximations to the marginals of the original model. The analysis described\nhere can be extended to structures of higher treewidth e.g., hypertrees,\nthereby making connections with more advanced approximations e.g., Kikuchi and\nvariants Yedidia et al., 2001; Minka, 2001.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.0602,https://arxiv.org/abs/1301.0602,"b'Abstract:  Active learning is a powerful approach to analyzing data effectively. We show\nthat the feasibility of active learning depends crucially on the choice of\nmeasure with respect to which the query is being optimized. The standard\ninformation gain, for example, does not permit an accurate evaluation with a\nsmall committee, a representative subset of the model space. We propose a\nsurrogate measure requiring only a small committee and discuss the properties\nof this new measure. We devise, in addition, a bootstrap approach for committee\nselection. The advantages of this approach are illustrated in the context of\nrecovering (regulatory) network models.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.0562,https://arxiv.org/abs/1301.0562,"b'Abstract:  A number of modern learning tasks involve estimation from heterogeneous\ninformation sources. This includes classification with labeled and unlabeled\ndata as well as other problems with analogous structure such as competitive\n(game theoretic) problems. The associated estimation problems can be typically\nreduced to solving a set of fixed point equations (consistency conditions). We\nintroduce a general method for combining a preferred information source with\nanother in this setting by evolving continuous paths of fixed points at\nintermediate allocations. We explicitly identify critical points along the\nunique paths to either increase the stability of estimation or to ensure a\nsignificant departure from the initial source. The homotopy continuation\napproach is guaranteed to terminate at the second source, and involves no\ncombinatorial effort. We illustrate the power of these ideas both in\nclassification tasks with labeled and unlabeled data, as well as in the context\nof a competitive (min-max) formulation of DNA sequence motif discovery.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1212.2466,https://arxiv.org/abs/1212.2466,"b""Abstract:  We formulate a principle for classification with the knowledge of the\nmarginal distribution over the data points (unlabeled data). The principle is\ncast in terms of Tikhonov style regularization where the regularization penalty\narticulates the way in which the marginal density should constrain otherwise\nunrestricted conditional distributions. Specifically, the regularization\npenalty penalizes any information introduced between the examples and labels\nbeyond what is provided by the available labeled examples. The work extends\nSzummer and Jaakkola's information regularization (NIPS 2002) to multiple\ndimensions, providing a regularizer independent of the covering of the space\nused in the derivation. We show in addition how the information regularizer can\nbe used as a measure of complexity of the classification task with unlabeled\ndata and prove a relevant sample-complexity bound. We illustrate the\nregularization principle in practice by restricting the class of conditional\ndistributions to be logistic regression models and constructing the\nregularization penalty from a finite set of unlabeled examples."""
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1208.5159,https://arxiv.org/abs/1208.5159,"b'Abstract:  This is the Proceedings of the Twenty-First Conference on Uncertainty in\nArtificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29\n2005.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1207.4255,https://arxiv.org/abs/1207.4255,"b'Abstract:  In this paper, we present $\\ell_{1,p}$ multi-task structure learning for\nGaussian graphical models. We analyze the sufficient number of samples for the\ncorrect recovery of the support union and edge signs. We also analyze the\nnecessary number of samples for any conceivable method by providing\ninformation-theoretic lower bounds. We compare the statistical efficiency of\nmulti-task learning versus that of single-task learning. For experiments, we\nuse a block coordinate descent method that is provably convergent and generates\na sequence of positive definite solutions. We provide experimental validation\non synthetic data as well as on two publicly available real-world data sets,\nincluding functional magnetic resonance imaging and gene expression data.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1206.6410,https://arxiv.org/abs/1206.6410,"b'Abstract:  In this paper we relate the partition function to the max-statistics of\nrandom variables. In particular, we provide a novel framework for approximating\nand bounding the partition function using MAP inference on randomly perturbed\nmodels. As a result, we can use efficient MAP solvers such as graph-cuts to\nevaluate the corresponding partition function. We show that our method excels\nin the typical ""high signal - high coupling"" regime that results in ragged\nenergy landscapes difficult for alternative approaches.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1206.5243,https://arxiv.org/abs/1206.5243,"b'Abstract:  Inference problems in graphical models are often approximated by casting them\nas constrained optimization problems. Message passing algorithms, such as\nbelief propagation, have previously been suggested as methods for solving these\noptimization problems. However, there are few convergence guarantees for such\nalgorithms, and the algorithms are therefore not guaranteed to solve the\ncorresponding optimization problem. Here we present an oriented tree\ndecomposition algorithm that is guaranteed to converge to the global optimum of\nthe Tree-Reweighted (TRW) variational problem. Our algorithm performs local\nupdates in the convex dual of the TRW problem - an unconstrained generalized\ngeometric program. Primal updates, also local, correspond to oriented\nreparametrization operations that leave the distribution intact.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1206.3288,https://arxiv.org/abs/1206.3288,"b'Abstract:  Linear Programming (LP) relaxations have become powerful tools for finding\nthe most probable (MAP) configuration in graphical models. These relaxations\ncan be solved efficiently using message-passing algorithms such as belief\npropagation and, when the relaxation is tight, provably find the MAP\nconfiguration. The standard LP relaxation is not tight enough in many\nreal-world problems, however, and this has lead to the use of higher order\ncluster-based LP relaxations. The computational cost increases exponentially\nwith the size of the clusters and limits the number and type of clusters we can\nuse. We propose to solve the cluster selection problem monotonically in the\ndual LP, iteratively selecting clusters with guaranteed improvement, and\nquickly re-solving with the added clusters by reusing the existing solution.\nOur dual message-passing algorithm finds the MAP configuration in protein\nsidechain placement, protein design, and stereo problems, in cases where the\nstandard LP relaxation fails.'"
Tommi Jaakkola,Jaakkola_Tommi,arXiv:cs/0508070,https://arxiv.org/abs/cs/0508070,"b'Abstract:  We develop and analyze methods for computing provably optimal {\\em maximum a\nposteriori} (MAP) configurations for a subclass of Markov random fields defined\non graphs with cycles. By decomposing the original distribution into a convex\ncombination of tree-structured distributions, we obtain an upper bound on the\noptimal value of the original problem (i.e., the log probability of the MAP\nassignment) in terms of the combined optimal values of the tree problems. We\nprove that this upper bound is tight if and only if all the tree distributions\nshare an optimal configuration in common. An important implication is that any\nsuch shared configuration must also be a MAP configuration for the original\ndistribution. Next we develop two approaches to attempting to obtain tight\nupper bounds: (a) a {\\em tree-relaxed linear program} (LP), which is derived\nfrom the Lagrangian dual of the upper bounds; and (b) a {\\em tree-reweighted\nmax-product message-passing algorithm} that is related to but distinct from the\nmax-product algorithm. In this way, we establish a connection between a certain\nLP relaxation of the mode-finding problem, and a reweighted form of the\nmax-product (min-sum) message-passing algorithm.'"
Daniel Jackson,Jackson_Daniel,arXiv:1812.07540,https://arxiv.org/abs/1812.07540,"b'Abstract:  Coherent excitation of an ensemble of quantum objects underpins quantum\nmany-body phenomena, and offers the opportunity to realize a quantum memory to\nstore information from a qubit. Thus far, a deterministic and coherent\ninterface between a single quantum system, e.g. a qubit, and such an ensemble\nhas remained elusive. We first use an electron to cool the mesoscopic\nnuclear-spin ensemble of a semiconductor quantum dot to the nuclear\nsideband-resolved regime. We then implement an all-optical approach to access\nthese individual quantized electronic-nuclear spin transitions. Finally, we\nperform coherent optical rotations of a single collective nuclear spin\nexcitation corresponding to a spin wave called a nuclear magnon. These results\nconstitute the building blocks of a dedicated local memory per quantum-dot spin\nqubit and promise a solid-state platform for quantum-state engineering of\nisolated many-body systems.'"
Daniel Jackson,Jackson_Daniel,arXiv:1808.08026,https://arxiv.org/abs/1808.08026,"b'Abstract:  The Einstein-Yang-Mills equations are the source of many interesting\nsolutions within general relativity, including families of particle-like and\nblack hole solutions, and critical phenomena of more than one type. These\nsolutions, discovered in the last thirty years, all assume a restricted form\nfor the Yang-Mills gauge potential known as the ""magnetic"" ansatz. In this\nthesis we relax that assumption and investigate the most general solutions of\nthe Einstein-Yang-Mills system assuming spherically symmetry, a Yang-Mills\ngauge group of SU(2), and zero cosmological constant. We proceed primarily by\nnumerically integrating the equations and find new static solutions, for both\nregular and black hole boundary conditions, which are not asymptotically flat,\nand attempt to classify the possible static behaviours. We develop a code to\nsolve the dynamic equations that uses a novel adaptive mesh refinement\nalgorithm making full use of double-null coordinates. We find that the ""type\nII"" critical behaviour in the general case exhibits non-universal critical\nsolutions, in contrast to the magnetic case and to all previously observed type\nII critical behaviour.'"
Daniel Jackson,Jackson_Daniel,arXiv:1805.09288,https://arxiv.org/abs/1805.09288,"b'Abstract:  The recently discovered (Rb,Cs)EuFe4As4 compounds exhibit an unusual\ncombination of superconductivity (Tc = 35 K) and ferromagnetism (Tm = 15 K). We\nhave performed a series of x-ray diffraction, ac magnetic susceptibility, dc\nmagnetization, and electrical resistivity measurements on both RbEuFe4As4 and\nCsEuFe4As4 to pressures as high as 30 GPa. We find that the superconductivity\nonset is suppressed monotonically by pressure while the magnetic transition is\nenhanced at initial rates of dTm/dP = 1.7 K/GPa and 1.5 K/GPa for RbEuFe4As4\nand CsEuFe4As4, respectively. Near 7 GPa, Tc onset and Tm become comparable. At\nhigher pressures, signatures of bulk superconductivity gradually disappear.\nRoom temperature x-ray diffraction measurements suggest the onset of a\ntransition from tetragonal (T) to a half collapsed-tetragonal (hcT) phase at 10\nGPa (RbEuFe4As4) and 12 GPa (CsEuFe4As4). The ability to tune Tc and Tm into\ncoincidence with relatively modest pressures highlights (Rb,Cs)EuFe4As4\ncompounds as ideal systems to study the interplay of superconductivity and\nferromagnetism.'"
Daniel Jackson,Jackson_Daniel,arXiv:1009.3478,https://arxiv.org/abs/1009.3478,"b'Abstract:  We describe an algorithm for distinguishing hyperbolic components in the\nparameter space of quadratic rational maps with a periodic critical point. We\nthen illustrate computer images of the hyperbolic components of the parameter\nspaces V1 - V4, which were produced using our algorithm. We also resolve the\nsingularities of the projective closure of V5 by blowups, giving an alternative\nproof that as an algebraic curve, the geometric genus of V5 is 1. This explains\nwhy we are unable to produce an image for V5.'"
Daniel Jackson,Jackson_Daniel,arXiv:cs/0605109,https://arxiv.org/abs/cs/0605109,"b'Abstract:  Knowledge flow analysis offers a simple and flexible way to find flaws in\nsecurity protocols. A protocol is described by a collection of rules\nconstraining the propagation of knowledge amongst principals. Because this\ncharacterization corresponds closely to informal descriptions of protocols, it\nallows a succinct and natural formalization; because it abstracts away message\nordering, and handles communications between principals and applications of\ncryptographic primitives uniformly, it is readily represented in a standard\nlogic. A generic framework in the Alloy modelling language is presented, and\ninstantiated for two standard protocols, and a new key management scheme.'"
Daniel Jackson,Jackson_Daniel,arXiv:math/0505014,https://arxiv.org/abs/math/0505014,"b'Abstract:  We classify invariant curves for birational surface maps that are expanding\non cohomology. When the expansion is exponential, the arithmetic genus of an\ninvariant curve is at most one. This implies severe constraints on both the\ntype and number of irreducible components of the curve. In the case of an\ninvariant curve with genus equal to one, we show that there is an associated\ninvariant meromorphic two form.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1811.08516,https://arxiv.org/abs/1811.08516,"b""Abstract:  We consider the following problem: Does there exist a probability\ndistribution over subsets of a finite partially ordered set (poset), such that\na set of constraints involving marginal probabilities of the poset's elements\nand maximal chains is satisfied? In this article, we present a combinatorial\nalgorithm to positively resolve this question. We show that this result plays a\ncrucial role in the equilibrium analysis of a generic security game on a\ncapacitated flow network. The game involves a routing entity that sends its\nflow through the network while facing path transportation costs, and an\ninterdictor who simultaneously interdicts one or more edges while facing edge\ninterdiction costs. The first (resp. second) player seeks to maximize the value\nof effective (resp. interdicted) flow net the total transportation (resp.\ninterdiction) cost. Using our existence result on posets and strict\ncomplementary slackness in linear programming, we show that the equilibrium\nproperties of this game can be described using primal and dual solutions of a\nminimum cost circulation problem. Our analysis provides a new characterization\nof the critical network components."""
Patrick Jaillet,Jaillet_Patrick,arXiv:1810.07732,https://arxiv.org/abs/1810.07732,"b'Abstract:  In this brief paper we find computable exponential convergence rates for a\nlarge class of stochastically ordered Markov processes. We extend the result of\nLund, Meyn, and Tweedie (1996), who found exponential convergence rates for\nstochastically ordered Markov processes starting from a fixed initial state, by\nallowing for a random initial condition that is also stochastically ordered.\nOur bounds are formulated in terms of moment-generating functions of hitting\ntimes. To illustrate our result, we find an explicit exponential convergence\nrate for an M/M/1 queue beginning in equilibrium and then experiencing a change\nin its arrival or departure rates, a setting which has not been studied to our\nknowledge.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1810.00447,https://arxiv.org/abs/1810.00447,"b'Abstract:  For online resource allocation problems, we propose a new demand arrival\nmodel where the sequence of arrivals contains both an adversarial component and\na stochastic one. Our model requires no demand forecasting; however, due to the\npresence of the stochastic component, we can partially predict future demand as\nthe sequence of arrivals unfolds. Under the proposed model, we study the\nproblem of the online allocation of a single resource to two types of\ncustomers, and design online algorithms that outperform existing ones. Our\nalgorithms are adjustable to the relative size of the stochastic component, and\nour analysis reveals that as the portion of the stochastic component grows, the\nloss due to making online decisions decreases. This highlights the value of\n(even partial) predictability in online resource allocation. We impose no\nconditions on how the resource capacity scales with the maximum number of\ncustomers. However, we show that using an adaptive algorithm---which makes\nonline decisions based on observed data---is particularly beneficial when\ncapacity scales linearly with the number of customers. Our work serves as a\nfirst step in bridging the long-standing gap between the two well-studied\napproaches to the design and analysis of online algorithms based on (1)\nadversarial models and (2) stochastic ones. Using novel algorithm design, we\ndemonstrate that even if the arrival sequence contains an adversarial\ncomponent, we can take advantage of the limited information that the data\nreveals to improve allocation decisions. We also study the classical secretary\nproblem under our proposed arrival model, and we show that randomizing over\nmultiple stopping rules may increase the probability of success.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1808.03526,https://arxiv.org/abs/1808.03526,"b""Abstract:  We study the problem of matching agents who arrive at a marketplace over time\nand leave after d time periods. Agents can only be matched while they are\npresent in the marketplace. Each pair of agents can yield a different match\nvalue, and the planner's goal is to maximize the total value over a finite time\nhorizon. First we study the case in which vertices arrive in an adversarial\norder. We provide a randomized 0.25-competitive algorithm building on a result\nby Feldman et al. (2009) and Lehman et al. (2006). We extend the model to the\ncase in which departure times are drawn independently from a distribution with\nnon-decreasing hazard rate, for which we establish a 1/8-competitive algorithm.\nWhen the arrival order is chosen uniformly at random, we show that a batching\nalgorithm, which computes a maximum-weighted matching every (d+1) periods, is\n0.279-competitive."""
Patrick Jaillet,Jaillet_Patrick,arXiv:1803.08415,https://arxiv.org/abs/1803.08415,"b""Abstract:  Vehicle-to-Infrastructure (V2I) communications are increasingly supporting\nhighway operations such as electronic toll collection, carpooling, and vehicle\nplatooning. In this paper we study the incentives of strategic misbehavior by\nindividual vehicles who can exploit the security vulnerabilities in V2I\ncommunications and negatively impact the highway operations. We consider a\nV2I-enabled highway segment facing two classes of vehicles (agent populations),\neach with an authorized access to one server (subset of lanes). Vehicles are\nstrategic in that they can misreport their class (type) to the system operator\nand get an unauthorized access to the server dedicated to the other class. This\nmisbehavior causes additional congestion externality on the compliant vehicles,\nand thus, needs to be deterred. We focus on an environment where the operator\nis able to inspect the vehicles for misbehavior. The inspection is costly and\nsuccessful detection incurs a fine on the misbehaving vehicle. We formulate a\nsignaling game to study the strategic interaction between the vehicle classes\nand the operator. Our equilibrium analysis provides conditions on the cost\nparameters that govern the vehicles' incentive to misbehave or not. We also\ndetermine the operator's equilibrium inspection strategy."""
Patrick Jaillet,Jaillet_Patrick,arXiv:1803.01285,https://arxiv.org/abs/1803.01285,"b""Abstract:  We study the problem of matching agents who arrive at a marketplace over time\nand leave after d time periods. Agents can only be matched while they are\npresent in the marketplace. Each pair of agents can yield a different match\nvalue, and the planner's goal is to maximize the total value over a finite time\nhorizon. We study matching algorithms that perform well over any sequence of\narrivals when there is no a priori information about the match values or\narrival times.\nOur main contribution is a 1/4-competitive algorithm. The algorithm randomly\nselects a subset of agents who will wait until right before their departure to\nget matched, and maintains a maximum-weight matching with respect to the other\nagents. The primal-dual analysis of the algorithm hinges on a careful\ncomparison between the initial dual value associated with an agent when it\nfirst arrives, and the final value after d time steps.\nIt is also shown that no algorithm is 1/2-competitive. We extend the model to\nthe case in which departure times are drawn i.i.d from a distribution with\nnon-decreasing hazard rate, and establish a 1/8-competitive algorithm in this\nsetting. Finally we show on real-world data that a modified version of our\nalgorithm performs well in practice."""
Patrick Jaillet,Jaillet_Patrick,arXiv:1711.00221,https://arxiv.org/abs/1711.00221,"b'Abstract:  This paper presents a novel variational inference framework for deriving a\nfamily of Bayesian sparse Gaussian process regression (SGPR) models whose\napproximations are variationally optimal with respect to the full-rank GPR\nmodel enriched with various corresponding correlation structures of the\nobservation noises.\nOur variational Bayesian SGPR (VBSGPR) models jointly treat both the\ndistributions of the inducing variables and hyperparameters as variational\nparameters, which enables the decomposability of the variational lower bound\nthat in turn can be exploited for stochastic optimization.\nSuch a stochastic optimization involves iteratively following the stochastic\ngradient of the variational lower bound to improve its estimates of the optimal\nvariational distributions of the inducing variables and hyperparameters (and\nhence the predictive distribution) of our VBSGPR models and is guaranteed to\nachieve asymptotic convergence to them.\nWe show that the stochastic gradient is an unbiased estimator of the exact\ngradient and can be computed in constant time per iteration, hence achieving\nscalability to big data.\nWe empirically evaluate the performance of our proposed framework on two\nreal-world, massive datasets.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1703.04769,https://arxiv.org/abs/1703.04769,"b'Abstract:  The Container Relocation Problem (CRP) is concerned with finding a sequence\nof moves of containers that minimizes the number of relocations needed to\nretrieve all containers, while respecting a given order of retrieval. However,\nthe assumption of knowing the full retrieval order of containers is\nparticularly unrealistic in real operations. This paper studies the stochastic\nCRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model,\ncalled the batch model, is introduced, motivated, and compared with an existing\nmodel (the online model). The two main contributions are an optimal algorithm\ncalled Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm\ncalled PBFS-Approximate with a bounded average error. Both algorithms,\napplicable in the batch and online models, are based on a new family of lower\nbounds for which we show some theoretical properties. Moreover, we introduce\ntwo new heuristics outperforming the best existing heuristics. Algorithms,\nbounds and heuristics are tested in an extensive computational section.\nFinally, based on strong computational evidence, we conjecture the optimality\nof the ""Leveling"" heuristic in a special ""no information"" case, where at any\nretrieval stage, any of the remaining containers is equally likely to be\nretrieved next.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1703.01484,https://arxiv.org/abs/1703.01484,"b'Abstract:  We study a convex resource allocation problem in which lower and upper bounds\nare imposed on partial sums of allocations. This model is linked to a large\nrange of applications, including production planning, speed optimization,\nstratified sampling, support vector machines, portfolio management, and\ntelecommunications. We propose an efficient gradient-free divide-and-conquer\nalgorithm, which uses monotonicity arguments to generate valid bounds from the\nrecursive calls, and eliminate linking constraints based on the information\nfrom sub-problems. This algorithm does not need strict convexity or\ndifferentiability. It produces an $\\epsilon$-approximate solution for the\ncontinuous problem in $\\mathcal{O}(n \\log m \\log \\frac{n B}{\\epsilon})$ time\nand an integer solution in $\\mathcal{O}(n \\log m \\log B)$ time, where $n$ is\nthe number of decision variables, $m$ is the number of constraints, and $B$ is\nthe resource bound. A complexity of $\\mathcal{O}(n \\log m)$ is also achieved\nfor the linear and quadratic cases. These are the best complexities known to\ndate for this important problem class. Our experimental analyses confirm the\ngood performance of the method, which produces optimal solutions for problems\nwith up to 1,000,000 variables in a few seconds. Promising applications to the\nsupport vector ordinal regression problem are also investigated.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1611.07096,https://arxiv.org/abs/1611.07096,"b'Abstract:  We propose a general approach for supervised learning with structured output\nspaces, such as combinatorial and polyhedral sets, that is based on minimizing\nestimated conditional risk functions. Given a loss function defined over pairs\nof output labels, we first estimate the conditional risk function by solving a\n(possibly infinite) collection of regularized least squares problems. A\nprediction is made by solving an inference problem that minimizes the estimated\nconditional risk function over the output space. We show that this approach\nenables, in some cases, efficient training and inference without explicitly\nintroducing a convex surrogate for the original loss function, even when it is\ndiscontinuous. Empirical evaluations on real-world and synthetic data sets\ndemonstrate the effectiveness of our method in adapting to a variety of loss\nfunctions.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1606.03626,https://arxiv.org/abs/1606.03626,"b'Abstract:  We study dynamic matching in an infinite-horizon stochastic market. While all\nagents are potentially compatible with each other, some are hard-to-match and\nothers are easy-to-match. Agents prefer to be matched as soon as possible and\nmatches are formed either bilaterally or indirectly through chains. We adopt an\nasymptotic approach and compute tight bounds on the limit of waiting time of\nagents under myopic policies that differ in matching technology and\nprioritization.\nWe find that the market composition is a key factor in the desired matching\ntechnology and prioritization level. When hard-to-match agents arrive less\nfrequently than easy-to-match ones (i) bilateral matching is almost as\nefficient as chains (waiting times scale similarly under both, though chains\nalways outperform bilateral matching by a constant factor), and (ii) assigning\npriorities to hard-to-match agents improves their waiting times. When\nhard-to-match agents arrive more frequently, chains are much more efficient\nthan bilateral matching and prioritization has no impact.\nWe further conduct comparative statics on arrival rates. Somewhat\nsurprisingly, we find that in a heterogeneous market and under bilateral\nmatching, increasing arrival rate has a non-monotone effect on waiting times,\ndue to the fact that, under some market compositions, there is an adverse\neffect of competition. Our comparative statics shed light on the impact of\nmerging markets and attracting altruistic agents (that initiate chains) or\neasy-to-match agents.\nThis work uncovers fundamental differences between heterogeneous and\nhomogeneous dynamic markets, and potentially helps policy makers to generate\ninsights on the operations of matching markets such as kidney exchange\nprograms.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1603.00522,https://arxiv.org/abs/1603.00522,"b'Abstract:  In order to find Nash-equilibria for two-player zero-sum games where each\nplayer plays combinatorial objects like spanning trees, matchings etc, we\nconsider two online learning algorithms: the online mirror descent (OMD)\nalgorithm and the multiplicative weights update (MWU) algorithm. The OMD\nalgorithm requires the computation of a certain Bregman projection, that has\nclosed form solutions for simple convex sets like the Euclidean ball or the\nsimplex. However, for general polyhedra one often needs to exploit the general\nmachinery of convex optimization. We give a novel primal-style algorithm for\ncomputing Bregman projections on the base polytopes of polymatroids. Next, in\nthe case of the MWU algorithm, although it scales logarithmically in the number\nof pure strategies or experts $N$ in terms of regret, the algorithm takes time\npolynomial in $N$; this especially becomes a problem when learning\ncombinatorial objects. We give a general recipe to simulate the multiplicative\nweights update algorithm in time polynomial in their natural dimension. This is\nuseful whenever there exists a polynomial time generalized counting oracle\n(even if approximate) over these objects. Finally, using the combinatorial\nstructure of symmetric Nash-equilibria (SNE) when both players play bases of\nmatroids, we show that these can be found with a single projection or convex\nminimization (without using online learning).'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1511.06890,https://arxiv.org/abs/1511.06890,"b'Abstract:  This paper presents a novel nonmyopic adaptive Gaussian process planning\n(GPP) framework endowed with a general class of Lipschitz continuous reward\nfunctions that can unify some active learning/sensing and Bayesian optimization\ncriteria and offer practitioners some flexibility to specify their desired\nchoices for defining new tasks/problems. In particular, it utilizes a\nprincipled Bayesian sequential decision problem framework for jointly and\nnaturally optimizing the exploration-exploitation trade-off. In general, the\nresulting induced GPP policy cannot be derived exactly due to an uncountable\nset of candidate observations. A key contribution of our work here thus lies in\nexploiting the Lipschitz continuity of the reward functions to solve for a\nnonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real\ntime, we further propose an asymptotically optimal, branch-and-bound anytime\nvariant of epsilon-GPP with performance guarantee. We empirically demonstrate\nthe effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian\noptimization and an energy harvesting task.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1510.01800,https://arxiv.org/abs/1510.01800,"b'Abstract:  Optimal regret bounds for Multi-Armed Bandit problems are now well\ndocumented. They can be classified into two categories based on the growth rate\nwith respect to the time horizon $T$: (i) small, distribution-dependent, bounds\nof order of magnitude $\\ln(T)$ and (ii) robust, distribution-free, bounds of\norder of magnitude $\\sqrt{T}$. The Bandits with Knapsacks model, an extension\nto the framework allowing to model resource consumption, lacks this clear-cut\ndistinction. While several algorithms have been shown to achieve asymptotically\noptimal distribution-free bounds on regret, there has been little progress\ntoward the development of small distribution-dependent regret bounds. We\npartially bridge the gap by designing a general-purpose algorithm with\ndistribution-dependent regret bounds that are logarithmic in the initial\nendowments of resources in several important cases that cover many practical\napplications, including dynamic pricing with limited supply, bid optimization\nin online advertisement auctions, and dynamic procurement.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1505.04229,https://arxiv.org/abs/1505.04229,"b'Abstract:  The Container Relocation Problem (CRP) is concerned with finding a sequence\nof moves of containers that minimizes the number of relocations needed to\nretrieve all containers respecting a given order of retrieval. While the\nproblem is known to be NP-hard, certain algorithms such as the A* search and\nheuristics perform reasonably well on many instances of the problem. In this\npaper, we first focus on the A* search algorithm, and analyze lower and upper\nbounds that are easy to compute and can be used to prune nodes. Our analysis\nsheds light on which bounds result in fast computation within a given\napproximation gap. We present extensive simulation results that improve upon\nour theoretical analysis, and further show that our method finds the optimum\nsolution on most instances of medium-size bays. On ""hard"" instances, our method\nfinds an approximate solution with a small gap and within a time frame that is\nfast for practical applications. We also study the average-case asymptotic\nbehavior of the CRP where the number of columns grows. We calculate the\nexpected number of relocations in the limit, and show that the optimum number\nof relocations converges to a simple and intuitive lower-bound. We further\nstudy the CRP with incomplete information by relaxing the assumption that the\norder of retrieval of all containers are initially known. This assumption is\nparticularly unrealistic in ports without an appointment system. We assume that\nthe retrieval order of a subset of containers is known initially and the\nretrieval order of the remaining containers is observed later at a given\nspecific time. Before this time, we assume a probabilistic distribution on the\nretrieval order of unknown containers. We combine the A* algorithm with\nsampling technique to solve this two-stage stochastic optimization problem. We\nshow that our algorithm is fast and the error due to sampling and pruning is\nreasonably small.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1503.01535,https://arxiv.org/abs/1503.01535,"b'Abstract:  We introduce a new model and mathematical formulation for planning crane\nmoves in the storage yard of container terminals. Our objective is to develop a\ntool that captures customer centric elements, especially service time, and\nhelps operators to manage costly relocation moves. Our model incorporates\nseveral practical details and provides port operators with expanded\ncapabilities including planning repositioning moves in off-peak hours,\ncontrolling wait times of each customer as well as total service time,\noptimizing the number of relocations and wait time jointly, and optimizing\nsimultaneously the container stacking and retrieval process. We also study a\nclass of flexible service policies which allow for out-of-order retrieval. We\nshow that under such flexible policies, we can decrease the number of\nrelocations and retrieval delays without creating inequities.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1412.2123,https://arxiv.org/abs/1412.2123,"b'Abstract:  We consider and formulate a class of distributed multi-depot routing\nproblems, where servers are to visit a set of requests, with the aim of\nminimizing the total distance travelled by all servers. These problems fall\ninto two categories: distributed offline routing problems where all the\nrequests that need to be visited are known from the start; distributed online\nrouting problems where the requests come to be known incrementally. A critical\nand novel feature of our formulations is that communications are not allowed\namong the servers, hence posing an interesting and challenging question: what\nperformance can be achieved in comparison to the best possible solution\nobtained from an omniscience planner with perfect communication capabilities?\nThe worst-case (over all possible request-set instances) performance metrics\nare given by the approximation ratio (offline case) and the competitive ratio\n(online case).\nOur first result indicates that the online and offline problems are\neffectively equivalent: for the same request-set instance, the approximation\nratio and the competitive ratio differ by at most an additive factor of 2,\nirrespective of the release dates in the online case. Therefore, we can\nrestrict our attention to the offline problem. For the offline problem, we show\nthat the approximation ratio given by the Voronoi partition is m (the number of\nservers). For two classes of depot configurations, when the depots form a line\nand when the ratios between the distances of pairs of depots are upper bounded\nby a sublinear function f(m) (i.e., f(m) = o(m)), we give partition schemes\nwith sublinear approximation ratios O(log m) and {\\Theta}(f(m)) respectively.\nWe also discuss several interesting open problems in our formulations: in\nparticular, how our initial results (on the two deliberately chosen classes of\ndepots) shape our conjecture on the open problems.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1411.5649,https://arxiv.org/abs/1411.5649,"b""Abstract:  In the convex optimization approach to online regret minimization, many\nmethods have been developed to guarantee a $O(\\sqrt{T})$ bound on regret for\nsubdifferentiable convex loss functions with bounded subgradients, by using a\nreduction to linear loss functions. This suggests that linear loss functions\ntend to be the hardest ones to learn against, regardless of the underlying\ndecision spaces. We investigate this question in a systematic fashion looking\nat the interplay between the set of possible moves for both the decision maker\nand the adversarial environment. This allows us to highlight sharp distinctive\nbehaviors about the learnability of piecewise linear loss functions. On the one\nhand, when the decision set of the decision maker is a polyhedron, we establish\n$\\Omega(\\sqrt{T})$ lower bounds on regret for a large class of piecewise linear\nloss functions with important applications in online linear optimization,\nrepeated zero-sum Stackelberg games, online prediction with side information,\nand online two-stage optimization. On the other hand, we exhibit $o(\\sqrt{T})$\nlearning rates, achieved by the Follow-The-Leader algorithm, in online linear\noptimization when the boundary of the decision maker's decision set is curved\nand when $0$ does not lie in the convex hull of the environment's decision set.\nHence, the curvature of the decision maker's decision set is a determining\nfactor for the optimal learning rate. These results hold in a completely\nadversarial setting."""
Patrick Jaillet,Jaillet_Patrick,arXiv:1411.4510,https://arxiv.org/abs/1411.4510,"b'Abstract:  The expressive power of a Gaussian process (GP) model comes at a cost of poor\nscalability in the data size. To improve its scalability, this paper presents a\nlow-rank-cum-Markov approximation (LMA) of the GP model that is novel in\nleveraging the dual computational advantages stemming from complementing a\nlow-rank approximate representation of the full-rank GP based on a support set\nof inputs with a Markov approximation of the resulting residual process; the\nlatter approximation is guaranteed to be closest in the Kullback-Leibler\ndistance criterion subject to some constraint and is considerably more refined\nthan that of existing sparse GP models utilizing low-rank representations due\nto its more relaxed conditional independence assumption (especially with larger\ndata). As a result, our LMA method can trade off between the size of the\nsupport set and the order of the Markov property to (a) incur lower\ncomputational cost than such sparse GP models while achieving predictive\nperformance comparable to them and (b) accurately represent features/patterns\nof any scale. Interestingly, varying the Markov order produces a spectrum of\nLMAs with PIC approximation and full-rank GP at the two extremes. An advantage\nof our LMA method is that it is amenable to parallelization on multiple\nmachines/cores, thereby gaining greater scalability. Empirical evaluation on\nthree real-world datasets in clusters of up to 32 computing nodes shows that\nour centralized and parallel LMA methods are significantly more time-efficient\nand scalable than state-of-the-art sparse and full-rank GP regression methods\nwhile achieving comparable predictive performances.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1408.3374,https://arxiv.org/abs/1408.3374,"b'Abstract:  We consider the problem of finding an optimal history-dependent routing\nstrategy on a directed graph weighted by stochastic arc costs when the\nobjective is to minimize the risk of spending more than a prescribed budget. To\nhelp mitigate the impact of the lack of information on the arc cost probability\ndistributions, we introduce a robust counterpart where the distributions are\nonly known through confidence intervals on some statistics such as the mean,\nthe mean absolute deviation, and any quantile. Leveraging recent results in\ndistributionally robust optimization, we develop a general-purpose algorithm to\ncompute an approximate optimal strategy. To illustrate the benefits of the\nrobust approach, we run numerical experiments with field data from the\nSingapore road network.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1408.2060,https://arxiv.org/abs/1408.2060,"b'Abstract:  Gaussian processes (GP) are Bayesian non-parametric models that are widely\nused for probabilistic regression. Unfortunately, it cannot scale well with\nlarge data nor perform real-time predictions due to its cubic time cost in the\ndata size. This paper presents two parallel GP regression methods that exploit\nlow-rank covariance matrix approximations for distributing the computational\nload among parallel machines to achieve time efficiency and scalability. We\ntheoretically guarantee the predictive performances of our proposed parallel\nGPs to be equivalent to that of some centralized approximate GP regression\nmethods: The computation of their centralized counterparts can be distributed\namong parallel machines, hence achieving greater time efficiency and\nscalability. We analytically compare the properties of our parallel GPs such as\ntime, space, and communication complexity. Empirical evaluation on two\nreal-world datasets in a cluster of 20 computing nodes shows that our parallel\nGPs are significantly more time-efficient and scalable than their centralized\ncounterparts and exact/full GP while achieving predictive performances\ncomparable to full GP.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1408.2046,https://arxiv.org/abs/1408.2046,"b'Abstract:  The problem of modeling and predicting spatiotemporal traffic phenomena over\nan urban road network is important to many traffic applications such as\ndetecting and forecasting congestion hotspots. This paper presents a\ndecentralized data fusion and active sensing (D2FAS) algorithm for mobile\nsensors to actively explore the road network to gather and assimilate the most\ninformative data for predicting the traffic phenomenon. We analyze the time and\ncommunication complexity of D2FAS and demonstrate that it can scale well with a\nlarge number of observations and sensors. We provide a theoretical guarantee on\nits predictive performance to be equivalent to that of a sophisticated\ncentralized sparse approximation for the Gaussian process (GP) model: The\ncomputation of such a sparse approximate GP model can thus be parallelized and\ndistributed among the mobile sensors (in a Google-like MapReduce paradigm),\nthereby achieving efficient and scalable prediction. We also theoretically\nguarantee its active sensing performance that improves under various practical\nenvironmental conditions. Empirical evaluation on real-world urban road network\ndata shows that our D2FAS algorithm is significantly more time-efficient and\nscalable than state-oftheart centralized algorithms while achieving comparable\npredictive performance.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1404.6694,https://arxiv.org/abs/1404.6694,"b'Abstract:  We propose an exact polynomial algorithm for a resource allocation problem\nwith convex costs and constraints on partial sums of resource consumptions, in\nthe presence of either continuous or integer variables. No assumption of strict\nconvexity or differentiability is needed. The method solves a hierarchy of\nresource allocation subproblems, whose solutions are used to convert\nconstraints on sums of resources into bounds for separate variables at higher\nlevels. The resulting time complexity for the integer problem is $O(n \\log m\n\\log (B/n))$, and the complexity of obtaining an $\\epsilon$-approximate\nsolution for the continuous case is $O(n \\log m \\log (B/\\epsilon))$, $n$ being\nthe number of variables, $m$ the number of ascending constraints (such that $m\n< n$), $\\epsilon$ a desired precision, and $B$ the total resource. This\nalgorithm attains the best-known complexity when $m = n$, and improves it when\n$\\log m = o(\\log n)$. Extensive experimental analyses are conducted with four\nrecent algorithms on various continuous problems issued from theory and\npractice. The proposed method achieves a higher performance than previous\nalgorithms, addressing all problems with up to one million variables in less\nthan one minute on a modern computer.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1401.7043,https://arxiv.org/abs/1401.7043,"b""Abstract:  The minmax regret problem for combinatorial optimization under uncertainty\ncan be viewed as a zero-sum game played between an optimizing player and an\nadversary, where the optimizing player selects a solution and the adversary\nselects costs with the intention of maximizing the regret of the player. The\nexisting minmax regret model considers only deterministic solutions/strategies,\nand minmax regret versions of most polynomial solvable problems are NP-hard. In\nthis paper, we consider a randomized model where the optimizing player selects\na probability distribution (corresponding to a mixed strategy) over solutions\nand the adversary selects costs with knowledge of the player's distribution,\nbut not its realization. We show that under this randomized model, the minmax\nregret version of any polynomial solvable combinatorial problem becomes\npolynomial solvable. This holds true for both the interval and discrete\nscenario representations of uncertainty. Using the randomized model, we show\nnew proofs of existing approximation algorithms for the deterministic model\nbased on primal-dual approaches. Finally, we prove that minmax regret problems\nare NP-hard under general convex uncertainty."""
Patrick Jaillet,Jaillet_Patrick,arXiv:1308.6705,https://arxiv.org/abs/1308.6705,"b""Abstract:  Many modern and growing cities are facing declines in public transport usage,\nwith few efficient methods to explain why. In this article, we show that urban\nmobility patterns and transport mode choices can be derived from cellphone call\ndetail records coupled with public transport data recorded from smart cards.\nSpecifically, we present new data mining approaches to determine the spatial\nand temporal variability of public and private transportation usage and\ntransport mode preferences across Singapore. Our results, which were validated\nby Singapore's quadriennial Household Interview Travel Survey (HITS), revealed\nthat there are 3.5 (HITS: 3.5 million) million and 4.3 (HITS: 4.4 million)\nmillion inter-district passengers by public and private transport,\nrespectively. Along with classifying which transportation connections are weak\nor underserved, the analysis shows that the mode share of public transport use\nincreases from 38 percent in the morning to 44 percent around mid-day and 52\npercent in the evening."""
Patrick Jaillet,Jaillet_Patrick,arXiv:1307.2536,https://arxiv.org/abs/1307.2536,"b'Abstract:  We study the average performance of online greedy matching algorithms on\n$G(n,n,p)$, the random bipartite graph with $n$ vertices on each side and edges\noccurring independently with probability $p=p(n)$. In the online model,\nvertices on one side of the graph are given up front while vertices on the\nother side arrive sequentially; when a vertex arrives its edges are revealed\nand it must be immediately matched or dropped. We begin by analyzing the\n\\textsc{oblivious} algorithm, which tries to match each arriving vertex to a\nrandom neighbor, even if the neighbor has already been matched. The algorithm\nis shown to have a performance ratio of at least $1-1/e$ for all monotonic\nfunctions $p(n)$, where the performance ratio is defined asymptotically as the\nratio of the expected matching size given by the algorithm to the expected\nmaximum matching size. Next we show that the conventional \\textsc{greedy}\nalgorithm, which assigns each vertex to a random unmatched neighbor, has a\nperformance ratio of at least 0.837 for all monotonic functions $p(n)$. Under\nthe $G(n,n,p)$ model, the performance of \\textsc{greedy} is equivalent to the\nperformance of the well known \\textsc{ranking} algorithm, so our results show\nthat \\textsc{ranking} has a performance ratio of at least 0.837. We finally\nconsider vertex-weighted bipartite matching. Our proofs are based on simple\ndifferential equations that describe the evolution of the matching process.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1305.5826,https://arxiv.org/abs/1305.5826,"b'Abstract:  Gaussian processes (GP) are Bayesian non-parametric models that are widely\nused for probabilistic regression. Unfortunately, it cannot scale well with\nlarge data nor perform real-time predictions due to its cubic time cost in the\ndata size. This paper presents two parallel GP regression methods that exploit\nlow-rank covariance matrix approximations for distributing the computational\nload among parallel machines to achieve time efficiency and scalability. We\ntheoretically guarantee the predictive performances of our proposed parallel\nGPs to be equivalent to that of some centralized approximate GP regression\nmethods: The computation of their centralized counterparts can be distributed\namong parallel machines, hence achieving greater time efficiency and\nscalability. We analytically compare the properties of our parallel GPs such as\ntime, space, and communication complexity. Empirical evaluation on two\nreal-world datasets in a cluster of 20 computing nodes shows that our parallel\nGPs are significantly more time-efficient and scalable than their centralized\ncounterparts and exact/full GP while achieving predictive performances\ncomparable to full GP.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1304.2488,https://arxiv.org/abs/1304.2488,b'Abstract:  We present bounds of quadratic form for the logarithm of the Gaussian\nQ-function. We also show an analytical method for deriving log-quadratic\napproximations of the Q-function and give an approximation with absolute error\nless than $10^{-3}$.'
Patrick Jaillet,Jaillet_Patrick,arXiv:1301.4529,https://arxiv.org/abs/1301.4529,"b'Abstract:  Rollout algorithms have demonstrated excellent performance on a variety of\ndynamic and discrete optimization problems. Interpreted as an approximate\ndynamic programming algorithm, a rollout algorithm estimates the value-to-go at\neach decision stage by simulating future events while following a greedy\npolicy, referred to as the base policy. While in many cases rollout algorithms\nare guaranteed to perform as well as their base policies, there have been few\ntheoretical results showing additional improvement in performance. In this\npaper we perform a probabilistic analysis of the subset sum problem and\nknapsack problem, giving theoretical evidence that rollout algorithms perform\nstrictly better than their base policies. Using a stochastic model from the\nexisting literature, we analyze two rollout methods that we refer to as the\nconsecutive rollout and exhaustive rollout, both of which employ a simple\ngreedy base policy. For the subset sum problem, we prove that after only a\nsingle iteration of the rollout algorithm, both methods yield at least a 30%\nreduction in the expected gap between the solution value and capacity, relative\nto the base policy. Analogous results are shown for the knapsack problem.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1301.3509,https://arxiv.org/abs/1301.3509,"b'Abstract:  Current kidney exchange pools are of moderate size and thin, as they consist\nof many highly sensitized patients. Creating a thicker pool can be done by\nwaiting for many pairs to arrive. We analyze a simple class of matching\nalgorithms that search periodically for allocations. We find that if only 2-way\ncycles are conducted, in order to gain a significant amount of matches over the\nonline scenario (matching each time a new incompatible pair joins the pool) the\nwaiting period should be ""very long"". If 3-way cycles are also allowed we find\nregimes in which waiting for a short period also increases the number of\nmatches considerably. Finally, a significant increase of matches can be\nobtained by using even one non-simultaneous chain while still matching in an\nonline fashion. Our theoretical findings and data-driven computational\nexperiments lead to policy recommendations.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1208.2596,https://arxiv.org/abs/1208.2596,"b'Abstract:  In this paper, we study a general online linear programming problem whose\nformulation encompasses many practical dynamic resource allocation problems,\nincluding internet advertising display applications, revenue management,\nvarious routing, packing, and auction problems. We propose a model, which under\nmild assumptions, allows us to design near-optimal learning-based online\nalgorithms that do not require the a priori knowledge about the total number of\nonline requests to come, a first of its kind. We then consider two variants of\nthe problem that relax the initial assumptions imposed on the proposed model.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1207.1333,https://arxiv.org/abs/1207.1333,"b'Abstract:  The most well-known conjecture in the context of matroid secretary problems\nclaims the existence of a constant-factor approximation applicable to any\nmatroid. Whereas this conjecture remains open, modified forms of it were shown\nto be true, when assuming that the assignment of weights to the secretaries is\nnot adversarial but uniformly random (Soto [SODA 2011], Oveis Gharan and\nVondr\xc3\xa1k [ESA 2011]). However, so far, there was no variant of the matroid\nsecretary problem with adversarial weight assignment for which a\nconstant-factor approximation was found. We address this point by presenting a\n9-approximation for the \\emph{free order model}, a model suggested shortly\nafter the introduction of the matroid secretary problem, and for which no\nconstant-factor approximation was known so far. The free order model is a\nrelaxed version of the original matroid secretary problem, with the only\ndifference that one can choose the order in which secretaries are interviewed.\nFurthermore, we consider the classical matroid secretary problem for the\nspecial case of laminar matroids. Only recently, a constant-factor\napproximation has been found for this case, using a clever but rather involved\nmethod and analysis (Im and Wang, [SODA 2011]) that leads to a\n16000/3-approximation. This is arguably the most involved special case of the\nmatroid secretary problem for which a constant-factor approximation is known.\nWe present a considerably simpler and stronger $3\\sqrt{3}e\\approx\n14.12$-approximation, based on reducing the problem to a matroid secretary\nproblem on a partition matroid.'"
Patrick Jaillet,Jaillet_Patrick,arXiv:1206.6230,https://arxiv.org/abs/1206.6230,"b'Abstract:  The problem of modeling and predicting spatiotemporal traffic phenomena over\nan urban road network is important to many traffic applications such as\ndetecting and forecasting congestion hotspots. This paper presents a\ndecentralized data fusion and active sensing (D2FAS) algorithm for mobile\nsensors to actively explore the road network to gather and assimilate the most\ninformative data for predicting the traffic phenomenon. We analyze the time and\ncommunication complexity of D2FAS and demonstrate that it can scale well with a\nlarge number of observations and sensors. We provide a theoretical guarantee on\nits predictive performance to be equivalent to that of a sophisticated\ncentralized sparse approximation for the Gaussian process (GP) model: The\ncomputation of such a sparse approximate GP model can thus be parallelized and\ndistributed among the mobile sensors (in a Google-like MapReduce paradigm),\nthereby achieving efficient and scalable prediction. We also theoretically\nguarantee its active sensing performance that improves under various practical\nenvironmental conditions. Empirical evaluation on real-world urban road network\ndata shows that our D2FAS algorithm is significantly more time-efficient and\nscalable than state-of-the-art centralized algorithms while achieving\ncomparable predictive performance.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1901.00032,https://arxiv.org/abs/1901.00032,"b""Abstract:  Leveraging new data sources is a key step in accelerating the pace of\nmaterials design and discovery. To complement the strides in synthesis planning\ndriven by historical, experimental, and computed data, we present an automated\nmethod for connecting scientific literature to synthesis insights. Starting\nfrom natural language text, we apply word embeddings from language models,\nwhich are fed into a named entity recognition model, upon which a conditional\nvariational autoencoder is trained to generate syntheses for arbitrary\nmaterials. We show the potential of this technique by predicting precursors for\ntwo perovskite materials, using only training data published over a decade\nprior to their first reported syntheses. We demonstrate that the model learns\nrepresentations of materials corresponding to synthesis-related properties, and\nthat the model's behavior complements existing thermodynamic knowledge.\nFinally, we apply the model to perform synthesizability screening for proposed\nnovel perovskite compounds."""
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1810.10775,https://arxiv.org/abs/1810.10775,"b'Abstract:  In this paper, we consider the problem of Gaussian process (GP) optimization\nwith an added robustness requirement: The returned point may be perturbed by an\nadversary, and we require the function value to remain as high as possible even\nafter this perturbation. This problem is motivated by settings in which the\nunderlying functions during optimization and implementation stages are\ndifferent, or when one is interested in finding an entire region of good inputs\nrather than only a single point. We show that standard GP optimization\nalgorithms do not exhibit the desired robustness properties, and provide a\nnovel confidence-bound based algorithm StableOpt for this purpose. We\nrigorously establish the required number of samples for StableOpt to find a\nnear-optimal point, and we complement this guarantee with an\nalgorithm-independent lower bound. We experimentally demonstrate several\npotential applications of interest using real-world data sets, and we show that\nStableOpt consistently succeeds in finding a stable maximizer where several\nbaseline methods fail.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1810.00826,https://arxiv.org/abs/1810.00826,"b'Abstract:  Graph Neural Networks (GNNs) are an effective framework for representation\nlearning of graphs. GNNs follow a neighborhood aggregation scheme, where the\nrepresentation vector of a node is computed by recursively aggregating and\ntransforming representation vectors of its neighboring nodes. Many GNN variants\nhave been proposed and have achieved state-of-the-art results on both node and\ngraph classification tasks. However, despite GNNs revolutionizing graph\nrepresentation learning, there is limited understanding of their\nrepresentational properties and limitations. Here, we present a theoretical\nframework for analyzing the expressive power of GNNs to capture different graph\nstructures. Our results characterize the discriminative power of popular GNN\nvariants, such as Graph Convolutional Networks and GraphSAGE, and show that\nthey cannot learn to distinguish certain simple graph structures. We then\ndevelop a simple architecture that is provably the most expressive among the\nclass of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism\ntest. We empirically validate our theoretical findings on a number of graph\nclassification benchmarks, and demonstrate that our model achieves\nstate-of-the-art performance.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1807.01808,https://arxiv.org/abs/1807.01808,"b'Abstract:  We consider the problem of inference in discrete probabilistic models, that\nis, distributions over subsets of a finite ground set. These encompass a range\nof well-known models in machine learning, such as determinantal point processes\nand Ising models. Locally-moving Markov chain Monte Carlo algorithms, such as\nthe Gibbs sampler, are commonly used for inference in such models, but their\nconvergence is, at times, prohibitively slow. This is often caused by\nstate-space bottlenecks that greatly hinder the movement of such samplers. We\npropose a novel sampling strategy that uses a specific mixture of product\ndistributions to propose global moves and, thus, accelerate convergence.\nFurthermore, we show how to construct such a mixture using semigradient\ninformation. We illustrate the effectiveness of combining our sampler with\nexisting ones, both theoretically on an example model, as well as practically\non three models learned from real-world data sets.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1806.10909,https://arxiv.org/abs/1806.10909,"b'Abstract:  We demonstrate that a very deep ResNet with stacked modules with one neuron\nper hidden layer and ReLU activation functions can uniformly approximate any\nLebesgue integrable function in $d$ dimensions, i.e. $\\ell_1(\\mathbb{R}^d)$.\nBecause of the identity mapping inherent to ResNets, our network has\nalternating layers of dimension one and $d$. This stands in sharp contrast to\nfully connected networks, which are not universal approximators if their width\nis the input dimension $d$ [Lu et al, 2017; Hanin and Sellke, 2017]. Hence, our\nresult implies an increase in representational power for narrow deep networks\nby the ResNet architecture.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1806.09277,https://arxiv.org/abs/1806.09277,"b'Abstract:  Many problems in machine learning involve calculating correspondences between\nsets of objects, such as point clouds or images. Discrete optimal transport\nprovides a natural and successful approach to such tasks whenever the two sets\nof objects can be represented in the same space, or at least distances between\nthem can be directly evaluated. Unfortunately neither requirement is likely to\nhold when object representations are learned from data. Indeed, automatically\nderived representations such as word embeddings are typically fixed only up to\nsome global transformations, for example, reflection or rotation. As a result,\npairwise distances across two such instances are ill-defined without specifying\ntheir relative transformation. In this work, we propose a general framework for\noptimal transport in the presence of latent global transformations. We cast the\nproblem as a joint optimization over transport couplings and transformations\nchosen from a flexible class of invariances, propose algorithms to solve it,\nand show promising results in various tasks, including a popular unsupervised\nword translation benchmark.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1806.03536,https://arxiv.org/abs/1806.03536,"b'Abstract:  Recent deep learning approaches for representation learning on graphs follow\na neighborhood aggregation procedure. We analyze some important properties of\nthese models, and propose a strategy to overcome those. In particular, the\nrange of ""neighboring"" nodes that a node\'s representation draws from strongly\ndepends on the graph structure, analogous to the spread of a random walk. To\nadapt to local neighborhood properties and tasks, we explore an architecture --\njumping knowledge (JK) networks -- that flexibly leverages, for each node,\ndifferent neighborhood ranges to enable better structure-aware representation.\nIn a number of experiments on social, bioinformatics and citation networks, we\ndemonstrate that our model achieves state-of-the-art performance. Furthermore,\ncombining the JK framework with models like Graph Convolutional Networks,\nGraphSAGE and Graph Attention Networks consistently improves those models\'\nperformance.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1802.09700,https://arxiv.org/abs/1802.09700,"b'Abstract:  Robustness of deep learning models is a property that has recently gained\nincreasing attention. We formally define a notion of robustness for generative\nadversarial models, and show that, perhaps surprisingly, the GAN in its\noriginal form is not robust. Indeed, the discriminator in GANs may be viewed as\nmerely offering ""teaching feedback"". Our notion of robustness relies on a\ndishonest discriminator, or noisy, adversarial interference with its feedback.\nWe explore, theoretically and empirically, the effect of model and training\nproperties on this robustness. In particular, we show theoretical conditions\nfor robustness that are supported by empirical evidence. We also test the\neffect of regularization. Our results suggest variations of GANs that are\nindeed more robust to noisy attacks and have more stable training behavior,\nrequiring less regularization in general. Inspired by our theoretical results,\nwe further extend our framework to obtain a class of models related to WGAN,\nwith good empirical performance. Overall, this work introduces a novel\nperspective on designing GAN models from the viewpoint of robustness.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1802.05249,https://arxiv.org/abs/1802.05249,"b'Abstract:  Submodular functions have applications throughout machine learning, but in\nmany settings, we do not have direct access to the underlying function $f$. We\nfocus on stochastic functions that are given as an expectation of functions\nover a distribution $P$. In practice, we often have only a limited set of\nsamples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by\nmaximizing the sum of $f_i$. However, this ignores generalization to the true\n(unknown) distribution. In this paper, we achieve better performance on the\nactual underlying function $f$ by directly optimizing a combination of bias and\nvariance. Algorithmically, we accomplish this by showing how to carry out\ndistributionally robust optimization (DRO) for submodular functions, providing\nefficient algorithms backed by theoretical guarantees which leverage several\nnovel contributions to the general theory of DRO. We also show compelling\nempirical evidence that DRO improves generalization to the unknown stochastic\nsubmodular function.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1712.06199,https://arxiv.org/abs/1712.06199,"b'Abstract:  Optimal Transport has recently gained interest in machine learning for\napplications ranging from domain adaptation, sentence similarities to deep\nlearning. Yet, its ability to capture frequently occurring structure beyond the\n""ground metric"" is limited. In this work, we develop a nonlinear generalization\nof (discrete) optimal transport that is able to reflect much additional\nstructure. We demonstrate how to leverage the geometry of this new model for\nfast algorithms, and explore connections and properties. Illustrative\nexperiments highlight the benefit of the induced structured couplings for tasks\nin domain adaptation and natural language processing.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1712.05510,https://arxiv.org/abs/1712.05510,"b'Abstract:  We introduce Graph-Sparse Logistic Regression, a new algorithm for\nclassification for the case in which the support should be sparse but connected\non a graph. We val- idate this algorithm against synthetic data and benchmark\nit against L1-regularized Logistic Regression. We then explore our technique in\nthe bioinformatics context of proteomics data on the interactome graph. We make\nall our experimental code public and provide GSLR as an open source package.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1706.09549,https://arxiv.org/abs/1706.09549,"b'Abstract:  We propose a framework for adversarial training that relies on a sample\nrather than a single sample point as the fundamental unit of discrimination.\nInspired by discrepancy measures and two-sample tests between probability\ndistributions, we propose two such distributional adversaries that operate and\npredict on samples, and show how they can be easily implemented on top of\nexisting models. Various experimental results show that generators trained with\nour distributional adversaries are much more stable and are remarkably less\nprone to mode collapse than traditional models trained with pointwise\nprediction discriminators. The application of our framework to domain\nadaptation also results in considerable improvement over recent\nstate-of-the-art.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1706.03583,https://arxiv.org/abs/1706.03583,"b'Abstract:  The need for real time analysis of rapidly producing data streams (e.g.,\nvideo and image streams) motivated the design of streaming algorithms that can\nefficiently extract and summarize useful information from massive data ""on the\nfly"". Such problems can often be reduced to maximizing a submodular set\nfunction subject to various constraints. While efficient streaming methods have\nbeen recently developed for monotone submodular maximization, in a wide range\nof applications, such as video summarization, the underlying utility function\nis non-monotone, and there are often various constraints imposed on the\noptimization problem to consider privacy or personalization. We develop the\nfirst efficient single pass streaming algorithm, Streaming Local Search, that\nfor any streaming monotone submodular maximization algorithm with approximation\nguarantee $\\alpha$ under a collection of independence systems ${\\cal I}$,\nprovides a constant $1/\\big(1+2/\\sqrt{\\alpha}+1/\\alpha\n+2d(1+\\sqrt{\\alpha})\\big)$ approximation guarantee for maximizing a\nnon-monotone submodular function under the intersection of ${\\cal I}$ and $d$\nknapsack constraints. Our experiments show that for video summarization, our\nmethod runs more than 1700 times faster than previous work, while maintaining\npractically the same performance.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1706.01445,https://arxiv.org/abs/1706.01445,"b'Abstract:  Bayesian optimization (BO) has become an effective approach for black-box\nfunction optimization problems when function evaluations are expensive and the\noptimum can be achieved within a relatively small number of queries. However,\nmany cases, such as the ones with high-dimensional inputs, may require a much\nlarger number of observations for optimization. Despite an abundance of\nobservations thanks to parallel experiments, current BO techniques have been\nlimited to merely a few thousand observations. In this paper, we propose\nensemble Bayesian optimization (EBO) to address three current challenges in BO\nsimultaneously: (1) large-scale observations; (2) high dimensional input\nspaces; and (3) selections of batch queries that balance quality and diversity.\nThe key idea of EBO is to operate on an ensemble of additive Gaussian process\nmodels, each of which possesses a randomized strategy to divide and conquer. We\nshow unprecedented, previously impossible results of scaling up BO to tens of\nthousands of observations within minutes of computation.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1705.07443,https://arxiv.org/abs/1705.07443,"b'Abstract:  Efficiently aggregating data from different sources is a challenging problem,\nparticularly when samples from each source are distributed differently. These\ndifferences can be inherent to the inference task or present for other reasons:\nsensors in a sensor network may be placed far apart, affecting their individual\nmeasurements. Conversely, it is computationally advantageous to split Bayesian\ninference tasks across subsets of data, but data need not be identically\ndistributed across subsets. One principled way to fuse probability\ndistributions is via the lens of optimal transport: the Wasserstein barycenter\nis a single distribution that summarizes a collection of input measures while\nrespecting their geometry. However, computing the barycenter scales poorly and\nrequires discretization of all input distributions and the barycenter itself.\nImproving on this situation, we present a scalable, communication-efficient,\nparallel algorithm for computing the Wasserstein barycenter of arbitrary\ndistributions. Our algorithm can operate directly on continuous input\ndistributions and is optimized for streaming data. Our method is even robust to\nnonstationary input distributions and produces a barycenter estimate that\ntracks the input measures over time. The algorithm is semi-discrete, needing to\ndiscretize only the barycenter estimate. To the best of our knowledge, we also\nprovide the first bounds on the quality of the approximate barycenter as the\ndiscretization becomes finer. Finally, we demonstrate the practical\neffectiveness of our method, both in tracking moving distributions on a sphere,\nas well as in a large-scale Bayesian inference task.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1705.06616,https://arxiv.org/abs/1705.06616,"b'Abstract:  We consider the problem of far-field sensing by means of a sensor array.\nTraditional array geometry design techniques are agnostic to prior information\nabout the far-field scene. However, in many applications such priors are\navailable and may be utilized to design more efficient array topologies. We\nformulate the problem of array geometry design with scene prior as one of\nfinding a sampling configuration that enables efficient inference, which turns\nout to be a combinatorial optimization problem. While generic combinatorial\noptimization problems are NP-hard and resist efficient solvers, we show how for\narray design problems the theory of submodular optimization may be utilized to\nobtain efficient algorithms that are guaranteed to achieve solutions within a\nconstant approximation factor from the optimum. We leverage the connection\nbetween array design problems and submodular optimization and port several\nresults of interest. We demonstrate efficient methods for designing arrays with\nconstraints on the sensing aperture, as well as arrays respecting combinatorial\nplacement constraints. This novel connection between array design and\nsubmodularity suggests the possibility for utilizing other insights and\ntechniques from the growing body of literature on submodular optimization in\nthe field of array design.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1703.02674,https://arxiv.org/abs/1703.02674,"b'Abstract:  We study dual volume sampling, a method for selecting k columns from an n x m\nshort and wide matrix (n <= k <= m) such that the probability of selection is\nproportional to the volume spanned by the rows of the induced submatrix. This\nmethod was proposed by Avron and Boutsidis (2013), who showed it to be a\npromising method for column subset selection and its multiple applications.\nHowever, its wider adoption has been hampered by the lack of polynomial time\nsampling algorithms. We remove this hindrance by developing an exact\n(randomized) polynomial time sampling algorithm as well as its derandomization.\nThereafter, we study dual volume sampling via the theory of real stable\npolynomials and prove that its distribution satisfies the ""Strong Rayleigh""\nproperty. This result has numerous consequences, including a provably\nfast-mixing Markov chain sampler that makes dual volume sampling much more\nattractive to practitioners. This sampler is closely related to classical\nalgorithms for popular experimental design methods that are to date lacking\ntheoretical analysis but are known to empirically work well.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1703.01973,https://arxiv.org/abs/1703.01973,"b'Abstract:  Optimization of high-dimensional black-box functions is an extremely\nchallenging problem. While Bayesian optimization has emerged as a popular\napproach for optimizing black-box functions, its applicability has been limited\nto low-dimensional problems due to its computational and statistical challenges\narising from high-dimensional settings. In this paper, we propose to tackle\nthese challenges by (1) assuming a latent additive structure in the function\nand inferring it properly for more efficient and effective BO, and (2)\nperforming multiple evaluations in parallel to reduce the number of iterations\nrequired by the method. Our novel approach learns the latent structure with\nGibbs sampling and constructs batched queries using determinantal point\nprocesses. Experimental validations on both synthetic and real-world functions\ndemonstrate that the proposed method outperforms the existing state-of-the-art\napproaches.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1703.01968,https://arxiv.org/abs/1703.01968,"b'Abstract:  Entropy Search (ES) and Predictive Entropy Search (PES) are popular and\nempirically successful Bayesian Optimization techniques. Both rely on a\ncompelling information-theoretic motivation, and maximize the information\ngained about the $\\arg\\max$ of the unknown function; yet, both are plagued by\nthe expensive computation for estimating entropies. We propose a new criterion,\nMax-value Entropy Search (MES), that instead uses the information about the\nmaximum function value. We show relations of MES to other Bayesian optimization\nmethods, and establish a regret bound. We observe that MES maintains or\nimproves the good empirical performance of ES/PES, while tremendously\nlightening the computational burden. In particular, MES is much more robust to\nthe number of samples used for computing the entropy, and hence more efficient\nfor higher dimensional problems.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1702.08791,https://arxiv.org/abs/1702.08791,"b'Abstract:  The optimal allocation of resources for maximizing influence, spread of\ninformation or coverage, has gained attention in the past years, in particular\nin machine learning and data mining. But in applications, the parameters of the\nproblem are rarely known exactly, and using wrong parameters can lead to\nundesirable outcomes. We hence revisit a continuous version of the Budget\nAllocation or Bipartite Influence Maximization problem introduced by Alon et\nal. (2012) from a robust optimization perspective, where an adversary may\nchoose the least favorable parameters within a confidence set. The resulting\nproblem is a nonconvex-concave saddle point problem (or game). We show that\nthis nonconvex problem can be solved exactly by leveraging connections to\ncontinuous submodular functions, and by solving a constrained submodular\nminimization problem. Although constrained submodular minimization is hard in\ngeneral, here, we establish conditions under which such a problem can be solved\nto arbitrary precision $\\epsilon$.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1612.01213,https://arxiv.org/abs/1612.01213,"b'Abstract:  Learning the representation and the similarity metric in an end-to-end\nfashion with deep networks have demonstrated outstanding results for clustering\nand retrieval. However, these recent approaches still suffer from the\nperformance degradation stemming from the local metric training procedure which\nis unaware of the global structure of the embedding space.\nWe propose a global metric learning scheme for optimizing the deep metric\nembedding with the learnable clustering function and the clustering metric\n(NMI) in a novel structured prediction framework.\nOur experiments on CUB200-2011, Cars196, and Stanford online products\ndatasets show state of the art performance both on the clustering and retrieval\ntasks measured in the NMI and Recall@K evaluation metrics.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1608.01008,https://arxiv.org/abs/1608.01008,"b'Abstract:  We study probability measures induced by set functions with constraints. Such\nmeasures arise in a variety of real-world settings, where prior knowledge,\nresource limitations, or other pragmatic considerations impose constraints. We\nconsider the task of rapidly sampling from such constrained measures, and\ndevelop fast Markov chain samplers for them. Our first main result is for MCMC\nsampling from Strongly Rayleigh (SR) measures, for which we present sharp\npolynomial bounds on the mixing time. As a corollary, this result yields a fast\nmixing sampler for Determinantal Point Processes (DPPs), yielding (to our\nknowledge) the first provably fast MCMC sampler for DPPs since their inception\nover four decades ago. Beyond SR measures, we develop MCMC samplers for\nprobabilistic models with hard constraints and identify sufficient conditions\nunder which their chains mix rapidly. We illustrate our claims by empirically\nverifying the dependence of mixing times on the key factors governing our\ntheoretical bounds.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1607.07762,https://arxiv.org/abs/1607.07762,"b'Abstract:  We introduce a framework for model learning and planning in stochastic\ndomains with continuous state and action spaces and non-Gaussian transition\nmodels. It is efficient because (1) local models are estimated only when the\nplanner requires them; (2) the planner focuses on the most relevant states to\nthe current planning problem; and (3) the planner focuses on the most\ninformative and/or high-value actions. Our theoretical analysis shows the\nvalidity and asymptotic optimality of the proposed approach. Empirically, we\ndemonstrate the effectiveness of our algorithm on a simulated multi-modal\npushing problem.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1607.03559,https://arxiv.org/abs/1607.03559,"b'Abstract:  In this note we consider sampling from (non-homogeneous) strongly Rayleigh\nprobability measures. As an important corollary, we obtain a fast mixing Markov\nChain sampler for Determinantal Point Processes.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1603.06052,https://arxiv.org/abs/1603.06052,"b'Abstract:  The Nystr\xc3\xb6m method has long been popular for scaling up kernel methods. Its\ntheoretical guarantees and empirical performance rely critically on the quality\nof the landmarks selected. We study landmark selection for Nystr\xc3\xb6m using\nDeterminantal Point Processes (DPPs), discrete probability models that allow\ntractable generation of diverse samples. We prove that landmarks selected via\nDPPs guarantee bounds on approximation errors; subsequently, we analyze\nimplications for kernel ridge regression. Contrary to prior reservations due to\ncubic complexity of DPPsampling, we show that (under certain conditions) Markov\nchain DPP sampling requires only linear time in the size of the data. We\npresent several empirical results that support our theoretical analysis, and\ndemonstrate the superior performance of DPP-based landmark selection compared\nwith existing approaches.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1512.01904,https://arxiv.org/abs/1512.01904,"b'Abstract:  We present a framework for accelerating a spectrum of machine learning\nalgorithms that require computation of bilinear inverse forms $u^\\top A^{-1}u$,\nwhere $A$ is a positive definite matrix and $u$ a given vector. Our framework\nis built on Gauss-type quadrature and easily scales to large, sparse matrices.\nFurther, it allows retrospective computation of lower and upper bounds on\n$u^\\top A^{-1}u$, which in turn accelerates several algorithms. We prove that\nthese bounds tighten iteratively and converge at a linear (geometric) rate. To\nour knowledge, ours is the first work to demonstrate these key properties of\nGauss-type quadrature, which is a classical and deeply studied topic. We\nillustrate empirical consequences of our results by using quadrature to\naccelerate machine learning tasks involving determinantal point processes and\nsubmodular optimization, and observe tremendous speedups in several instances.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1511.07069,https://arxiv.org/abs/1511.07069,"b'Abstract:  Precisely-labeled data sets with sufficient amount of samples are very\nimportant for training deep convolutional neural networks (CNNs). However, many\nof the available real-world data sets contain erroneously labeled samples and\nthose errors substantially hinder the learning of very accurate CNN models. In\nthis work, we consider the problem of training a deep CNN model for image\nclassification with mislabeled training samples - an issue that is common in\nreal image data sets with tags supplied by amateur users. To solve this\nproblem, we propose an auxiliary image regularization technique, optimized by\nthe stochastic Alternating Direction Method of Multipliers (ADMM) algorithm,\nthat automatically exploits the mutual context information among training\nimages and encourages the model to select reliable images to robustify the\nlearning process. Comprehensive experiments on benchmark data sets clearly\ndemonstrate our proposed regularized CNN model is resistant to label noise in\ntraining data.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1511.06452,https://arxiv.org/abs/1511.06452,"b'Abstract:  Learning the distance metric between pairs of examples is of great importance\nfor learning and visual recognition. With the remarkable success from the state\nof the art convolutional neural networks, recent works have shown promising\nresults on discriminatively training the networks to learn semantic feature\nembeddings where similar examples are mapped close to each other and dissimilar\nexamples are mapped farther apart. In this paper, we describe an algorithm for\ntaking full advantage of the training batches in the neural network training by\nlifting the vector of pairwise distances within the batch to the matrix of\npairwise distances. This step enables the algorithm to learn the state of the\nart feature embedding by optimizing a novel structured prediction objective on\nthe lifted problem. Additionally, we collected Online Products dataset: 120k\nimages of 23k classes of online products for metric learning. Our experiments\non the CUB-200-2011, CARS196, and Online Products datasets demonstrate\nsignificant improvement over existing deep feature embedding methods on all\nexperimented embedding sizes with the GoogLeNet network.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1510.06423,https://arxiv.org/abs/1510.06423,"b'Abstract:  Recently, there has been rising interest in Bayesian optimization -- the\noptimization of an unknown function with assumptions usually expressed by a\nGaussian Process (GP) prior. We study an optimization strategy that directly\nuses an estimate of the argmax of the function. This strategy offers both\npractical and theoretical advantages: no tradeoff parameter needs to be\nselected, and, moreover, we establish close connections to the popular GP-UCB\nand GP-PI strategies. Our approach can be understood as automatically and\nadaptively trading off exploration and exploitation in GP-UCB and GP-PI. We\nillustrate the effects of this adaptive tuning via bounds on the regret as well\nas an extensive empirical evaluation on robotics and vision tasks,\ndemonstrating the robustness of this strategy for a range of performance\ncriteria.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1509.01618,https://arxiv.org/abs/1509.01618,"b'Abstract:  Determinantal Point Processes (DPPs) are elegant probabilistic models of\nrepulsion and diversity over discrete sets of items. But their applicability to\nlarge sets is hindered by expensive cubic-complexity matrix operations for\nbasic tasks such as sampling. In light of this, we propose a new method for\napproximate sampling from discrete $k$-DPPs. Our method takes advantage of the\ndiversity property of subsets sampled from a DPP, and proceeds in two stages:\nfirst it constructs coresets for the ground set of items; thereafter, it\nefficiently samples subsets based on the constructed coresets. As opposed to\nprevious approaches, our algorithm aims to minimize the total variation\ndistance to the original distribution. Experiments on both synthetic and real\ndatasets indicate that our sampling algorithm works efficiently on large data\nsets, and yields more accurate samples than previous approaches.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1503.01563,https://arxiv.org/abs/1503.01563,"b'Abstract:  Energy minimization has been an intensely studied core problem in computer\nvision. With growing image sizes (2D and 3D), it is now highly desirable to run\nenergy minimization algorithms in parallel. But many existing algorithms, in\nparticular, some efficient combinatorial algorithms, are difficult to\npar-allelize. By exploiting results from convex and submodular theory, we\nreformulate the quadratic energy minimization problem as a total variation\ndenoising problem, which, when viewed geometrically, enables the use of\nprojection and reflection based convex methods. The resulting min-cut algorithm\n(and code) is conceptually very simple, and solves a sequence of TV denoising\nproblems. We perform an extensive empirical evaluation comparing\nstate-of-the-art combinatorial algorithms and convex optimization techniques.\nOn small problems the iterative convex methods match the combinatorial max-flow\nalgorithms, while on larger problems they offer other flexibility and important\ngains: (a) their memory footprint is small; (b) their straightforward\nparallelizability fits multi-core platforms; (c) they can easily be\nwarm-started; and (d) they quickly reach approximately good solutions, thereby\nenabling faster ""inexact"" solutions. A key consequence of our approach based on\nsubmodularity and convexity is that it is allows to combine any arbitrary\ncombinatorial or convex methods as subroutines, which allows one to obtain\nhybrid combinatorial and convex optimization algorithms that benefit from the\nstrengths of both.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1501.05973,https://arxiv.org/abs/1501.05973,"b'Abstract:  We introduce and study methods for inferring and learning from\ncorrespondences among neurons. The approach enables alignment of data from\ndistinct multiunit studies of nervous systems. We show that the methods for\ninferring correspondences combine data effectively from cross-animal studies to\nmake joint inferences about behavioral decision making that are not possible\nwith the data from a single animal. We focus on data collection, machine\nlearning, and prediction in the representative and long-studied invertebrate\nnervous system of the European medicinal leech. Acknowledging the computational\nintractability of the general problem of identifying correspondences among\nneurons, we introduce efficient computational procedures for matching neurons\nacross animals. The methods include techniques that adjust for missing cells or\nadditional cells in the different data sets that may reflect biological or\nexperimental variation. The methods highlight the value harnessing inference\nand learning in new kinds of computational microscopes for multiunit\nneurobiological studies.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1411.1752,https://arxiv.org/abs/1411.1752,"b'Abstract:  To cope with the high level of ambiguity faced in domains such as Computer\nVision or Natural Language processing, robust prediction methods often search\nfor a diverse set of high-quality candidate solutions or proposals. In\nstructured prediction problems, this becomes a daunting task, as the solution\nspace (image labelings, sentence parses, etc.) is exponentially large. We study\ngreedy algorithms for finding a diverse subset of solutions in\nstructured-output spaces by drawing new connections between submodular\nfunctions over combinatorial item sets and High-Order Potentials (HOPs) studied\nfor graphical models. Specifically, we show via examples that when marginal\ngains of submodular diversity functions allow structured representations, this\nenables efficient (sub-linear time) approximate maximization by reducing the\ngreedy augmentation step to inference in a factor graph with appropriately\nconstructed HOPs. We discuss benefits, tradeoffs, and show that our\nconstructions lead to significantly better proposals.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1406.6507,https://arxiv.org/abs/1406.6507,"b'Abstract:  The increasing prominence of weakly labeled data nurtures a growing demand\nfor object detection methods that can cope with minimal supervision. We propose\nan approach that automatically identifies discriminative configurations of\nvisual patterns that are characteristic of a given object class. We formulate\nthe problem as a constrained submodular optimization problem and demonstrate\nthe benefits of the discovered configurations in remedying mislocalizations and\nfinding informative positive and negative training examples. Together, these\nlead to state-of-the-art weakly-supervised detection results on the challenging\nPASCAL VOC dataset.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1406.6474,https://arxiv.org/abs/1406.6474,"b'Abstract:  Submodular functions describe a variety of discrete problems in machine\nlearning, signal processing, and computer vision. However, minimizing\nsubmodular functions poses a number of algorithmic challenges. Recent work\nintroduced an easy-to-use, parallelizable algorithm for minimizing submodular\nfunctions that decompose as the sum of ""simple"" submodular functions.\nEmpirically, this algorithm performs extremely well, but no theoretical\nanalysis was given. In this paper, we show that the algorithm converges\nlinearly, and we provide upper and lower bounds on the rate of convergence. Our\nproof relies on the geometry of submodular polyhedra and draws on results from\nspectral graph theory.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1403.1024,https://arxiv.org/abs/1403.1024,"b'Abstract:  Learning to localize objects with minimal supervision is an important problem\nin computer vision, since large fully annotated datasets are extremely costly\nto obtain. In this paper, we propose a new method that achieves this goal with\nonly image-level labels of whether the objects are present or not. Our approach\ncombines a discriminative submodular cover problem for automatically\ndiscovering a set of positive object windows with a smoothed latent SVM\nformulation. The latter allows us to leverage efficient quasi-Newton\noptimization techniques. Our experiments demonstrate that the proposed approach\nprovides a 50% relative improvement in mean average precision over the current\nstate-of-the-art on PASCAL VOC 2007 detection.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1402.0240,https://arxiv.org/abs/1402.0240,"b'Abstract:  We study an extension of the classical graph cut problem, wherein we replace\nthe modular (sum of edge weights) cost function by a submodular set function\ndefined over graph edges. Special cases of this problem have appeared in\ndifferent applications in signal processing, machine learning, and computer\nvision. In this paper, we connect these applications via the generic\nformulation of ""cooperative graph cuts"", for which we study complexity,\nalgorithms, and connections to polymatroidal network flows. Finally, we compare\nthe proposed algorithms empirically.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1311.4296,https://arxiv.org/abs/1311.4296,"b'Abstract:  Recently, it has become evident that submodularity naturally captures widely\noccurring concepts in machine learning, signal processing and computer vision.\nConsequently, there is need for efficient optimization procedures for\nsubmodular functions, especially for minimization problems. While general\nsubmodular minimization is challenging, we propose a new method that exploits\nexisting decomposability of submodular functions. In contrast to previous\napproaches, our method is neither approximate, nor impractical, nor does it\nneed any cumbersome parameter tuning. Moreover, it is easy to implement and\nparallelize. A key component of our method is a formulation of the discrete\nsubmodular minimization problem as a continuous best approximation problem that\nis solved through a sequence of reflections, and its solution can be easily\nthresholded to obtain an optimal discrete solution. This method solves both the\ncontinuous and discrete formulations of the problem, and therefore has\napplications in learning, inference, and reconstruction. In our experiments, we\nillustrate the benefits of our method on two image segmentation tasks.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1311.2110,https://arxiv.org/abs/1311.2110,"b""Abstract:  We investigate three related and important problems connected to machine\nlearning: approximating a submodular function everywhere, learning a submodular\nfunction (in a PAC-like setting [53]), and constrained minimization of\nsubmodular functions. We show that the complexity of all three problems depends\non the 'curvature' of the submodular function, and provide lower and upper\nbounds that refine and improve previous results [3, 16, 18, 52]. Our proof\ntechniques are fairly generic. We either use a black-box transformation of the\nfunction (for approximation and learning), or a transformation of algorithms to\nuse an appropriate surrogate function (for minimization). Curiously, curvature\nhas been known to influence approximations for submodular maximization [7, 55],\nbut its effect on minimization, approximation and learning has hitherto been\nopen. We complete this picture, and also support our theoretical claims by\nempirical results."""
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1308.1006,https://arxiv.org/abs/1308.1006,"b'Abstract:  We present a practical and powerful new framework for both unconstrained and\nconstrained submodular function optimization based on discrete\nsemidifferentials (sub- and super-differentials). The resulting algorithms,\nwhich repeatedly compute and then efficiently optimize submodular\nsemigradients, offer new and generalize many old methods for submodular\noptimization. Our approach, moreover, takes steps towards providing a unifying\nparadigm applicable to both submodular min- imization and maximization,\nproblems that historically have been treated quite distinctly. The practicality\nof our algorithms is important since interest in submodularity, owing to its\nnatural and wide applicability, has recently been in ascendance within machine\nlearning. We analyze theoretical properties of our algorithms for minimization\nand maximization, and show that many state-of-the-art maximization algorithms\nare special cases. Lastly, we complement our theoretical analyses with\nsupporting empirical experiments.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1307.8049,https://arxiv.org/abs/1307.8049,"b'Abstract:  Research on distributed machine learning algorithms has focused primarily on\none of two extremes - algorithms that obey strict concurrency constraints or\nalgorithms that obey few or no such constraints. We consider an intermediate\nalternative in which algorithms optimistically assume that conflicts are\nunlikely and if conflicts do arise a conflict-resolution protocol is invoked.\nWe view this ""optimistic concurrency control"" paradigm as particularly\nappropriate for large-scale machine learning algorithms, particularly in the\nunsupervised setting. We demonstrate our approach in three problem areas:\nclustering, feature learning and online facility location. We evaluate our\nmethods via large-scale experiments in a cluster computing environment.'"
Stefanie Jegelka,Jegelka_Stefanie,arXiv:0812.0389,https://arxiv.org/abs/0812.0389,"b'Abstract:  In the past few years powerful generalizations to the Euclidean k-means\nproblem have been made, such as Bregman clustering [7], co-clustering (i.e.,\nsimultaneous clustering of rows and columns of an input matrix) [9,18], and\ntensor clustering [8,34]. Like k-means, these more general problems also suffer\nfrom the NP-hardness of the associated optimization. Researchers have developed\napproximation algorithms of varying degrees of sophistication for k-means,\nk-medians, and more recently also for Bregman clustering [2]. However, there\nseem to be no approximation algorithms for Bregman co- and tensor clustering.\nIn this paper we derive the first (to our knowledge) guaranteed methods for\nthese increasingly important clustering settings. Going beyond Bregman\ndivergences, we also prove an approximation factor for tensor clustering with\narbitrary separable metrics. Through extensive experiments we evaluate the\ncharacteristics of our method, and show that it also has practical impact.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1901.06109,https://arxiv.org/abs/1901.06109,"b'Abstract:  This paper addresses the problem of planning for a robot with a directional\nobstacle-detection sensor that must move through a cluttered environment. The\nplanning objective is to remain safe by finding a path for the complete robot,\nincluding sensor, that guarantees that the robot will not move into any part of\nthe workspace before it has been seen by the sensor. Although a great deal of\nwork has addressed a version of this problem in which the ""field of view"" of\nthe sensor is a sphere around the robot, there is very little work addressing\nrobots with a narrow or occluded field of view. We give a formal definition of\nthe problem, several solution methods with different computational trade-offs,\nand experimental results in illustrative domains.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1901.00279,https://arxiv.org/abs/1901.00279,"b'Abstract:  In this paper, we theoretically prove that we can eliminate all suboptimal\nlocal minima by adding one neuron per output unit to any deep neural network,\nfor multi-class classification, binary classification, and regression with an\narbitrary loss function. At every local minimum of any deep neural network with\nadded neurons, the set of parameters of the original neural network (without\nadded neurons) is guaranteed to be a global minimum of the original neural\nnetwork. The effects of the added neurons are proven to automatically vanish at\nevery local minimum. Unlike many related results in the literature, our\ntheoretical results are directly applicable to common deep learning tasks\nbecause the results only rely on the assumptions that automatically hold in the\ncommon tasks. Moreover, we discuss several limitations in eliminating the\nsuboptimal local minima in this manner by providing additional theoretical\nresults and several examples.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1812.07768,https://arxiv.org/abs/1812.07768,"b""Abstract:  Modular meta-learning is a new framework that generalizes to unseen datasets\nby combining a small set of neural modules in different ways. In this work we\npropose abstract graph networks: using graphs as abstractions of a system's\nsubparts without a fixed assignment of nodes to system subparts, for which we\nwould need supervision. We combine this idea with modular meta-learning to get\na flexible framework with combinatorial generalization to new tasks built in.\nWe then use it to model the pushing of arbitrarily shaped objects from little\nor no training data."""
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1812.06298,https://arxiv.org/abs/1812.06298,"b'Abstract:  We present Residual Policy Learning (RPL): a simple method for improving\nnondifferentiable policies using model-free deep reinforcement learning. RPL\nthrives in complex robotic manipulation tasks where good but imperfect\ncontrollers are available. In these tasks, reinforcement learning from scratch\nremains data-inefficient or intractable, but learning a residual on top of the\ninitial controller can yield substantial improvements. We study RPL in six\nchallenging MuJoCo tasks involving partial observability, sensor noise, model\nmisspecification, and controller miscalibration. For initial controllers, we\nconsider both hand-designed policies and model-predictive controllers with\nknown or learned transition models. By combining learning with control\nalgorithms, RPL can perform long-horizon, sparse-reward tasks for which\nreinforcement learning alone fails. Moreover, we find that RPL consistently and\nsubstantially improves on the initial controllers. We argue that RPL is a\npromising approach for combining the complementary strengths of deep\nreinforcement learning and robotic control, pushing the boundaries of what\neither can achieve independently. Video and code at\nthis https URL.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1811.09558,https://arxiv.org/abs/1811.09558,"b'Abstract:  Bayesian optimization usually assumes that a Bayesian prior is given.\nHowever, the strong theoretical guarantees in Bayesian optimization are often\nregrettably compromised in practice because of unknown parameters in the prior.\nIn this paper, we adopt a variant of empirical Bayes and show that, by\nestimating the Gaussian process prior from offline data sampled from the same\nprior and constructing unbiased estimators of the posterior, variants of both\nGP-UCB and probability of improvement achieve a near-zero regret bound, which\ndecreases to a constant proportional to the observational noise as the number\nof offline data and the number of online evaluations increase. Empirically, we\nhave verified our approach on challenging simulated robotic problems featuring\ntask and motion planning.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1811.08150,https://arxiv.org/abs/1811.08150,"b'Abstract:  In this paper, we analyze the effects of depth and width on the quality of\nlocal minima, without strong over-parameterization and simplification\nassumptions in the literature. Without any simplification assumption, for deep\nnonlinear neural networks with the squared loss, we theoretically show that the\nquality of local minima tends to improve towards the global minimum value as\ndepth and width increase. Furthermore, with a locally-induced structure on deep\nnonlinear neural networks, the values of local minima of neural networks are\ntheoretically proven to be no worse than the globally optimal values of\ncorresponding classical machine learning models. We empirically support our\ntheoretical observation with a synthetic dataset as well as MNIST, CIFAR-10 and\nSVHN datasets. When compared to previous studies with strong\nover-parameterization assumptions, the results in this paper do not require\nover-parameterization, and instead show the gradual effects of\nover-parameterization as consequences of general results.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1810.11177,https://arxiv.org/abs/1810.11177,"b""Abstract:  We present a representation for describing transition models in complex\nuncertain domains using relational rules. For any action, a rule selects a set\nof relevant objects and computes a distribution over properties of just those\nobjects in the resulting state given their properties in the previous state. An\niterative greedy algorithm is used to construct a set of deictic references\nthat determine which objects are relevant in any given state. Feed-forward\nneural networks are used to learn the transition distribution on the relevant\nobjects' properties. This strategy is demonstrated to be both more versatile\nand more sample efficient than learning a monolithic transition model in a\nsimulated domain in which a robot pushes stacks of objects on a cluttered\ntable."""
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1809.07878,https://arxiv.org/abs/1809.07878,"b'Abstract:  Multi-object manipulation problems in continuous state and action spaces can\nbe solved by planners that search over sampled values for the continuous\nparameters of operators. The efficiency of these planners depends critically on\nthe effectiveness of the samplers used, but effective sampling in turn depends\non details of the robot, environment, and task. Our strategy is to learn\nfunctions called ""specializers"" that generate values for continuous operator\nparameters, given a state description and values for the discrete parameters.\nRather than trying to learn a single specializer for each operator from large\namounts of data on a single task, we take a modular meta-learning approach. We\ntrain on multiple tasks and learn a variety of specializers that, on a new\ntask, can be quickly adapted using relatively little data -- thus, our system\n""learns quickly to plan quickly"" using these specializers. We validate our\napproach experimentally in simulated 3D pick-and-place tasks with continuous\nstate and action spaces. Visit this http URL for a\nsupplementary video.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1808.03246,https://arxiv.org/abs/1808.03246,"b'Abstract:  An efficient, generalizable physical simulator with universal uncertainty\nestimates has wide applications in robot state estimation, planning, and\ncontrol. In this paper, we build such a simulator for two scenarios, planar\npushing and ball bouncing, by augmenting an analytical rigid-body simulator\nwith a neural network that learns to model uncertainty as residuals. Combining\nsymbolic, deterministic simulators with learnable, stochastic neural nets\nprovides us with expressiveness, efficiency, and generalizability\nsimultaneously. Our model outperforms both purely analytical and purely learned\nsimulators consistently on real, standard benchmarks. Compared with methods\nthat model uncertainty using Gaussian processes, our model runs much faster,\ngeneralizes better to new object shapes, and is able to characterize the\ncomplex distribution of object trajectories.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1807.09962,https://arxiv.org/abs/1807.09962,"b'Abstract:  In this paper, we propose a learning algorithm that speeds up the search in\ntask and motion planning problems. Our algorithm proposes solutions to three\ndifferent challenges that arise in learning to improve planning efficiency:\nwhat to predict, how to represent a planning problem instance, and how to\ntransfer knowledge from one problem instance to another. We propose a method\nthat predicts constraints on the search space based on a generic representation\nof a planning problem instance, called score-space, where we represent a\nproblem instance in terms of the performance of a set of solutions attempted so\nfar. Using this representation, we transfer knowledge, in the form of\nconstraints, from previous problems based on the similarity in score space. We\ndesign a sequential algorithm that efficiently predicts these constraints, and\nevaluate it in three different challenging task and motion planning problems.\nResults indicate that our approach performs orders of magnitudes faster than an\nunguided planner'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1806.10166,https://arxiv.org/abs/1806.10166,"b'Abstract:  Many prediction problems, such as those that arise in the context of\nrobotics, have a simplifying underlying structure that could accelerate\nlearning. In this paper, we present a strategy for learning a set of neural\nnetwork modules that can be combined in different ways. We train different\nmodular structures on a set of related tasks and generalize to new tasks by\ncomposing the learned modules in new ways. We show this improves performance in\ntwo robotics-related problems.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1805.08263,https://arxiv.org/abs/1805.08263,"b""Abstract:  In many robotic applications, an autonomous agent must act within and explore\na partially observed environment that is unobserved by its human teammate. We\nconsider such a setting in which the agent can, while acting, transmit\ndeclarative information to the human that helps them understand aspects of this\nunseen environment. In this work, we address the algorithmic question of how\nthe agent should plan out what actions to take and what information to\ntransmit. Naturally, one would expect the human to have preferences, which we\nmodel information-theoretically by scoring transmitted information based on the\nchange it induces in weighted entropy of the human's belief state. We formulate\nthis setting as a belief MDP and give a tractable algorithm for solving it\napproximately. Then, we give an algorithm that allows the agent to learn the\nhuman's preferences online, through exploration. We validate our approach\nexperimentally in simulated discrete and continuous partially observed\nsearch-and-recover domains. Visit this http URL for a\nsupplementary video."""
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1805.02874,https://arxiv.org/abs/1805.02874,"b'Abstract:  In many applications that involve processing high-dimensional data, it is\nimportant to identify a small set of entities that account for a significant\nfraction of detections. Rather than formalize this as a clustering problem, in\nwhich all detections must be grouped into hard or soft categories, we formalize\nit as an instance of the frequent items or heavy hitters problem, which finds\ngroups of tightly clustered objects that have a high density in the feature\nspace. We show that the heavy hitters formulation generates solutions that are\nmore accurate and effective than the clustering formulation. In addition, we\npresent a novel online algorithm for heavy hitters, called HAC, which addresses\nproblems in continuous space, and demonstrate its effectiveness on real video\nand household domains.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1803.00967,https://arxiv.org/abs/1803.00967,"b'Abstract:  The objective of this work is to augment the basic abilities of a robot by\nlearning to use new sensorimotor primitives to enable the solution of complex\nlong-horizon problems. Solving long-horizon problems in complex domains\nrequires flexible generative planning that can combine primitive abilities in\nnovel combinations to solve problems as they arise in the world. In order to\nplan to combine primitive actions, we must have models of the preconditions and\neffects of those actions: under what circumstances will executing this\nprimitive achieve some particular effect in the world?\nWe use, and develop novel improvements on, state-of-the-art methods for\nactive learning and sampling. We use Gaussian process methods for learning the\nconditions of operator effectiveness from small numbers of expensive training\nexamples collected by experimentation on a robot. We develop adaptive sampling\nmethods for generating diverse elements of continuous sets (such as robot\nconfigurations and object poses) during planning for solving a new task, so\nthat planning is as efficient as possible. We demonstrate these methods in an\nintegrated system, combining newly learned models with an efficient\ncontinuous-space robot task and motion planner to learn to solve long horizon\nproblems more efficiently than was previously possible.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1803.00119,https://arxiv.org/abs/1803.00119,"b""Abstract:  In partially observed environments, it can be useful for a human to provide\nthe robot with declarative information that represents probabilistic relational\nconstraints on properties of objects in the world, augmenting the robot's\nsensory observations. For instance, a robot tasked with a search-and-rescue\nmission may be informed by the human that two victims are probably in the same\nroom. An important question arises: how should we represent the robot's\ninternal knowledge so that this information is correctly processed and combined\nwith raw sensory information? In this paper, we provide an efficient belief\nstate representation that dynamically selects an appropriate factoring,\ncombining aspects of the belief when they are correlated through information\nand separating them when they are not. This strategy works in open domains, in\nwhich the set of possible objects is not known in advance, and provides\nsignificant improvements in inference time over a static factoring, leading to\nmore efficient planning for complex partially observed tasks. We validate our\napproach experimentally in two open-domain planning problems: a 2D discrete\ngridworld task and a 3D continuous cooking task. A supplementary video can be\nfound at this http URL."""
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1802.08705,https://arxiv.org/abs/1802.08705,"b'Abstract:  Many planning applications involve complex relationships defined on\nhigh-dimensional, continuous variables. For example, robotic manipulation\nrequires planning with kinematic, collision, visibility, and motion constraints\ninvolving robot configurations, object transforms, and robot trajectories.\nThese constraints typically require specialized procedures to sample satisfying\nvalues. We extend the STRIPS planning language to support a generic,\ndeclarative specification for these procedures while treating their\nimplementation as black boxes. We also describe cost-sensitive planning within\nthis framework. We provide several domain-independent algorithms that reduce\nSTRIPStream problems to a sequence of finite-domain STRIPS planning problems.\nFinally, we evaluate our algorithms on three robotic planning domains.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1802.07426,https://arxiv.org/abs/1802.07426,"b'Abstract:  This paper introduces a novel measure-theoretic theory for machine learning\nthat does not require statistical assumptions. Based on this theory, a new\nregularization method in deep learning is derived and shown to outperform\nprevious methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed\ntheory provides a theoretical basis for a family of practically successful\nregularization methods in deep learning. We discuss several consequences of our\nresults on one-shot learning, representation learning, deep learning, and\ncurriculum learning. Unlike statistical learning theory, the proposed learning\ntheory analyzes each problem instance individually via measure theory, rather\nthan a set of problem instances via statistics. As a result, it provides\ndifferent types of results and insights when compared to statistical learning\ntheory.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1801.00680,https://arxiv.org/abs/1801.00680,"b'Abstract:  This paper presents a general-purpose formulation of a large class of\ndiscrete-time planning problems, with hybrid state and control-spaces, as\nfactored transition systems. Factoring allows state transitions to be described\nas the intersection of several constraints each affecting a subset of the state\nand control variables. Robotic manipulation problems with many movable objects\ninvolve constraints that only affect several variables at a time and therefore\nexhibit large amounts of factoring. We develop a theoretical framework for\nsolving factored transition systems with sampling-based algorithms. The\nframework characterizes conditions on the submanifold in which solutions lie,\nleading to a characterization of robust feasibility that incorporates\ndimensionality-reducing constraints. It then connects those conditions to\ncorresponding conditional samplers that can be composed to produce values on\nthis submanifold. We present two domain-independent, probabilistically complete\nplanning algorithms that take, as input, a set of conditional samplers. We\ndemonstrate the empirical efficiency of these algorithms on a set of\nchallenging task and motion planning problems involving picking, placing, and\npushing.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1711.03243,https://arxiv.org/abs/1711.03243,"b'Abstract:  Program synthesis is a class of regression problems where one seeks a\nsolution, in the form of a source-code program, mapping the inputs to their\ncorresponding outputs exactly. Due to its precise and combinatorial nature,\nprogram synthesis is commonly formulated as a constraint satisfaction problem,\nwhere input-output examples are encoded as constraints and solved with a\nconstraint solver. A key challenge of this formulation is scalability: while\nconstraint solvers work well with a few well-chosen examples, a large set of\nexamples can incur significant overhead in both time and memory. We describe a\nmethod to discover a subset of examples that is both small and representative:\nthe subset is constructed iteratively, using a neural network to predict the\nprobability of unchosen examples conditioned on the chosen examples in the\nsubset, and greedily adding the least probable example. We empirically evaluate\nthe representativeness of the subsets constructed by our method, and\ndemonstrate such subsets can significantly improve synthesis time and\nstability.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1711.01391,https://arxiv.org/abs/1711.01391,"b'Abstract:  In robotics, it is essential to be able to plan efficiently in\nhigh-dimensional continuous state-action spaces for long horizons. For such\ncomplex planning problems, unguided uniform sampling of actions until a path to\na goal is found is hopelessly inefficient, and gradient-based approaches often\nfall short when the optimization manifold of a given problem is not smooth. In\nthis paper we present an approach that guides the search of a state-space\nplanner, such as A*, by learning an action-sampling distribution that can\ngeneralize across different instances of a planning problem. The motivation is\nthat, unlike typical learning approaches for planning for continuous action\nspace that estimate a policy, an estimated action sampler is more robust to\nerror since it has a planner to fall back on. We use a Generative Adversarial\nNetwork (GAN), and address an important issue: search experience consists of a\nrelatively large number of actions that are not on a solution path and a\nrelatively small number of actions that actually are on a solution path. We\nintroduce a new technique, based on an importance-ratio estimation method, for\nusing samples from a non-target distribution to make GAN learning more\ndata-efficient. We provide theoretical guarantees and empirical evaluation in\nthree challenging continuous robot planning problems to illustrate the\neffectiveness of our algorithm.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1710.05468,https://arxiv.org/abs/1710.05468,"b'Abstract:  Throughout this chapter, we provide theoretical insights into why and how\ndeep learning can generalize well, despite its large capacity, complexity,\npossible algorithmic instability, nonrobustness, and sharp minima, responding\nto an open question in the literature. We also propose new open problems and\ndiscuss the limitations of our results.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1705.10907,https://arxiv.org/abs/1705.10907,"b'Abstract:  As drones and autonomous cars become more widespread it is becoming\nincreasingly important that robots can operate safely under realistic\nconditions. The noisy information fed into real systems means that robots must\nuse estimates of the environment to plan navigation. Efficiently guaranteeing\nthat the resulting motion plans are safe under these circumstances has proved\ndifficult. We examine how to guarantee that a trajectory or policy is safe with\nonly imperfect observations of the environment. We examine the implications of\nvarious mathematical formalisms of safety and arrive at a mathematical notion\nof safety of a long-term execution, even when conditioned on observational\ninformation. We present efficient algorithms that can prove that trajectories\nor policies are safe with much tighter bounds than in previous work. Notably,\nthe complexity of the environment does not affect our methods ability to\nevaluate if a trajectory or policy is safe. We then use these safety checking\nmethods to design a safe variant of the RRT planning algorithm.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1704.06131,https://arxiv.org/abs/1704.06131,"b'Abstract:  We consider the problem of diagnosis where a set of simple observations are\nused to infer a potentially complex hidden hypothesis. Finding the optimal\nsubset of observations is intractable in general, thus we focus on the problem\nof active diagnosis, where the agent selects the next most-informative\nobservation based on the results of previous observations. We show that under\nthe assumption of uniform observation entropy, one can build an implication\nmodel which directly predicts the outcome of the potential next observation\nconditioned on the results of past observations, and selects the observation\nwith the maximum entropy. This approach enjoys reduced computation complexity\nby bypassing the complicated hypothesis space, and can be trained on\nobservation data alone, learning how to query without knowledge of the hidden\nhypothesis.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1701.00287,https://arxiv.org/abs/1701.00287,"b'Abstract:  Many robotic planning applications involve continuous actions with highly\nnon-linear constraints, which cannot be modeled using modern planners that\nconstruct a propositional representation. We introduce STRIPStream: an\nextension of the STRIPS language which can model these domains by supporting\nthe specification of blackbox generators to handle complex constraints. The\noutputs of these generators interact with actions through possibly infinite\nstreams of objects and static predicates. We provide two algorithms which both\nreduce STRIPStream problems to a sequence of finite-domain planning problems.\nThe representation and algorithms are entirely domain independent. We\ndemonstrate our framework on simple illustrative domains, and then on a\nhigh-dimensional, continuous robotic task and motion planning domain.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1608.01335,https://arxiv.org/abs/1608.01335,"b""Abstract:  Mobile manipulation problems involving many objects are challenging to solve\ndue to the high dimensionality and multi-modality of their hybrid configuration\nspaces. Planners that perform a purely geometric search are prohibitively slow\nfor solving these problems because they are unable to factor the configuration\nspace. Symbolic task planners can efficiently construct plans involving many\nvariables but cannot represent the geometric and kinematic constraints required\nin manipulation. We present the FFRob algorithm for solving task and motion\nplanning problems. First, we introduce Extended Action Specification (EAS) as a\ngeneral purpose planning representation that supports arbitrary predicates as\nconditions. We adapt existing heuristic search ideas for solving \\proc{strips}\nplanning problems, particularly delete-relaxations, to solve EAS problem\ninstances. We then apply the EAS representation and planners to manipulation\nproblems resulting in FFRob. FFRob iteratively discretizes task and motion\nplanning problems using batch sampling of manipulation primitives and a\nmulti-query roadmap structure that can be conditionalized to evaluate\nreachability under different placements of movable objects. This structure\nenables the EAS planner to efficiently compute heuristics that incorporate\ngeometric and kinematic planning constraints to give a tight estimate of the\ndistance to the goal. Additionally, we show FFRob is probabilistically complete\nand has finite expected runtime. Finally, we empirically demonstrate FFRob's\neffectiveness on complex and diverse task and motion planning tasks including\nrearrangement planning and navigation among movable objects."""
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1608.01302,https://arxiv.org/abs/1608.01302,"b""Abstract:  We investigate learning heuristics for domain-specific planning. Prior work\nframed learning a heuristic as an ordinary regression problem. However, in a\ngreedy best-first search, the ordering of states induced by a heuristic is more\nindicative of the resulting planner's performance than mean squared error.\nThus, we instead frame learning a heuristic as a learning to rank problem which\nwe solve using a RankSVM formulation. Additionally, we introduce new methods\nfor computing features that capture temporal interactions in an approximate\nplan. Our experiments on recent International Planning Competition problems\nshow that the RankSVM learned heuristics outperform both the original\nheuristics and heuristics learned through ordinary regression."""
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1607.07762,https://arxiv.org/abs/1607.07762,"b'Abstract:  We introduce a framework for model learning and planning in stochastic\ndomains with continuous state and action spaces and non-Gaussian transition\nmodels. It is efficient because (1) local models are estimated only when the\nplanner requires them; (2) the planner focuses on the most relevant states to\nthe current planning problem; and (3) the planner focuses on the most\ninformative and/or high-value actions. Our theoretical analysis shows the\nvalidity and asymptotic optimality of the proposed approach. Empirically, we\ndemonstrate the effectiveness of our algorithm on a simulated multi-modal\npushing problem.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1604.03468,https://arxiv.org/abs/1604.03468,"b'Abstract:  In this paper we address planning problems in high-dimensional hybrid\nconfiguration spaces, with a particular focus on manipulation planning problems\ninvolving many objects. We present the hybrid backward-forward (HBF) planning\nalgorithm that uses a backward identification of constraints to direct the\nsampling of the infinite action space in a forward search from the initial\nstate towards a goal configuration. The resulting planner is probabilistically\ncomplete and can effectively construct long manipulation plans requiring both\nprehensile and nonprehensile actions in cluttered environments.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1604.01348,https://arxiv.org/abs/1604.01348,"b'Abstract:  This paper presents a Bayesian optimization method with exponential\nconvergence without the need of auxiliary optimization and without the\ndelta-cover sampling. Most Bayesian optimization methods require auxiliary\noptimization: an additional non-convex global optimization problem, which can\nbe time-consuming and hard to implement in practice. Also, the existing\nBayesian optimization method with exponential convergence requires access to\nthe delta-cover sampling, which was considered to be impractical. Our approach\neliminates both requirements and achieves an exponential convergence rate.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1512.00573,https://arxiv.org/abs/1512.00573,"b'Abstract:  To accomplish tasks in human-centric indoor environments, robots need to\nrepresent and understand the world in terms of objects and their attributes. We\nrefer to this attribute-based representation as a world model, and consider how\nto acquire it via noisy perception and maintain it over time, as objects are\nadded, changed, and removed in the world. Previous work has framed this as\nmultiple-target tracking problem, where objects are potentially in motion at\nall times. Although this approach is general, it is computationally expensive.\nWe argue that such generality is not needed in typical world modeling tasks,\nwhere objects only change state occasionally. More efficient approaches are\nenabled by restricting ourselves to such semi-static environments.\nWe consider a previously-proposed clustering-based world modeling approach\nthat assumed static environments, and extend it to semi-static domains by\napplying a dependent Dirichlet-process (DDP) mixture model. We derive a novel\nMAP inference algorithm under this model, subject to data association\nconstraints. We demonstrate our approach improves computational performance in\nsemi-static environments.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1408.1484,https://arxiv.org/abs/1408.1484,"b'Abstract:  Cooperative games are those in which both agents share the same payoff\nstructure. Value-based reinforcement-learning algorithms, such as variants of\nQ-learning, have been applied to learning cooperative games, but they only\napply when the game state is completely observable to both agents. Policy\nsearch methods are a reasonable alternative to value-based methods for\npartially observable environments. In this paper, we provide a gradient-based\ndistributed policy-search method for cooperative games and compare the notion\nof local optimum to that of Nash equilibrium. We demonstrate the effectiveness\nof this method experimentally in a small, partially observable simulated soccer\ndomain.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1402.2871,https://arxiv.org/abs/1402.2871,"b'Abstract:  We describe a probabilistic framework for synthesizing control policies for\ngeneral multi-robot systems, given environment and sensor models and a cost\nfunction. Decentralized, partially observable Markov decision processes\n(Dec-POMDPs) are a general model of decision processes where a team of agents\nmust cooperate to optimize some objective (specified by a shared reward or cost\nfunction) in the presence of uncertainty, but where communication limitations\nmean that the agents cannot share their state, so execution must proceed in a\ndecentralized fashion. While Dec-POMDPs are typically intractable to solve for\nreal-world problems, recent research on the use of macro-actions in Dec-POMDPs\nhas significantly increased the size of problem that can be practically solved\nas a Dec-POMDP. We describe this general model, and show how, in contrast to\nmost existing methods that are specialized to a particular problem class, it\ncan synthesize control policies that use whatever opportunities for\ncoordination are present in the problem, while balancing off uncertainty in\noutcomes, sensor information, and information about other agents. We use three\nvariations on a warehouse task to show that a single planner of this type can\ngenerate cooperative behavior using task allocation, direct communication, and\nsignaling, as appropriate.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1303.1491,https://arxiv.org/abs/1303.1491,"b'Abstract:  We describe a method for time-critical decision making involving sequential\ntasks and stochastic processes. The method employs several iterative refinement\nroutines for solving different aspects of the decision making problem. This\npaper concentrates on the meta-level control problem of deliberation\nscheduling, allocating computational resources to these routines. We provide\ndifferent models corresponding to optimization problems that capture the\ndifferent circumstances and computational strategies for decision making under\ntime constraints. We consider precursor models in which all decision making is\nperformed prior to execution and recurrent models in which decision making is\nperformed in parallel with execution, accounting for the states observed during\nexecution and anticipating future states. We describe algorithms for precursor\nand recurrent models and provide the results of our empirical investigations to\ndate.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1302.4971,https://arxiv.org/abs/1302.4971,"b'Abstract:  Markov decision problems (MDPs) provide the foundations for a number of\nproblems of interest to AI researchers studying automated planning and\nreinforcement learning. In this paper, we summarize results regarding the\ncomplexity of solving MDPs and the running time of MDP solution algorithms. We\nargue that, although MDPs can be solved efficiently in theory, more study is\nneeded to reveal practical algorithms for solving large problems quickly. To\nencourage future research, we sketch some alternative methods of analysis that\nrely on the structure of MDPs.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.7381,https://arxiv.org/abs/1301.7381,"b'Abstract:  We investigate the use of temporally abstract actions, or macro-actions, in\nthe solution of Markov decision processes. Unlike current models that combine\nboth primitive actions and macro-actions and leave the state space unchanged,\nwe propose a hierarchical model (using an abstract MDP) that works with\nmacro-actions only, and that significantly reduces the size of the state space.\nThis is achieved by treating macroactions as local policies that act in certain\nregions of state space, and by restricting states in the abstract MDP to those\nat the boundaries of regions. The abstract MDP approximates the original and\ncan be solved more efficiently. We discuss several ways in which macro-actions\ncan be generated to ensure good solution quality. Finally, we consider ways in\nwhich macro-actions can be reused to solve multiple, related MDPs; and we show\nthat this can justify the computational overhead of macro-action generation.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.6730,https://arxiv.org/abs/1301.6730,"b'Abstract:  Many applications require that we learn the parameters of a model from data.\nEM is a method used to learn the parameters of probabilistic models for which\nthe data for some of the variables in the models is either missing or hidden.\nThere are instances in which this method is slow to converge. Therefore,\nseveral accelerations have been proposed to improve the method. None of the\nproposed acceleration methods are theoretically dominant and experimental\ncomparisons are lacking. In this paper, we present the different proposed\naccelerations and try to compare them experimentally. From the results of the\nexperiments, we argue that some acceleration of EM is always possible, but that\nwhich acceleration is superior depends on properties of the problem.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.6721,https://arxiv.org/abs/1301.6721,"b""Abstract:  Reactive (memoryless) policies are sufficient in completely observable Markov\ndecision processes (MDPs), but some kind of memory is usually necessary for\noptimal control of a partially observable MDP. Policies with finite memory can\nbe represented as finite-state automata. In this paper, we extend Baird and\nMoore's VAPS algorithm to the problem of learning general finite-state\nautomata. Because it performs stochastic gradient descent, this algorithm can\nbe shown to converge to a locally optimal finite-state controller. We provide\nthe details of the algorithm and then consider the question of under what\nconditions stochastic gradient descent will outperform exact gradient descent.\nWe conclude with empirical results comparing the performance of stochastic and\nexact gradient descent, and showing the ability of our algorithm to extract the\nuseful information contained in the sequence of past observations to compensate\nfor the lack of observability at each time-step."""
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.6720,https://arxiv.org/abs/1301.6720,"b'Abstract:  Solving partially observable Markov decision processes (POMDPs) is highly\nintractable in general, at least in part because the optimal policy may be\ninfinitely large. In this paper, we explore the problem of finding the optimal\npolicy from a restricted set of policies, represented as finite state automata\nof a given size. This problem is also intractable, but we show that the\ncomplexity can be greatly reduced when the POMDP and/or policy are further\nconstrained. We demonstrate good empirical results with a branch-and-bound\nmethod for finding globally optimal deterministic policies, and a\ngradient-ascent method for finding locally optimal stochastic policies.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.3882,https://arxiv.org/abs/1301.3882,"b'Abstract:  Sampling is an important tool for estimating large, complex sums and\nintegrals over high dimensional spaces. For instance, important sampling has\nbeen used as an alternative to exact methods for inference in belief networks.\nIdeally, we want to have a sampling distribution that provides optimal-variance\nestimators. In this paper, we present methods that improve the sampling\ndistribution by systematically adapting it as we obtain information from the\nsamples. We present a stochastic-gradient-descent method for sequentially\nupdating the sampling distribution based on the direct minization of the\nvariance. We also present other stochastic-gradient-descent methods based on\nthe minimizationof typical notions of distance between the current sampling\ndistribution and approximations of the target, optimal distribution. We finally\nvalidate and compare the different methods empirically by applying them to the\nproblem of action evaluation in influence diagrams.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.0567,https://arxiv.org/abs/1301.0567,"b'Abstract:  Most reinforcement learning methods operate on propositional representations\nof the world state. Such representations are often intractably large and\ngeneralize poorly. Using a deictic representation is believed to be a viable\nalternative: they promise generalization while allowing the use of existing\nreinforcement-learning methods. Yet, there are few experiments on learning with\ndeictic representations reported in the literature. In this paper we explore\nthe effectiveness of two forms of deictic representation and a na\xc3\xafve\npropositional representation in a simple blocks-world domain. We find,\nempirically, that the deictic representations actually worsen learning\nperformance. We conclude with a discussion of possible causes of these results\nand strategies for more effective learning in domains with objects.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1206.5928,https://arxiv.org/abs/1206.5928,"b'Abstract:  We apply decision theoretic techniques to construct non-player characters\nthat are able to assist a human player in collaborative games. The method is\nbased on solving Markov decision processes, which can be difficult when the\ngame state is described by many variables. To scale to more complex games, the\nmethod allows decomposition of a game task into subtasks, each of which can be\nmodelled by a Markov decision process. Intention recognition is used to infer\nthe subtask that the human is currently performing, allowing the helper to\nassist the human in performing the correct task. Experiments show that the\nmethod can be effective, giving near-human level performance in helping a human\nin a collaborative game.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1206.5249,https://arxiv.org/abs/1206.5249,"b""Abstract:  The ways in which an agent's actions affect the world can often be modeled\ncompactly using a set of relational probabilistic planning rules. This paper\naddresses the problem of learning such rule sets for multiple related tasks. We\ntake a hierarchical Bayesian approach, in which the system learns a prior\ndistribution over rule sets. We present a class of prior distributions\nparameterized by a rule set prototype that is stochastically modified to\nproduce a task-specific rule set. We also describe a coordinate ascent\nalgorithm that iteratively optimizes the task-specific rule sets and the prior\ndistribution. Experiments using this algorithm show that transferring\ninformation from related tasks significantly reduces the amount of training\ndata required to predict action effects in blocks-world domains."""
Leslie Kaelbling,Kaelbling_Leslie,arXiv:cs/0105032,https://arxiv.org/abs/cs/0105032,"b'Abstract:  Cooperative games are those in which both agents share the same payoff\nstructure. Value-based reinforcement-learning algorithms, such as variants of\nQ-learning, have been applied to learning cooperative games, but they only\napply when the game state is completely observable to both agents. Policy\nsearch methods are a reasonable alternative to value-based methods for\npartially observable environments. In this paper, we provide a gradient-based\ndistributed policy-search method for cooperative games and compare the notion\nof local optimum to that of Nash equilibrium. We demonstrate the effectiveness\nof this method experimentally in a small, partially observable simulated soccer\ndomain.'"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:cs/0103003,https://arxiv.org/abs/cs/0103003,"b""Abstract:  In order for an agent to perform well in partially observable domains, it is\nusually necessary for actions to depend on the history of observations. In this\npaper, we explore a {\\it stigmergic} approach, in which the agent's actions\ninclude the ability to set and clear bits in an external memory, and the\nexternal memory is included as part of the input to the agent. In this case, we\nneed to learn a reactive policy in a highly non-Markovian domain. We explore\ntwo algorithms: SARSA(\\lambda), which has had empirical success in partially\nobservable domains, and VAPS, a new algorithm due to Baird and Moore, with\nconvergence guarantees in partially observable domains. We compare the\nperformance of these two algorithms on benchmark problems."""
David Karger,Karger_David,arXiv:1705.08992,https://arxiv.org/abs/1705.08992,"b'Abstract:  This paper formulates a novel problem on graphs: find the minimal subset of\nedges in a fully connected graph, such that the resulting graph contains all\nspanning trees for a set of specifed sub-graphs. This formulation is motivated\nby an un-supervised grammar induction problem from computational linguistics.\nWe present a reduction to some known problems and algorithms from graph theory,\nprovide computational complexity results, and describe an approximation\nalgorithm.'"
David Karger,Karger_David,arXiv:1409.6680,https://arxiv.org/abs/1409.6680,"b""Abstract:  Constructing a good conference schedule for a large multi-track conference\nneeds to take into account the preferences and constraints of organizers,\nauthors, and attendees. Creating a schedule which has fewer conflicts for\nauthors and attendees, and thematically coherent sessions is a challenging\ntask.\nCobi introduced an alternative approach to conference scheduling by engaging\nthe community to play an active role in the planning process. The current Cobi\npipeline consists of committee-sourcing and author-sourcing to plan a\nconference schedule. We further explore the design space of community-sourcing\nby introducing attendee-sourcing -- a process that collects input from\nconference attendees and encodes them as preferences and constraints for\ncreating sessions and schedule. For CHI 2014, a large multi-track conference in\nhuman-computer interaction with more than 3,000 attendees and 1,000 authors, we\ncollected attendees' preferences by making available all the accepted papers at\nthe conference on a paper recommendation tool we built called Confer, for a\nperiod of 45 days before announcing the conference program (sessions and\nschedule). We compare the preferences marked on Confer with the preferences\ncollected from Cobi's author-sourcing approach. We show that attendee-sourcing\ncan provide insights beyond what can be discovered by author-sourcing. For CHI\n2014, the results show value in the method and attendees' participation. It\nproduces data that provides more alternatives in scheduling and complements\ndata collected from other methods for creating coherent sessions and reducing\nconflicts."""
David Karger,Karger_David,arXiv:1401.3488,https://arxiv.org/abs/1401.3488,"b'Abstract:  We present a novel Bayesian topic model for learning discourse-level document\nstructure. Our model leverages insights from discourse theory to constrain\nlatent topic assignments in a way that reflects the underlying organization of\ndocument topics. We propose a global model in which both topic selection and\nordering are biased to be similar across a collection of related documents. We\nshow that this space of orderings can be effectively represented using a\ndistribution over permutations called the Generalized Mallows Model. We apply\nour method to three complementary discourse-level tasks: cross-document\nalignment, document segmentation, and information ordering. Our experiments\nshow that incorporating our permutation-based model in these applications\nyields substantial improvements in performance over previously proposed\nmethods.'"
David Karger,Karger_David,arXiv:1204.2995,https://arxiv.org/abs/1204.2995,"b'Abstract:  Realtime crowdsourcing research has demonstrated that it is possible to\nrecruit paid crowds within seconds by managing a small, fast-reacting worker\npool. Realtime crowds enable crowd-powered systems that respond at interactive\nspeeds: for example, cameras, robots and instant opinion polls. So far, these\ntechniques have mainly been proof-of-concept prototypes: research has not yet\nattempted to understand how they might work at large scale or optimize their\ncost/performance trade-offs. In this paper, we use queueing theory to analyze\nthe retainer model for realtime crowdsourcing, in particular its expected wait\ntime and cost to requesters. We provide an algorithm that allows requesters to\nminimize their cost subject to performance requirements. We then propose and\nanalyze three techniques to improve performance: push notifications, shared\nretainer pools, and precruitment, which involves recalling retainer workers\nbefore a task actually arrives. An experimental validation finds that\nprecruited workers begin a task 500 milliseconds after it is posted, delivering\nresults below the one-second cognitive threshold for an end-user to stay in\nflow.'"
David Karger,Karger_David,arXiv:1110.3564,https://arxiv.org/abs/1110.3564,"b'Abstract:  Crowdsourcing systems, in which numerous tasks are electronically distributed\nto numerous ""information piece-workers"", have emerged as an effective paradigm\nfor human-powered solving of large scale problems in domains such as image\nclassification, data entry, optical character recognition, recommendation, and\nproofreading. Because these low-paid workers can be unreliable, nearly all such\nsystems must devise schemes to increase confidence in their answers, typically\nby assigning each task multiple times and combining the answers in an\nappropriate manner, e.g. majority voting.\nIn this paper, we consider a general model of such crowdsourcing tasks and\npose the problem of minimizing the total price (i.e., number of task\nassignments) that must be paid to achieve a target overall reliability. We give\na new algorithm for deciding which tasks to assign to which workers and for\ninferring correct answers from the workers\' answers. We show that our\nalgorithm, inspired by belief propagation and low-rank matrix approximation,\nsignificantly outperforms majority voting and, in fact, is optimal through\ncomparison to an oracle that knows the reliability of every worker. Further, we\ncompare our approach with a more general class of algorithms which can\ndynamically assign tasks. By adaptively deciding which questions to ask to the\nnext arriving worker, one might hope to reduce uncertainty more efficiently. We\nshow that, perhaps surprisingly, the minimum price necessary to achieve a\ntarget reliability scales in the same manner under both adaptive and\nnon-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under\nboth scenarios. This strongly relies on the fact that workers are fleeting and\ncan not be exploited. Therefore, architecturally, our results suggest that\nbuilding a reliable worker-reputation system is essential to fully harnessing\nthe potential of adaptive designs.'"
David Karger,Karger_David,arXiv:1109.6881,https://arxiv.org/abs/1109.6881,"b""Abstract:  Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible\nto task people with small jobs, such as labeling images or looking up phone\nnumbers, via a programmatic interface. MTurk tasks for processing datasets with\nhumans are currently designed with significant reimplementation of common\nworkflows and ad-hoc selection of parameters such as price to pay per task. We\ndescribe how we have integrated crowds into a declarative workflow engine\ncalled Qurk to reduce the burden on workflow designers. In this paper, we focus\non how to use humans to compare items for sorting and joining data, two of the\nmost common operations in DBMSs. We describe our basic query interface and the\nuser interface of the tasks we post to MTurk. We also propose a number of\noptimizations, including task batching, replacing pairwise comparisons with\nnumerical ratings, and pre-filtering tables before joining them, which\ndramatically reduce the overall cost of running sorts and joins on the crowd.\nIn an experiment joining two sets of images, we reduce the overall cost from\n$67 in a naive implementation to about $3, without substantially affecting\naccuracy or latency. In an end-to-end experiment, we reduced cost by a factor\nof 14.5."""
David Karger,Karger_David,arXiv:1107.3013,https://arxiv.org/abs/1107.3013,"b'Abstract:  We present an algorithm for generating Poisson-disc patterns taking O(N) time\nto generate $N$ points. The method is based on a grid of regions which can\ncontain no more than one point in the final pattern, and uses an explicit model\nof point arrival times under a uniform Poisson process.'"
David Karger,Karger_David,arXiv:1104.2527,https://arxiv.org/abs/1104.2527,"b""Abstract:  We use network coding to improve the speed of distributed computation in the\ndynamic network model of Kuhn, Lynch and Oshman [STOC '10]. In this model an\nadversary adaptively chooses a new network topology in every round, making even\nbasic distributed computations challenging.\nKuhn et al. show that n nodes, each starting with a d-bit token, can\nbroadcast them to all nodes in time O(n^2) using b-bit messages, where b > d +\nlog n. Their algorithms take the natural approach of {token forwarding}: in\nevery round each node broadcasts some particular token it knows. They prove\nmatching Omega(n^2) lower bounds for a natural class of token forwarding\nalgorithms and an Omega(n log n) lower bound that applies to all\ntoken-forwarding algorithms.\nWe use network coding, transmitting random linear combinations of tokens, to\nbreak both lower bounds. Our algorithm's performance is quadratic in the\nmessage size b, broadcasting the n tokens in roughly d/b^2 * n^2 rounds. For b\n= d = O(log n) our algorithms use O(n^2/log n) rounds, breaking the first lower\nbound, while for larger message sizes we obtain linear-time algorithms. We also\nconsider networks that change only every T rounds, and achieve an additional\nfactor T^2 speedup. This contrasts with related lower and upper bounds of Kuhn\net al. implying that for natural token-forwarding algorithms a speedup of T,\nbut not more, can be obtained. Lastly, we give a general way to derandomize\nrandom linear network coding, that also leads to new deterministic information\ndissemination algorithms."""
David Karger,Karger_David,arXiv:0802.2418,https://arxiv.org/abs/0802.2418,"b'Abstract:  This paper presents improved approximation algorithms for the problem of\nmultiprocessor scheduling under uncertainty, or SUU, in which the execution of\neach job may fail probabilistically. This problem is motivated by the\nincreasing use of distributed computing to handle large, computationally\nintensive tasks. In the SUU problem we are given n unit-length jobs and m\nmachines, a directed acyclic graph G of precedence constraints among jobs, and\nunrelated failure probabilities q_{ij} for each job j when executed on machine\ni for a single timestep. Our goal is to find a schedule that minimizes the\nexpected makespan, which is the expected time at which all jobs complete.\nLin and Rajaraman gave the first approximations for this NP-hard problem for\nthe special cases of independent jobs, precedence constraints forming disjoint\nchains, and precedence constraints forming trees. In this paper, we present\nasymptotically better approximation algorithms. In particular, we give an\nO(loglog min(m,n))-approximation for independent jobs (improving on the\npreviously best O(log n)-approximation). We also give an O(log(n+m) loglog\nmin(m,n))-approximation algorithm for precedence constraints that form disjoint\nchains (improving on the previously best\nO(log(n)log(m)log(n+m)/loglog(n+m))-approximation by a (log n/loglog n)^2\nfactor when n = poly(m). Our algorithm for precedence constraints forming\nchains can also be used as a component for precedence constraints forming\ntrees, yielding a similar improvement over the previously best algorithms for\ntrees.'"
David Karger,Karger_David,arXiv:cs/0603022,https://arxiv.org/abs/cs/0603022,"b'Abstract:  We examine the issue of separation and code design for networks that operate\nover finite fields. We demonstrate that source-channel (or source-network)\nseparation holds for several canonical network examples like the noisy multiple\naccess channel and the erasure degraded broadcast channel, when the whole\nnetwork operates over a common finite field. This robustness of separation is\npredicated on the fact that noise and inputs are independent, and we examine\nthe failure of separation when noise is dependent on inputs in multiple access\nchannels.\nOur approach is based on the sufficiency of linear codes. Using a simple and\nunifying framework, we not only re-establish with economy the optimality of\nlinear codes for single-transmitter, single-receiver channels and for\nSlepian-Wolf source coding, but also establish the optimality of linear codes\nfor multiple access and for erasure degraded broadcast channels. The linearity\nallows us to obtain simple optimal code constructions and to study capacity\nregions of the noisy multiple access and the degraded broadcast channel. The\nlinearity of both source and network coding blurs the delineation between\nsource and network codes. While our results point to the fact that separation\nof source coding and channel coding is optimal in some canonical networks, we\nshow that decomposing networks into canonical subnetworks may not be effective.\nThus, we argue that it may be the lack of decomposability of a network into\ncanonical network modules, rather than the lack of separation between source\nand channel coding, that presents major challenges for coding over networks.'"
David Karger,Karger_David,arXiv:cs/0503064,https://arxiv.org/abs/cs/0503064,"b'Abstract:  We consider the problem of establishing minimum-cost multicast connections\nover coded packet networks, i.e. packet networks where the contents of outgoing\npackets are arbitrary, causal functions of the contents of received packets. We\nconsider both wireline and wireless packet networks as well as both static\nmulticast (where membership of the multicast group remains constant for the\nduration of the connection) and dynamic multicast (where membership of the\nmulticast group changes in time, with nodes joining and leaving the group).\nFor static multicast, we reduce the problem to a polynomial-time solvable\noptimization problem, and we present decentralized algorithms for solving it.\nThese algorithms, when coupled with existing decentralized schemes for\nconstructing network codes, yield a fully decentralized approach for achieving\nminimum-cost multicast. By contrast, establishing minimum-cost static multicast\nconnections over routed packet networks is a very difficult problem even using\ncentralized computation, except in the special cases of unicast and broadcast\nconnections.\nFor dynamic multicast, we reduce the problem to a dynamic programming problem\nand apply the theory of dynamic programming to suggest how it may be solved.'"
David Karger,Karger_David,arXiv:cs/0207078,https://arxiv.org/abs/cs/0207078,"b""Abstract:  We improve on random sampling techniques for approximately solving problems\nthat involve cuts and flows in graphs. We give a near-linear-time construction\nthat transforms any graph on n vertices into an O(n\\log n)-edge graph on the\nsame vertices whose cuts have approximately the same value as the original\ngraph's. In this new graph, for example, we can run the O(m^{3/2})-time maximum\nflow algorithm of Goldberg and Rao to find an s--t minimum cut in O(n^{3/2})\ntime. This corresponds to a (1+epsilon)-times minimum s--t cut in the original\ngraph. In a similar way, we can approximate a sparsest cut to within O(log n)\nin O(n^2) time using a previous O(mn)-time algorithm. A related approach leads\nto a randomized divide and conquer algorithm producing an approximately maximum\nflow in O(m sqrt{n}) time."""
David Karger,Karger_David,arXiv:cs/0205051,https://arxiv.org/abs/cs/0205051,"b'Abstract:  The multiway-cut problem is, given a weighted graph and k >= 2 terminal\nnodes, to find a minimum-weight set of edges whose removal separates all the\nterminals. The problem is NP-hard, and even NP-hard to approximate within\n1+delta for some small delta > 0.\nCalinescu, Karloff, and Rabani (1998) gave an algorithm with performance\nguarantee 3/2-1/k, based on a geometric relaxation of the problem. In this\npaper, we give improved randomized rounding schemes for their relaxation,\nyielding a 12/11-approximation algorithm for k=3 and a 1.3438-approximation\nalgorithm in general.\nOur approach hinges on the observation that the problem of designing a\nrandomized rounding scheme for a geometric relaxation is itself a linear\nprogramming problem. The paper explores computational solutions to this\nproblem, and gives a proof that for a general class of geometric relaxations,\nthere are always randomized rounding schemes that match the integrality gap.'"
David Karger,Karger_David,arXiv:cs/9812008,https://arxiv.org/abs/cs/9812008,"b'Abstract:  We consider the problem of coloring k-colorable graphs with the fewest\npossible colors. We present a randomized polynomial time algorithm that colors\na 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta log\nn), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of any\nvertex. Besides giving the best known approximation ratio in terms of n, this\nmarks the first non-trivial approximation result as a function of the maximum\ndegree Delta. This result can be generalized to k-colorable graphs to obtain a\ncoloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)}\nlog^{1/2} n) colors. Our results are inspired by the recent work of Goemans and\nWilliamson who used an algorithm for semidefinite optimization problems, which\ngeneralize linear programs, to obtain improved approximations for the MAX CUT\nand MAX 2-SAT problems. An intriguing outcome of our work is a duality\nrelationship established between the value of the optimum solution to our\nsemidefinite program and the Lovasz theta-function. We show lower bounds on the\ngap between the optimum solution of our semidefinite program and the actual\nchromatic number; by duality this also demonstrates interesting new facts about\nthe theta-function.'"
David Karger,Karger_David,arXiv:cs/9812007,https://arxiv.org/abs/cs/9812007,"b""Abstract:  We significantly improve known time bounds for solving the minimum cut\nproblem on undirected graphs. We use a ``semi-duality'' between minimum cuts\nand maximum spanning tree packings combined with our previously developed\nrandom sampling techniques. We give a randomized algorithm that finds a minimum\ncut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We\nalso give a simpler randomized algorithm that finds all minimum cuts with high\nprobability in O(n^2 log n) time. This variant has an optimal RNC\nparallelization. Both variants improve on the previous best time bound of O(n^2\nlog^3 n). Other applications of the tree-packing approach are new, nearly tight\nbounds on the number of near minimum cuts a graph may have and a new data\nstructure for representing them in a space-efficient manner."""
David Karger,Karger_David,arXiv:cs/9809012,https://arxiv.org/abs/cs/9809012,"b'Abstract:  The classic all-terminal network reliability problem posits a graph, each of\nwhose edges fails independently with some given probability.'"
Dina Katabi,Katabi_Dina,arXiv:1902.02037,https://arxiv.org/abs/1902.02037,"b'Abstract:  We consider the problem of inferring the values of an arbitrary set of\nvariables (e.g., risk of diseases) given other observed variables (e.g.,\nsymptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images\nor EEG). This is a common problem in healthcare since variables of interest\noften differ for different patients. Existing methods including Bayesian\nnetworks and structured prediction either do not incorporate high-dimensional\nsignals or fail to model conditional dependencies among variables. To address\nthese issues, we propose bidirectional inference networks (BIN), which stich\ntogether multiple probabilistic neural networks, each modeling a conditional\ndependency. Predictions are then made via iteratively updating variables using\nbackpropagation (BP) to maximize corresponding posterior probability.\nFurthermore, we extend BIN to composite BIN (CBIN), which involves the\niterative prediction process in the training stage and improves both accuracy\nand computational efficiency by adaptively smoothing the optimization\nlandscape. Experiments on synthetic and real-world datasets (a sleep study and\na dermatology dataset) show that CBIN is a single model that can achieve\nstate-of-the-art performance and obtain better accuracy in most inference tasks\nthan multiple models each specifically trained for a different task.'"
Dina Katabi,Katabi_Dina,arXiv:1706.06935,https://arxiv.org/abs/1706.06935,"b'Abstract:  There is much interest in integrating millimeter wave radios (mmWave) into\nwireless LANs and 5G cellular networks to benefit from their multiple GHz of\navailable spectrum. Yet unlike existing technologies, e.g., WiFi, mmWave radios\nrequire highly directional antennas. Since the antennas have pencil-beams, the\ntransmitter and receiver need to align their antenna beams before they can\ncommunicate. Existing solutions scan the entire space to find the best\nalignment. Such a process has been shown to introduce up to seconds of delay,\nand is unsuitable for wireless networks where an access point has to quickly\nswitch between users and accommodate mobile clients.\nThis paper presents Rapid-Link, a new protocol that can find the best mmWave\nbeam alignment without scanning the space. Given all possible directions for\nsetting the antenna beam, Rapid-Link provably finds the optimal direction in\nlogarithmic number of measurements. Further, Rapid-Link works within the\nexisting 802.11ad standard for mmWave LAN, and can support both clients and\naccess points. We have implemented Rapid-Link in a mmWave radio and evaluated\nit empirically. Our results show that it reduces beam alignment delay by orders\nof magnitude. In particular, for highly directional mmWave devices operating\nunder 802.11ad, the delay drops from over a second to 2.5 ms.'"
Dina Katabi,Katabi_Dina,arXiv:1612.02307,https://arxiv.org/abs/1612.02307,"b'Abstract:  Many sensor applications are interested in computing a function over\nmeasurements (e.g., sum, average, max) as opposed to collecting all sensor\ndata. Today, such data aggregation is done in a cluster-head. Sensor nodes\ntransmit their values sequentially to a cluster-head node, which calculates the\naggregation function and forwards it to the base station. In contrast, this\npaper explores the possibility of computing a desired function over the air. We\ndevise a solution that enables sensors to transmit coherently over the wireless\nmedium so that the cluster-head directly receives the value of the desired\nfunction. We present analysis and preliminary results that demonstrate that\nsuch a design yield a large improvement in network throughput.'"
Dina Katabi,Katabi_Dina,arXiv:1505.03446,https://arxiv.org/abs/1505.03446,"b'Abstract:  Time-of-flight, i.e., the time incurred by a signal to travel from\ntransmitter to receiver, is perhaps the most intuitive way to measure distances\nusing wireless signals. It is used in major positioning systems such as GPS,\nRADAR, and SONAR. However, attempts at using time-of-flight for indoor\nlocalization have failed to deliver acceptable accuracy due to fundamental\nlimitations in measuring time on Wi-Fi and other RF consumer technologies.\nWhile the research community has developed alternatives for RF-based indoor\nlocalization that do not require time-of-flight, those approaches have their\nown limitations that hamper their use in practice. In particular, many existing\napproaches need receivers with large antenna arrays while commercial Wi-Fi\nnodes have two or three antennas. Other systems require fingerprinting the\nenvironment to create signal maps. More fundamentally, none of these methods\nsupport indoor positioning between a pair of Wi-Fi devices\nwithout~third~party~support.\nIn this paper, we present a set of algorithms that measure the time-of-flight\nto sub-nanosecond accuracy on commercial Wi-Fi cards. We implement these\nalgorithms and demonstrate a system that achieves accurate device-to-device\nlocalization, i.e. enables a pair of Wi-Fi devices to locate each other without\nany support from the infrastructure, not even the location of the access\npoints.'"
Dina Katabi,Katabi_Dina,arXiv:1303.1209,https://arxiv.org/abs/1303.1209,"b'Abstract:  We present the first sample-optimal sublinear time algorithms for the sparse\nDiscrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our\nalgorithms are analyzed for /average case/ signals. For signals whose spectrum\nis exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,\nwhere k is the expected sparsity of the signal. For signals whose spectrum is\napproximately sparse, our algorithm uses O(k log n) samples and runs in O(k\nlog^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of\nsamples used by our algorithms matches the known lower bounds for the\nrespective signal models.\nBy a known reduction, our algorithms give similar results for the\none-dimensional sparse Discrete Fourier Transform when n is a power of a small\ncomposite number (e.g., n = 6^t).'"
Dina Katabi,Katabi_Dina,arXiv:1201.2501,https://arxiv.org/abs/1201.2501,"b'Abstract:  We consider the problem of computing the k-sparse approximation to the\ndiscrete Fourier transform of an n-dimensional signal. We show:\n* An O(k log n)-time randomized algorithm for the case where the input signal\nhas at most k non-zero Fourier coefficients, and\n* An O(k log n log(n/k))-time randomized algorithm for general input signals.\nBoth algorithms achieve o(n log n) time, and thus improve over the Fast\nFourier Transform, for any k = o(n). They are the first known algorithms that\nsatisfy this property. Also, if one assumes that the Fast Fourier Transform is\noptimal, the algorithm for the exactly k-sparse case is optimal for any k =\nn^{\\Omega(1)}.\nWe complement our algorithmic results by showing that any algorithm for\ncomputing the sparse Fourier transform of a general signal must use at least\n\\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to perform\nadaptive sampling.'"
Manolis Kellis,Kellis_Manolis,arXiv:1901.08540,https://arxiv.org/abs/1901.08540,"b'Abstract:  Summary statistics of genome-wide association studies (GWAS) teach causal\nrelationship between millions of genetic markers and tens and thousands of\nphenotypes. However, underlying biological mechanisms are yet to be elucidated.\nWe can achieve necessary interpretation of GWAS in a causal mediation\nframework, looking to establish a sparse set of mediators between genetic and\ndownstream variables, but there are several challenges. Unlike existing methods\nrely on strong and unrealistic assumptions, we tackle practical challenges\nwithin a principled summary-based causal inference framework. We analyzed the\nproposed methods in extensive simulations generated from real-world genetic\ndata. We demonstrated only our approach can accurately redeem causal genes,\neven without knowing actual individual-level data, despite the presence of\ncompeting non-causal trails.'"
Manolis Kellis,Kellis_Manolis,arXiv:1811.01431,https://arxiv.org/abs/1811.01431,"b""Abstract:  Artificial Intelligence (AI) incorporating genetic and medical information\nhave been applied in disease risk prediction, unveiling disease mechanism, and\nadvancing therapeutics. However, AI training relies on highly sensitive and\nprivate data which significantly limit their applications and robustness\nevaluation. Moreover, the data access management after sharing across\norganization heavily relies on legal restriction, and there is no guarantee in\npreventing data leaking after sharing. Here, we present Genie, a secure AI\nplatform which allows AI models to be trained on medical data securely. The\nplatform combines the security of Intel Software Guarded eXtensions (SGX),\ntransparency of blockchain technology, and verifiability of open algorithms and\nsource codes. Genie shares insights of genetic and medical data without\nexposing anyone's raw data. All data is instantly encrypted upon upload and\ncontributed to the models that the user chooses. The usage of the model and the\nvalue generated from the genetic and health data will be tracked via a\nblockchain, giving the data transparent and immutable ownership."""
Manolis Kellis,Kellis_Manolis,arXiv:1811.00464,https://arxiv.org/abs/1811.00464,"b'Abstract:  Electronic health records (EHR) are rich heterogeneous collection of patient\nhealth information, whose broad adoption provides great opportunities for\nsystematic health data mining. However, heterogeneous EHR data types and biased\nascertainment impose computational challenges. Here, we present mixEHR, an\nunsupervised generative model integrating collaborative filtering and latent\ntopic models, which jointly models the discrete distributions of data\nobservation bias and actual data using latent disease-topic distributions. We\napply mixEHR on 12.8 million phenotypic observations from the MIMIC dataset,\nand use it to reveal latent disease topics, interpret EHR results, impute\nmissing data, and predict mortality in intensive care units. Using both\nsimulation and real data, we show that mixEHR outperforms previous methods and\nreveals meaningful multi-disease insights.'"
Manolis Kellis,Kellis_Manolis,arXiv:1606.07383,https://arxiv.org/abs/1606.07383,"b'Abstract:  Several significant models have been developed that enable the study of\ndiffusion of signals across biological, social and engineered networks. Within\nthese established frameworks, the inverse problem of identifying the source of\nthe propagated signal is challenging, owing to the numerous alternative\npossibilities for signal progression through the network. In real world\nnetworks, the challenge of determining sources is compounded as the true\npropagation dynamics are typically unknown, and when they have been directly\nmeasured, they rarely conform to the assumptions of any of the well-studied\nmodels. In this paper we introduce a method called Network Infusion (NI) that\nhas been designed to circumvent these issues, making source inference practical\nfor large, complex real world networks. The key idea is that to infer the\nsource node in the network, full characterization of diffusion dynamics, in\nmany cases, may not be necessary. This objective is achieved by creating a\ndiffusion kernel that well-approximates standard diffusion models, but lends\nitself to inversion, by design, via likelihood maximization or error\nminimization. We apply NI for both single-source and multi-source diffusion,\nfor both single-snapshot and multi-snapshot observations, and for both\nhomogeneous and heterogeneous diffusion setups. We prove the mean-field\noptimality of NI for different scenarios, and demonstrate its effectiveness\nover several synthetic networks. Moreover, we apply NI to a real-data\napplication, identifying news sources in the Digg social network, and\ndemonstrate the effectiveness of NI compared to existing methods. Finally, we\npropose an integrative source inference framework that combines NI with a\ndistance centrality-based method, which leads to a robust performance in cases\nwhere the underlying dynamics are unknown.'"
Manolis Kellis,Kellis_Manolis,arXiv:1606.04789,https://arxiv.org/abs/1606.04789,"b""Abstract:  We introduce Network Maximal Correlation (NMC) as a multivariate measure of\nnonlinear association among random variables. NMC is defined via an\noptimization that infers transformations of variables by maximizing aggregate\ninner products between transformed variables. For finite discrete and jointly\nGaussian random variables, we characterize a solution of the NMC optimization\nusing basis expansion of functions over appropriate basis functions. For finite\ndiscrete variables, we propose an algorithm based on alternating conditional\nexpectation to determine NMC. Moreover we propose a distributed algorithm to\ncompute an approximation of NMC for large and dense graphs using graph\npartitioning. For finite discrete variables, we show that the probability of\ndiscrepancy greater than any given level between NMC and NMC computed using\nempirical distributions decays exponentially fast as the sample size grows. For\njointly Gaussian variables, we show that under some conditions the NMC\noptimization is an instance of the Max-Cut problem. We then illustrate an\napplication of NMC in inference of graphical model for bijective functions of\njointly Gaussian variables. Finally, we show NMC's utility in a data\napplication of learning nonlinear dependencies among genes in a cancer dataset."""
Manolis Kellis,Kellis_Manolis,arXiv:1602.04181,https://arxiv.org/abs/1602.04181,"b""Abstract:  Graph alignment refers to the problem of finding a bijective mapping across\nvertices of two graphs such that, if two nodes are connected in the first\ngraph, their images are connected in the second graph. This problem arises in\nmany fields such as computational biology, social sciences, and computer vision\nand is often cast as a quadratic assignment problem (QAP). Most standard graph\nalignment methods consider an optimization that maximizes the number of matches\nbetween the two graphs, ignoring the effect of mismatches. We propose a\ngeneralized graph alignment formulation that considers both matches and\nmismatches in a standard QAP formulation. This modification can have a major\nimpact in aligning graphs with different sizes and heterogenous edge densities.\nMoreover, we propose two methods for solving the generalized graph alignment\nproblem based on spectral decomposition of matrices. We compare the performance\nof proposed methods with some existing graph alignment algorithms including\nNatalie2, GHOST, IsoRank, NetAlign, Klau's approach as well as a semidefinite\nprogramming-based method over various synthetic and real graph models. Our\nproposed method based on simultaneous alignment of multiple eigenvectors leads\nto consistently good performance in different graph models. In particular, in\nthe alignment of regular graph structures which is one of the most difficult\ngraph alignment cases, our proposed method significantly outperforms other\nmethods."""
Manolis Kellis,Kellis_Manolis,arXiv:1507.05941,https://arxiv.org/abs/1507.05941,"b'Abstract:  Information theory is rapidly approaching its 70th birthday. What are\npromising future directions for research in information theory? Where will\ninformation theory be having the most impact in 10-20 years? What new and\nemerging areas are ripe for the most impact, of the sort that information\ntheory has had on the telecommunications industry over the last 60 years? How\nshould the IEEE Information Theory Society promote high-risk new research\ndirections and broaden the reach of information theory, while continuing to be\ntrue to its ideals and insisting on the intellectual rigor that makes its\nbreakthroughs so powerful? These are some of the questions that an ad hoc\ncommittee (composed of the present authors) explored over the past two years.\nWe have discussed and debated these questions, and solicited detailed inputs\nfrom experts in fields including genomics, biology, economics, and\nneuroscience. This report is the result of these discussions.'"
Manolis Kellis,Kellis_Manolis,arXiv:1411.6307,https://arxiv.org/abs/1411.6307,"b'Abstract:  We propose a novel diverse feature selection method based on determinantal\npoint processes (DPPs). Our model enables one to flexibly define diversity\nbased on the covariance of features (similar to orthogonal matching pursuit) or\nalternatively based on side information. We introduce our approach in the\ncontext of Bayesian sparse regression, employing a DPP as a variational\napproximation to the true spike and slab posterior distribution. We\nsubsequently show how this variational DPP approximation generalizes and\nextends mean-field approximation, and can be learned efficiently by exploiting\nthe fast sampling properties of DPPs. Our motivating application comes from\nbioinformatics, where we aim to identify a diverse set of genes whose\nexpression profiles predict a tumor type where the diversity is defined with\nrespect to a gene-gene interaction network. We also explore an application in\nspatial statistics. In both cases, we demonstrate that the proposed method\nyields significantly more diverse feature sets than classic sparse methods,\nwithout compromising accuracy.'"
Jing Kong,Kong_Jing,arXiv:1812.07111,https://arxiv.org/abs/1812.07111,"b'Abstract:  When the Fermi level matches the Dirac point in graphene, the reduced charge\nscreening can dramatically enhance electron-electron (e-e) scattering to\nproduce a strongly interacting Dirac liquid. While the dominance of e-e\nscattering already leads to novel behaviors, such as electron hydrodynamic\nflow, further exotic phenomena have been predicted to arise specifically from\nthe unique kinematics of e-e scattering in massless Dirac systems. Here, we use\noptoelectronic probes, which are highly sensitive to the kinematics of electron\nscattering, to uncover a giant intrinsic photocurrent response in pristine\ngraphene. This photocurrent emerges exclusively at the charge neutrality point\nand vanishes abruptly at non-zero charge densities. Moreover, it is observed at\nplaces with broken reflection symmetry, and it is selectively enhanced at free\ngraphene edges with sharp bends. Our findings reveal that the photocurrent\nrelaxation is strongly suppressed by a drastic change of fast photocarrier\nkinematics in graphene when its Fermi level matches the Dirac point. The\nemergence of robust photocurrents in neutral Dirac materials promises new\nenergy-harvesting functionalities and highlights intriguing electron dynamics\nin the optoelectronic response of Dirac fluids.'"
Jing Kong,Kong_Jing,arXiv:1811.12777,https://arxiv.org/abs/1811.12777,"b'Abstract:  Hexagonal boron nitride (h-BN) is a two dimensional (2D) layered insulator\nwith superior dielectric performance that offers excellent interaction with\nother 2D materials (e.g. graphene, MoS2). Large-area h-BN can be readily grown\non metallic substrates via chemical vapor deposition (CVD), but the impact of\nlocal inhomogeneities on the electrical properties of the h-BN and their effect\nin electronic devices is unknown. Here it is shown that the electrical\nproperties of h-BN stacks grown on polycrystalline Pt vary a lot depending on\nthe crystalline orientation of the Pt grain, but within the same grain the\nelectrical properties of the h-BN are very homogeneous. The reason is that the\nthickness of the CVD-grown h-BN stack is different on each Pt grain. Conductive\natomic force microscopy (CAFM) maps show that the tunneling current across the\nh-BN stack fluctuates up to 3 orders of magnitude from one Pt grain to another.\nHowever, probe station experiments reveal that the variability of electronic\ndevices fabricated within the same Pt grain is surprisingly small. As\ncutting-edge electronic devices are ultra-scaled, and as the size of the\nmetallic substrate grains can easily exceed 100 {\\mu}m (in diameter), CVD-grown\nh-BN stacks may be useful to fabricate electronic devices with low variability.'"
Jing Kong,Kong_Jing,arXiv:1807.06154,https://arxiv.org/abs/1807.06154,"b'Abstract:  Property by design is one appealing idea in material synthesis but hard to\nachieve in practice. A recent successful example is the demonstration of van\nder Waals (vdW) heterostructures,1-3 in which atomic layers are stacked on each\nother and different ingredients can be combined beyond symmetry and lattice\nmatching. This concept, usually described as a nanoscale Lego blocks, allows to\nbuild sophisticated structures layer by layer. However, this concept has been\nso far limited in two dimensional (2D) materials. Here we show a class of new\nmaterial where different layers are coaxially (instead of planarly) stacked. As\nthe structure is in one dimensional (1D) form, we name it ""1D vdW\nheterostructures"". We demonstrate a 5 nm diameter nanotube consisting of three\ndifferent materials: an inner conductive carbon nanotube (CNT), a middle\ninsulating hexagonal boron nitride nanotube (BNNT) and an outside\nsemiconducting MoS2 nanotube. As the technique is highly applicable to other\nmaterials in the current 2D libraries,4-6 we anticipate our strategy to be a\nstarting point for discovering a class of new semiconducting nanotube\nmaterials. A plethora of function-designable 1D heterostructures will appear\nafter the combination of CNTs, BNNTs and semiconducting nanotubes.'"
Jing Kong,Kong_Jing,arXiv:1806.03493,https://arxiv.org/abs/1806.03493,"b'Abstract:  Second-order nonlinear optical interactions, including second harmonic\ngeneration (SHG) and sum-frequency generation (SFG), can reveal a wealth of\ninformation about chemical, electronic, and vibrational dynamics at the\nnanoscale. Here, we demonstrate a powerful and flexible new approach, called\nphase-modulated degenerate parametric amplification (DPA). The technique, which\nallows for facile retrieval of both the amplitude and phase of the second-order\nnonlinear optical response, has many advantages over conventional or\nheterodyne-detected SHG, including the flexibility to detect the signal at\neither the second harmonic or fundamental field wavelength. We demonstrate the\ncapabilities of this approach by imaging multi-grain flakes of single-layer\nMoS2. We identify the absolute crystal orientation of each MoS2 domain and\nresolve grain boundaries with high signal contrast and sub-diffraction-limited\nspatial resolution. This robust all-optical method can be used to characterize\nstructure and dynamics in organic and inorganic systems, including biological\ntissue, soft materials, and metal and semiconductor nanostructures, and is\nparticularly well-suited for imaging in media that are absorptive or highly\nscattering to visible and ultraviolet light.'"
Jing Kong,Kong_Jing,arXiv:1804.10347,https://arxiv.org/abs/1804.10347,"b'Abstract:  We report a rare atom-like interaction between excitons in monolayer WS2,\nmeasured using ultrafast absorption spectroscopy. At increasing excitation\ndensity, the exciton resonance energy exhibits a pronounced redshift followed\nby an anomalous blueshift. Using both material-realistic computation and\nphenomenological modeling, we attribute this observation to plasma effects and\nan attraction-repulsion crossover of the exciton-exciton interaction that\nmimics the Lennard-Jones potential between atoms. Our experiment demonstrates a\nstrong analogy between excitons and atoms with respect to inter-particle\ninteraction, which holds promise to pursue the predicted liquid and crystalline\nphases of excitons in two-dimensional materials.'"
Jing Kong,Kong_Jing,arXiv:1804.01061,https://arxiv.org/abs/1804.01061,"b'Abstract:  The ability to confine light into tiny spatial dimensions is important for\napplications such as microscopy, sensing and nanoscale lasers. While plasmons\noffer an appealing avenue to confine light, Landau damping in metals imposes a\ntrade-off between optical field confinement and losses. We show that a\ngraphene-insulator-metal heterostructure can overcome that trade-off, and\ndemonstrate plasmon confinement down to the ultimate limit of the lengthscale\nof one atom. This is achieved by far-field excitation of plasmon modes squeezed\ninto an atomically thin hexagonal boron nitride dielectric h-BN spacer between\ngraphene and metal rods. A theoretical model which takes into account the\nnon-local optical response of both graphene and metal is used to describe the\nresults. These ultra-confined plasmonic modes, addressed with far-field light\nexcitation, enables a route to new regimes of ultra-strong light-matter\ninteractions.'"
Jing Kong,Kong_Jing,arXiv:1803.01369,https://arxiv.org/abs/1803.01369,"b'Abstract:  Atomic-level structural changes in materials are important but challenging to\nstudy. Here, we demonstrate the dynamics and the possibility of manipulating a\nphosphorus dopant atom in graphene using scanning transmission electron\nmicroscopy (STEM). The mechanisms of various processes are explored and\ncompared with those of other dopant species by first-principles calculations.\nThis work paves the way for designing a more precise and optimized protocol for\natomic engineering.'"
Jing Kong,Kong_Jing,arXiv:1712.07925,https://arxiv.org/abs/1712.07925,"b'Abstract:  The valley pseudospin in monolayer transition metal dichalcogenides (TMDs)\nhas been proposed as a new way to manipulate information in various\noptoelectronic devices. This relies on a large valley polarization that remains\nstable over long timescales (hundreds of ns). However, time resolved\nmeasurements report valley lifetimes of only a few ps. This has been attributed\nto mechanisms such as phonon-mediated inter-valley scattering and a precession\nof the valley psedospin through electron-hole exchange. Here we use transient\nspin grating to directly measure the valley depolarization lifetime in\nmonolayer MoSe$_{2}$. We find a fast valley decay rate that scales linearly\nwith the excitation density at different temperatures. This establishes the\npresence of strong exciton-exciton Coulomb exchange interactions enhancing the\nvalley depolarization. Our work highlights the microscopic processes inhibiting\nthe efficient use of the exciton valley pseudospin in monolayer TMDs.'"
Jing Kong,Kong_Jing,arXiv:1708.05369,https://arxiv.org/abs/1708.05369,"b'Abstract:  High-performance materials rely on small reorganization energies to\nfacilitate both charge separation and charge transport. Here, we performed DFT\ncalculations to predict small reorganization energies of rectangular silicene\nnanoclusters with hydrogen-passivated edges denoted by H-SiNC. We observe that\nacross all geometries, H-SiNCs feature large electron affinities and highly\nstabilized anionic states, indicating their potential as n-type materials. Our\nfindings suggest that fine-tuning the size of H-SiNCs along the zigzag and\narmchair directions may permit the design of novel n-type electronic materials\nand spinctronics devices that incorporate both high electron affinities and\nvery low internal reorganization energies.'"
Jing Kong,Kong_Jing,arXiv:1703.07346,https://arxiv.org/abs/1703.07346,"b'Abstract:  Coherent light-matter interaction can be used to manipulate the energy levels\nof atoms, molecules and solids. When light with frequency {\\omega} is detuned\naway from a resonance {\\omega}o, repulsion between the photon-dressed (Floquet)\nstates can lead to a shift of energy resonance. The dominant effect is the\noptical Stark shift (1/({\\omega}0-{\\omega})), but there is an additional\ncontribution from the so-called Bloch-Siegert shift (1/({\\omega}o+{\\omega})).\nAlthough it is common in atoms and molecules, the observation of Bloch-Siegert\nshift in solids has so far been limited only to artificial atoms since the\nshifts were small (<1 {\\mu}eV) and inseparable from the optical Stark shift.\nHere we observe an exceptionally large Bloch-Siegert shift (~10 meV) in\nmonolayer WS2 under infrared optical driving by virtue of the strong\nlight-matter interaction in this system. Moreover, we can disentangle the\nBloch-Siegert shift entirely from the optical Stark shift, because the two\neffects are found to obey opposite selection rules at different valleys. By\ncontrolling the light helicity, we can confine the Bloch-Siegert shift to occur\nonly at one valley, and the optical Stark shift at the other valley. Such a\nvalley-exclusive Bloch-Siegert shift allows for enhanced control over the\nvalleytronic properties in two-dimensional materials, and offers a new avenue\nto explore quantum optics in solids.'"
Jing Kong,Kong_Jing,arXiv:1703.04531,https://arxiv.org/abs/1703.04531,"b'Abstract:  Coherent optical dressing of quantum materials offers technological\nadvantages to control their electronic properties, such as the electronic\nvalley degree of freedom in monolayer transition metal dichalcogenides (TMDs).\nHere, we observe a new type of optical Stark effect in monolayer WS2, one that\nis mediated by intervalley biexcitons under the blue-detuned driving with\ncircularly polarized light. We found that such helical optical driving not only\ninduces an exciton energy downshift at the excitation valley, but also causes\nan anomalous energy upshift at the opposite valley, which is normally forbidden\nby the exciton selection rules but now made accessible through the intervalley\nbiexcitons. These findings reveal the critical, but hitherto neglected, role of\nbiexcitons to couple the two seemingly independent valleys, and to enhance the\noptical control in valleytronics.'"
Jing Kong,Kong_Jing,arXiv:1703.01666,https://arxiv.org/abs/1703.01666,"b'Abstract:  Two-dimensional (2-D) materials are of tremendous interest to integrated\nphotonics given their singular optical characteristics spanning light emission,\nmodulation, saturable absorption, and nonlinear optics. To harness their\noptical properties, these atomically thin materials are usually attached onto\nprefabricated devices via a transfer process. In this paper, we present a new\nroute for 2-D material integration with planar photonics. Central to this\napproach is the use of chalcogenide glass, a multifunctional material which can\nbe directly deposited and patterned on a wide variety of 2-D materials and can\nsimultaneously function as the light guiding medium, a gate dielectric, and a\npassivation layer for 2-D materials. Besides claiming improved fabrication\nyield and throughput compared to the traditional transfer process, our\ntechnique also enables unconventional multilayer device geometries optimally\ndesigned for enhancing light-matter interactions in the 2-D layers.\nCapitalizing on this facile integration method, we demonstrate a series of\nhigh-performance glass-on-graphene devices including ultra-broadband on-chip\npolarizers, energy-efficient thermo-optic switches, as well as graphene-based\nmid-infrared (mid-IR) waveguide-integrated photodetectors and modulators.'"
Jing Kong,Kong_Jing,arXiv:1612.05119,https://arxiv.org/abs/1612.05119,"b'Abstract:  Plasmons in graphene nanostructures show great promise for mid-infrared\napplications ranging from a few to tens of microns. However, mid-infrared\nplasmonic resonances in graphene nanostructures are usually weak and\nnarrow-banded, limiting their potential in light manipulation and detection.\nHere we investigate the coupling among graphene plasmonic nanostructures and\nfurther show that by engineering the coupling, enhancement of light-graphene\ninteraction strength and broadening of spectral width can be achieved\nsimultaneously. Leveraging the concept of coupling, we demonstrate a hybrid\n2-layer graphene nanoribbon array which shows 5 to 7% extinction within the\nentire 8 to 14 {\\mu}m (~700 to 1250 cm-1) wavelength range, covering one of the\nimportant atmosphere ""infrared transmission windows"". Such coupled hybrid\ngraphene plasmonic nanostructures may find applications in infrared sensing and\nfree-space communications.'"
Jing Kong,Kong_Jing,arXiv:1610.07646,https://arxiv.org/abs/1610.07646,"b'Abstract:  In the effort to make 2D materials-based devices smaller, faster, and more\nefficient, it is important to control charge carrier at lengths approaching the\nnanometer scale. Traditional gating techniques based on capacitive coupling\nthrough a gate dielectric cannot generate strong and uniform electric fields at\nthis scale due to divergence of the fields in dielectrics. This field\ndivergence limits the gating strength, boundary sharpness, and pitch size of\nperiodic structures, and restricts possible geometries of local gates (due to\nwire packaging), precluding certain device concepts, such as plasmonics and\ntransformation optics based on metamaterials. Here we present a new gating\nconcept based on a dielectric-free self-aligned electrolyte technique that\nallows spatially modulating charges with nanometer resolution. We employ a\ncombination of a solid-polymer electrolyte gate and an ion-impenetrable\ne-beam-defined resist mask to locally create excess charges on top of the gated\nsurface. Electrostatic simulations indicate high carrier density variations of\n$\\Delta n =10^{14}\\text{cm}^{-2}$ across a length of 10 nm at the mask\nboundaries on the surface of a 2D conductor, resulting in a sharp depletion\nregion and a strong in-plane electric field of $6\\times10^8 \\text{Vm}^{-1}$\nacross the so-created junction. We apply this technique to the 2D material\ngraphene to demonstrate the creation of tunable p-n junctions for\noptoelectronic applications. We also demonstrate the spatial versatility and\nself-aligned properties of this technique by introducing a novel graphene\nthermopile photodetector.'"
Jing Kong,Kong_Jing,arXiv:1601.02265,https://arxiv.org/abs/1601.02265,"b'Abstract:  Ultrafast electron thermalization - the process leading to Auger\nrecombination, carrier multiplication via impact ionization and hot carrier\nluminescence - occurs when optically excited electrons in a material undergo\nrapid electron-electron scattering to redistribute excess energy and reach\nelectronic thermal equilibrium. Due to extremely short time and length scales,\nthe measurement and manipulation of electron thermalization in nanoscale\ndevices remains challenging even with the most advanced ultrafast laser\ntechniques. Here, we overcome this challenge by leveraging the atomic thinness\nof two-dimensional van der Waals (vdW) materials in order to introduce a highly\ntunable electron transfer pathway that directly competes with electron\nthermalization. We realize this scheme in a graphene-boron nitride-graphene\n(G-BN-G) vdW heterostructure, through which optically excited carriers are\ntransported from one graphene layer to the other. By applying an interlayer\nbias voltage or varying the excitation photon energy, interlayer carrier\ntransport can be controlled to occur faster or slower than the intralayer\nscattering events, thus effectively tuning the electron thermalization pathways\nin graphene. Our findings, which demonstrate a novel means to probe and\ndirectly modulate electron energy transport in nanoscale materials, represent\nan important step toward designing and implementing novel optoelectronic and\nenergy-harvesting devices with tailored microscopic properties.'"
Jing Kong,Kong_Jing,arXiv:1512.04492,https://arxiv.org/abs/1512.04492,"b'Abstract:  Diverse parallel stitched two-dimensional heterostructures are synthesized,\nincluding metal-semiconductor (graphene-MoS2), semiconductor-semiconductor\n(WS2-MoS2), and insulator-semiconductor (hBN-MoS2), directly through selective\nsowing of aromatic molecules as the seeds in chemical vapor deposition (CVD)\nmethod. Our methodology enables the large-scale fabrication of lateral\nheterostructures with arbitrary patterns, and clean and precisely aligned\ninterfaces, which offers tremendous potential for its application in integrated\ncircuits.'"
Jing Kong,Kong_Jing,arXiv:1502.07804,https://arxiv.org/abs/1502.07804,"b'Abstract:  As a new two-dimensional layered material, black phosphorus (BP) is a\npromising material for nanoelectronics and nano-optoelectronics. We use Raman\nspectroscopy and first-principles theory to report our findings related to\nlow-frequency (LF) interlayer breathing modes (<100 cm-1) in few-layer BP for\nthe first time. The breathing modes are assigned to Ag symmetry by the laser\npolarization dependence study and group theory analysis. Compared to the\nhigh-frequency (HF) Raman modes, the LF breathing modes are much more sensitive\nto interlayer coupling and thus their frequencies show much stronger dependence\non the number of layers. Hence, they could be used as effective means to probe\nboth the crystalline orientation and thickness for few-layer BP. Furthermore,\nthe temperature dependence study shows that the breathing modes have a harmonic\nbehavior, in contrast to HF Raman modes which are known to exhibit\nanharmonicity.'"
Jing Kong,Kong_Jing,arXiv:1410.6750,https://arxiv.org/abs/1410.6750,"b'Abstract:  A single-term density functional model for nondynamic and strong correlation\nis presented, based on single-determinant Kohn-Sham density functional theory.\nIt is derived from modeling the adiabatic connection and contains only two\nnonlinear empirical parameters. Preliminary tests show that the model recovers\nmajority of nondynamic correlation during a molecular dissociation and at the\nsame time performs reasonably for atomization energies. It demonstrates the\nfeasibility of developing DFT functionals for nondynamic and strong correlation\nwithin the single-determinant KS scheme.'"
Jing Kong,Kong_Jing,arXiv:1407.7297,https://arxiv.org/abs/1407.7297,"b'Abstract:  Variable selection is of increasing importance to address the difficulties of\nhigh dimensionality in many scientific areas. In this paper, we demonstrate a\nproperty for distance covariance, which is incorporated in a novel feature\nscreening procedure together with the use of distance correlation. The approach\nmakes no distributional assumptions for the variables and does not require the\nspecification of a regression model, and hence is especially attractive in\nvariable selection given an enormous number of candidate attributes without\nmuch information about the true model with the response. The method is applied\nto two genetic risk problems, where issues including uncertainty of variable\nselection via cross validation, subgroup of hard-to-classify cases and the\napplication of a reject option are discussed.'"
Jing Kong,Kong_Jing,arXiv:1407.6997,https://arxiv.org/abs/1407.6997,"b'Abstract:  Recently emerging large-area single-layer MoS2 grown by chemical vapor\ndeposition has triggered great interest due to its exciting potential for\napplications in advanced electronic and optoelectronic devices. Unlike gapless\ngraphene, MoS2 has an intrinsic band gap in the visible which crosses over from\nan indirect to a direct gap when reduced to a single atomic layer. In this\narticle, we report a comprehensive study of fundamental optical properties of\nMoS2 revealed by optical spectroscopy of Raman, photoluminescence, and vacuum\nultraviolet spectroscopic ellipsometry. A band gap of 1.42 eV is determined by\nthe absorption threshold of bulk MoS2 that shifts to 1.83 eV in monolayer MoS2.\nWe extracted the high precision dielectric function up to 9.0 eV which leads to\nthe identification of many unique interband transitions at high symmetry points\nin the MoS2 momentum space. The positions of the A and B excitons in single\nlayers are found to shift upwards in energy compared with those of the bulk\nform and have smaller separation. A very strong optical critical point\npredicted to correspond to a quasi-particle gap is observed at 2.86 eV, which\nis attributed to optical transitions along the parallel bands between the M and\ngama points in the reduced Brillouin zone. The absence of the bulk MoS2\nspin-orbit interaction peak at ~ 3.0 eV in monolayer MoS2 is, as predicted, the\nconsequence of the coalescence of nearby excitons. A higher energy optical\ntransition at 3.98 eV, commonly occurred in bulk semiconductors, is associated\nwith a combination of several critical points.These optical transitions herein\nreported enhance our understanding of monolayer MoS2 as well as of\ntwo-dimensional systems in general, and thus provide informative guidelines for\nMoS2 optical device designs and theoretical considerations.'"
Jing Kong,Kong_Jing,arXiv:1407.1825,https://arxiv.org/abs/1407.1825,"b'Abstract:  Breaking space-time symmetries in two-dimensional crystals (2D) can\ndramatically influence their macroscopic electronic properties. Monolayer\ntransition-metal dichalcogenides (TMDs) are prime examples where the\nintrinsically broken crystal inversion symmetry permits the generation of\nvalley-selective electron populations, even though the two valleys are\nenergetically degenerate, locked by time-reversal symmetry. Lifting the valley\ndegeneracy in these materials is of great interest because it would allow for\nvalley-specific band engineering and offer additional control in valleytronic\napplications. While applying a magnetic field should in principle accomplish\nthis task, experiments to date have observed no valley-selective energy level\nshifts in fields accessible in the laboratory. Here we show the first direct\nevidence of lifted valley degeneracy in the monolayer TMD WS2. By applying\nintense circularly polarized light, which breaks time-reversal symmetry, we\ndemonstrate that the exciton level in each valley can be selectively tuned by\nas much as 18 meV via the optical Stark effect. These results offer a novel way\nto control valley degree of freedom, and may provide a means to realize new\nvalley-selective Floquet topological phases in 2D TMDs.'"
Jing Kong,Kong_Jing,arXiv:1401.4951,https://arxiv.org/abs/1401.4951,"b'Abstract:  Layered transition metal dichalcogenides display a wide range of attractive\nphysical and chemical properties and are potentially important for various\ndevice applications. Here we report the electronic transport and device\nproperties of monolayer molybdenum disulphide (MoS2) grown by chemical vapour\ndeposition (CVD). We show that these devices have the potential to suppress\nshort channel effects and have high critical breakdown electric field. However,\nour study reveals that the electronic properties of these devices are at\npresent, severely limited by the presence of a significant amount of band tail\ntrapping states. Through capacitance and ac conductance measurements, we\nsystematically quantify the density-of-states and response time of these\nstates. Due to the large amount of trapped charges, the measured effective\nmobility also leads to a large underestimation of the true band mobility and\nthe potential of the material. Continual engineering efforts on improving the\nsample quality are needed for its potential applications.'"
Jing Kong,Kong_Jing,arXiv:1312.2918,https://arxiv.org/abs/1312.2918,"b'Abstract:  Interactions between two excitons can result in the formation of bound\nquasiparticles, known as biexcitons. Their properties are determined by the\nconstituent excitons, with orbital and spin states resembling those of atoms.\nMonolayer transition metal dichalcogenides (TMDs) present a unique system where\nexcitons acquire a new degree of freedom, the valley pseudospin, from which a\nnovel intervalley biexciton can be created. These biexcitons comprise two\nexcitons from different valleys, which are distinct from biexcitons in\nconventional semiconductors and have no direct analogue in atomic and molecular\nsystems. However, their valley properties are not accessible to traditional\ntransport and optical measurements. Here, we report the observation of\nintervalley biexcitons in the monolayer TMD MoS2 using ultrafast pump-probe\nspectroscopy. By applying broadband probe pulses with different helicities, we\nidentify two species of intervalley biexcitons with large binding energies of\n60 meV and 40 meV. In addition, we also reveal effects beyond biexcitonic\npairwise interactions in which the exciton energy redshifts at increasing\nexciton densities, indicating the presence of many-body interactions among\nthem.'"
Jing Kong,Kong_Jing,arXiv:1305.5890,https://arxiv.org/abs/1305.5890,"b'Abstract:  Due to their exceptional mechanical and optical properties, dielectric\nsilicon nitride (SiN) micromembrane resonators have become the centerpiece of\nmany optomechanical experiments. Efficient capacitive coupling of the membrane\nto an electrical system would facilitate exciting hybrid optoelectromechanical\ndevices. However, capacitive coupling of such dielectric membranes is rather\nweak. Here we add a single layer of graphene on SiN micromembranes and compare\nelectromechanical coupling and mechanical properties to bare dielectric\nmembranes and to membranes metallized with an aluminium layer. The\nelectrostatic coupling of graphene coated membranes is found to be equal to a\nperfectly conductive membrane. Our results show that a single layer of graphene\nsubstantially enhances the electromechanical capacitive coupling without\nsignificantly adding mass, decreasing the superior mechanical quality factor or\naffecting the optical properties of SiN micromembrane resonators.'"
Jing Kong,Kong_Jing,arXiv:1302.4027,https://arxiv.org/abs/1302.4027,b'Abstract:  2D nanoelectronics based on single-layer MoS2 offers great advantages for\nboth conventional and ubiquitous applications. This paper discusses the\nlarge-scale CVD growth of single-layer MoS2 and fabrication of devices and\ncircuits for the first time. Both digital and analog circuits are fabricated to\ndemonstrate its capability for mixed-signal applications.'
Jing Kong,Kong_Jing,arXiv:1208.1078,https://arxiv.org/abs/1208.1078,"b""Abstract:  Two-dimensional (2D) materials, such as molybdenum disulfide (MoS2), have\nbeen shown to exhibit excellent electrical and optical properties. The\nsemiconducting nature of MoS2 allows it to overcome the shortcomings of\nzero-bandgap graphene, while still sharing many of graphene's advantages for\nelectronic and optoelectronic applications. Discrete electronic and\noptoelectronic components, such as field-effect transistors, sensors and\nphotodetectors made from few-layer MoS2 show promising performance as potential\nsubstitute of Si in conventional electronics and of organic and amorphous Si\nsemiconductors in ubiquitous systems and display applications. An important\nnext step is the fabrication of fully integrated multi-stage circuits and logic\nbuilding blocks on MoS2 to demonstrate its capability for complex digital logic\nand high-frequency ac applications. This paper demonstrates an inverter, a NAND\ngate, a static random access memory, and a five-stage ring oscillator based on\na direct-coupled transistor logic technology. The circuits comprise between two\nto twelve transistors seamlessly integrated side-by-side on a single sheet of\nbilayer MoS2. Both enhancement-mode and depletion-mode transistors were\nfabricated thanks to the use of gate metals with different work functions."""
Jing Kong,Kong_Jing,arXiv:1207.3369,https://arxiv.org/abs/1207.3369,"b'Abstract:  The chemical functionalization of graphene enables control over electronic\nproperties and sensor recognition sites. However, its study is confounded by an\nunusually strong influence of the underlying substrate. In this paper, we show\na stark difference in the rate of electron transfer chemistry with aryl\ndiazonium salts on monolayer graphene supported on a broad range of substrates.\nReactions proceed rapidly when graphene is on SiO_2 and Al_2O_3 (sapphire), but\nnegligibly on alkyl-terminated and hexagonal boron nitride (hBN) surfaces. The\neffect is contrary to expectations based on doping levels and can instead be\ndescribed using a reactivity model accounting for substrate-induced\nelectron-hole puddles in graphene. Raman spectroscopic mapping is used to\ncharacterize the effect of the substrates on graphene. Reactivity imprint\nlithography (RIL) is demonstrated as a technique for spatially patterning\nchemical groups on graphene by patterning the underlying substrate, and is\napplied to the covalent tethering of proteins on graphene.'"
Jing Kong,Kong_Jing,arXiv:1112.4831,https://arxiv.org/abs/1112.4831,"b'Abstract:  In this letter, we analyze the carrier transit delay in graphene field-effect\ntransistors (GFETs). GFETs are fabricated at the wafer-scale on sapphire\nsubstrate. For a device with a gate length of 210 nm, a current gain cut-off\nfrequency fT of 18 GHz and 22 GHz is obtained before and after de-embedding.\nThe extraction of the internal (Cgs,i, Cgd,i) and external capacitances (Cgs,ex\nand Cgd,ex) from the scaling behavior of the gate capacitances Cgs and Cgd\nallows the intrinsic ({\\tau}_int), extrinsic ({\\tau}_ext) and parasitic delays\n({\\tau}_par) to be obtained. In addition, the extraction of the intrinsic delay\nprovides a new way to directly estimate carrier velocity from the experimental\ndata while the breakdown of the total delay into intrinsic, extrinsic, and\nparasitic components can offer valuable information for optimizing RF GFETs\nstructures.'"
Jing Kong,Kong_Jing,arXiv:1110.4356,https://arxiv.org/abs/1110.4356,b'Abstract:  We present experimental measurements of the electronic contribution to the\nRaman spectra of individual metallic single-walled carbon nanotubes (MSWNTs).\nPhotoexcited carriers are inelastically scattered by a continuum of low-energy\nelectron-hole pairs created across the graphenelike linear electronic subbands\nof the MSWNTs. The optical resonances in MSWNTs give rise to well-defined\nelectronic Raman peaks. This resonant electronic Raman scattering is a unique\nfeature of the electronic structure of these one-dimensional quasimetals.'
Jing Kong,Kong_Jing,arXiv:1101.4985,https://arxiv.org/abs/1101.4985,b'Abstract:  We report strong THz-induced transparency in CVD-grown graphene where 92%-96%\nof the peak-field is transmitted compared to 74% at lower field strength.\nTime-resolved THz-pump/THz-probe studies reveal that the absorption recovers in\n2-3 ps. The induced transparency is believed to arise from nonlinear pumping of\ncarriers in graphene which suppresses the mobility and consequently the\nconductivity in a spectral region where the light-matter interaction is\nparticularly strong.'
Jing Kong,Kong_Jing,arXiv:0906.4037,https://arxiv.org/abs/0906.4037,"b'Abstract:  We demonstrate anisotropic etching of single-layer graphene by\nthermally-activated nickel nanoparticles. Using this technique, we obtain\nsub-10nm nanoribbons and other graphene nanostructures with edges aligned along\na single crystallographic direction. We observe a new catalytic channeling\nbehavior, whereby etched cuts do not intersect, resulting in continuously\nconnected geometries. Raman spectroscopy and electronic measurements show that\nthe quality of the graphene is resilient under the etching conditions,\nindicating that this method may serve as a powerful technique to produce\ngraphene nanocircuits with well-defined crystallographic edges.'"
Jing Kong,Kong_Jing,arXiv:0906.2236,https://arxiv.org/abs/0906.2236,"b'Abstract:  We report graphene films composed mostly of one or two layers of graphene\ngrown by controlled carbon precipitation on the surface of polycrystalline Ni\nthin films during atmospheric chemical vapor deposition(CVD). Controlling both\nthe methane concentration during CVD and the substrate cooling rate during\ngraphene growth can significantly improve the thickness uniformity. As a\nresult, one- or two- layer graphene regions occupy up to 87% of the film area.\nSingle layer coverage accounts for 5-11% of the overall film. These regions\nexpand across multiple grain boundaries of the underlying polycrystalline Ni\nfilm. The number density of sites with multilayer graphene/graphite (>2 layers)\nis reduced as the cooling rate decreases. These films can also be transferred\nto other substrates and their sizes are only limited by the sizes of the Ni\nfilm and the CVD chamber. Here, we demonstrate the formation of films as large\nas 1 in2. These findings represent an important step towards the fabrication of\nlarge-scale high-quality graphene samples.'"
Jing Kong,Kong_Jing,arXiv:cond-mat/0610196,https://arxiv.org/abs/cond-mat/0610196,"b'Abstract:  We study the electrical transport properties of well-contacted ballistic\nsingle-walled carbon nanotubes in a three-terminal configuration at low\ntemperatures. We observe signatures of strong electron-electron interactions:\nthe conductance exhibits bias-voltage-dependent amplitudes of quantum\ninterference oscillation, and both the current noise and Fano factor manifest\nbias-voltage-dependent power-law scalings. We analyze our data within the\nTomonaga-Luttinger liquid model using the non-equilibrium Keldysh formalism and\nfind qualitative and quantitative agreement between experiment and theory.'"
Jing Kong,Kong_Jing,arXiv:cond-mat/0504059,https://arxiv.org/abs/cond-mat/0504059,"b'Abstract:  Progress in the fabrication of nanometer-scale electronic devices is opening\nnew opportunities to uncover the deepest aspects of the Kondo effect, one of\nthe paradigmatic phenomena in the physics of strongly correlated electrons.\nArtificial single-impurity Kondo systems have been realized in various\nnanostructures, including semiconductor quantum dots, carbon nanotubes and\nindividual molecules. The Kondo effect is usually regarded as a spin-related\nphenomenon, namely the coherent exchange of the spin between a localized state\nand a Fermi sea of electrons. In principle, however, the role of the spin could\nbe replaced by other degrees of freedom, such as an orbital quantum number.\nHere we demonstrate that the unique electronic structure of carbon nanotubes\nenables the observation of a purely orbital Kondo effect. We use a magnetic\nfield to tune spin-polarized states into orbital degeneracy and conclude that\nthe orbital quantum number is conserved during tunneling. When orbital and spin\ndegeneracies are simultaneously present, we observe a strongly enhanced Kondo\neffect, with a multiple splitting of the Kondo resonance at finite field and\npredicted to obey a so-called SU(4) symmetry.'"
Jing Kong,Kong_Jing,arXiv:cond-mat/0309044,https://arxiv.org/abs/cond-mat/0309044,"b'Abstract:  Contacting metallic single-walled carbon nanotubes by palladium (Pd) affords\nhighly reproducible ohmic contacts and allows for detailed elucidation of\nballistic transport in metallic nanotubes. The Pd ohmic contacts are more\nreliable than titanium (Ti) previously used for ballistic nanotube devices. In\ncontrast, Pt contacts appear to give non-ohmic contacts to metallic nanotubes.\nFor both ohmic and non-ohmic contacts, the length of the nanotube under the\nmetal contact area is electrically turned off. Transport occurs from metal to\nnanotube at the edges of the contacts. Measurements with large numbers of Pd\ncontacted nanotube samples reveal that the mean free path for defect scattering\nin SWNTs grown by chemical vapor deposition can be up to 4 microns. The mean\nfree paths for acoustic phonon scattering are on the order of 500 nm at room\ntemperature and >> 4 microns at low temperatures.'"
Butler Lampson,Lampson_Butler,arXiv:1209.3811,https://arxiv.org/abs/1209.3811,"b'Abstract:  In Programming by Example, a system attempts to infer a program from input\nand output examples, generally by searching for a composition of certain base\nfunctions. Performing a naive brute force search is infeasible for even mildly\ninvolved tasks. We note that the examples themselves often present clues as to\nwhich functions to compose, and how to rank the resulting programs. In text\nprocessing, which is our domain of interest, clues arise from simple textual\nfeatures: for example, if parts of the input and output strings are\npermutations of one another, this suggests that sorting may be useful. We\ndescribe a system that learns the reliability of such clues, allowing for\nfaster search and a principled ranking over programs. Experiments on a\nprototype of this system show that this learning scheme facilitates efficient\ninference on a range of text processing tasks.'"
Jeffrey Lang,Lang_Jeffrey,arXiv:1603.02157,https://arxiv.org/abs/1603.02157,"b'Abstract:  Spatially-dense pressure measurements are needed on curved surfaces in marine\nenvironments to provide marine vehicles with the detailed, real-time\nmeasurements of the near-field flow necessary to improve performance through\nflow control. To address this challenge, a waterproof and conformal pressure\nsensor array comprising carbon black-doped-silicone closed-cell foam (CBPDMS\nfoam) was developed for use in marine applications. The response of the CBPDMS\nfoam sensor arrays was characterized using periodic hydrodynamic pressure\nstimuli from vertical plunging, from which a piecewise polynomial calibration\nwas developed to describe the sensor response. Inspired by the distributed\npressure and velocity sensing capabilities of the fish lateral line, the CBPDMS\nfoam sensor arrays have significant advantages over existing commercial sensors\nfor distributed flow reconstruction and control. Experimental results have\nshown the sensor arrays to have sensitivity on the order of 5 Pascal, dynamic\nrange of 50-500 Pascal; are contained in a waterproof and completely flexible\npackage, and have material cost less than $10 per sensor.'"
Charles Leiserson,Leiserson_Charles,arXiv:1812.00076,https://arxiv.org/abs/1812.00076,"b'Abstract:  Organized crime inflicts human suffering on a genocidal scale: the Mexican\ndrug cartels have murdered 150,000 people since 2006, upwards of 700,000 people\nper year are ""exported"" in a human trafficking industry enslaving an estimated\n40 million people. These nefarious industries rely on sophisticated money\nlaundering schemes to operate. Despite tremendous resources dedicated to\nanti-money laundering (AML) only a tiny fraction of illicit activity is\nprevented. The research community can help. In this brief paper, we map the\nstructural and behavioral dynamics driving the technical challenge. We review\nAML methods, current and emergent. We provide a first look at scalable graph\nconvolutional neural networks for forensic analysis of financial data, which is\nmassive, dense, and dynamic. We report preliminary experimental results using a\nlarge synthetic graph (1M nodes, 9M edges) generated by a data simulator we\ncreated called AMLSim. We consider opportunities for high performance\nefficiency, in terms of computation and memory, and we share results from a\nsimple graph compression experiment. Our results support our working hypothesis\nthat graph deep learning for AML bears great promise in the fight against\ncriminal financial activity.'"
Charles Leiserson,Leiserson_Charles,arXiv:1804.04773,https://arxiv.org/abs/1804.04773,"b'Abstract:  This paper investigates a variant of the work-stealing algorithm that we call\nthe localized work-stealing algorithm. The intuition behind this variant is\nthat because of locality, processors can benefit from working on their own\nwork. Consequently, when a processor is free, it makes a steal attempt to get\nback its own work. We call this type of steal a steal-back. We show that the\nexpected running time of the algorithm is $T_1/P+O(T_\\infty P)$, and that under\nthe ""even distribution of free agents assumption"", the expected running time of\nthe algorithm is $T_1/P+O(T_\\infty\\lg P)$. In addition, we obtain another\nrunning-time bound based on ratios between the sizes of serial tasks in the\ncomputation. If $M$ denotes the maximum ratio between the largest and the\nsmallest serial tasks of a processor after removing a total of $O(P)$ serial\ntasks across all processors from consideration, then the expected running time\nof the algorithm is $T_1/P+O(T_\\infty M)$.'"
Charles Leiserson,Leiserson_Charles,arXiv:1706.03184,https://arxiv.org/abs/1706.03184,"b'Abstract:  Inspired by applications in parallel computing, we analyze the setting of\nwork stealing in multithreaded computations. We obtain tight upper bounds on\nthe number of steals when the computation can be modeled by rooted trees. In\nparticular, we show that if the computation with $n$ processors starts with one\nprocessor having a complete $k$-ary tree of height $h$ (and the remaining $n-1$\nprocessors having nothing), the maximum possible number of steals is\n$\\sum_{i=1}^n(k-1)^i\\binom{h}{i}$.'"
Charles Leiserson,Leiserson_Charles,arXiv:1408.0393,https://arxiv.org/abs/1408.0393,b'Abstract:  It is our view that the state of the art in constructing a large collection\nof graph algorithms in terms of linear algebraic operations is mature enough to\nsupport the emergence of a standard set of primitive building blocks. This\npaper is a position paper defining the problem and announcing our intention to\nlaunch an open effort to define this standard.'
Jae Lim,Lim_Jae,arXiv:1804.07204,https://arxiv.org/abs/1804.07204,"b'Abstract:  Public safety networks avail to disseminate information during emergency\nsituations through its dedicated servers. Public safety networks accommodate\npublic safety communication (PSC) applications to track the location of its\nutilizers and enable to sustain transmissions even in the crucial scenarios.\nDespite that, if the traditional setups responsible for PSCs are unavailable,\nit becomes prodigiously arduous to handle any of the safety applications, which\nmay cause havoc in the society. Dependence on a secondary network may assist to\nsolve such an issue. But, the secondary networks should be facilely deployable\nand must not cause exorbitant overheads in terms of cost and operation. For\nthis, LoRaWAN can be considered as an ideal solution as it provides low power\nand long-range communication. However, an excessive utilization of the\nsecondary network may result in high depletion of its own resources and can\nlead to a complete shutdown of services, which is a quandary at hand. As a\nsolution, this paper proposes a novel network model via a combination of\nLoRaWAN and traditional public safety networks, and uses a self-enforcing\nagreement based game theory for allocating resources efficiently amongst the\navailable servers. The proposed approach adopts memory and energy constraints\nas agreements, which are satisfied through Nash equilibrium. The numerical\nresults show that the proposed approach is capable of efficiently allocating\nthe resources with sufficiently high gains for resource conservation, network\nsustainability, resource restorations and probability to continue at the\npresent conditions even in the complete absence of traditional Access Points\n(APs) compared with a baseline scenario with no failure of nodes.'"
Jae Lim,Lim_Jae,arXiv:1705.02894,https://arxiv.org/abs/1705.02894,"b'Abstract:  Generative Adversarial Nets (GANs) represent an important milestone for\neffective generative models, which has inspired numerous variants seemingly\ndifferent from each other. One of the main contributions of this paper is to\nreveal a unified geometric structure in GAN and its variants. Specifically, we\nshow that the adversarial generative model training can be decomposed into\nthree geometric steps: separating hyperplane search, discriminator parameter\nupdate away from the separating hyperplane, and the generator update along the\nnormal vector direction of the separating hyperplane. This geometric intuition\nreveals the limitations of the existing approaches and leads us to propose a\nnew formulation called geometric GAN using SVM separating hyperplane that\nmaximizes the margin. Our theoretical analysis shows that the geometric GAN\nconverges to a Nash equilibrium between the discriminator and generator. In\naddition, extensive numerical results show that the superior performance of\ngeometric GAN.'"
Jae Lim,Lim_Jae,arXiv:1108.4315,https://arxiv.org/abs/1108.4315,"b'Abstract:  Detecting the edges of objects within images is critical for quality image\nprocessing. We present an edge-detecting technique that uses morphological\namoebas that adjust their shape based on variation in image contours. We\nevaluate the method both quantitatively and qualitatively for edge detection of\nimages, and compare it to classic morphological methods. Our amoeba-based\nedge-detection system performed better than the classic edge detectors.'"
Barbara Liskov,Liskov_Barbara,arXiv:1811.04967,https://arxiv.org/abs/1811.04967,"b""Abstract:  Optimistic concurrency control (OCC) can exploit the strengths of parallel\nhardware to provide excellent performance for uncontended transactions, and is\npopular in high-performance in-memory databases and transactional systems. But\nat high contention levels, OCC is susceptible to frequent aborts, leading to\nwasted work and degraded performance. Contention managers, mixed\noptimistic/pessimistic concurrency control algorithms, and novel\noptimistic-inspired concurrency control algorithms, such as TicToc, aim to\naddress this problem, but these mechanisms introduce sometimes-high overheads\nof their own. We show that in real-world benchmarks, traditional OCC can\noutperform these alternative mechanisms by simply adding fine-grained version\ntimestamps (using different timestamps for disjoint components of each record).\nWith fine-grained timestamps, OCC gets 1.14x TicToc's throughput in TPC-C at\n128 cores (previous work reported TicToc having 1.8x higher throughput than OCC\nat 80 hyperthreads). Our study shows that timestamp granularity has a greater\nimpact than previously thought on the performance of transaction processing\nsystems, and should not be overlooked in the push for faster concurrency\ncontrol schemes."""
Luqiao Liu,Liu_Luqiao,arXiv:1903.01887,https://arxiv.org/abs/1903.01887,"b'Abstract:  Coupled microwave photon-magnon hybrid systems offer promising applications\nby harnessing various magnon physics. At present, in order to realize high\ncoupling strength between the two subsystems, bulky ferromagnets with large\nspin numbers are utilized, which limits their potential applications for\nscalable quantum information processing. In this paper, by enhancing single\nspin coupling strength using lithographically defined superconducting\nresonators, we report high cooperativities between a resonator mode and a\nKittel mode in nanometer thick Permalloy wires. The on-chip, lithographically\nscalable, and superconducting quantum circuit compatible design provides a\ndirect route towards realizing hybrid quantum systems with nanomagnets, whose\ncoupling strength can be precisely engineered and dynamic properties can be\ncontrolled by various mechanisms derived from spintronic studies.'"
Luqiao Liu,Liu_Luqiao,arXiv:1902.05669,https://arxiv.org/abs/1902.05669,"b'Abstract:  We report broadband microwave absorption spectroscopy of the layered\nantiferromagnet CrCl3. We observe a rich structure of resonances arising from\nquasi-two-dimensional antiferromagnetic dynamics. Due to the weak interlayer\nmagnetic coupling in this material, we are able to observe both optical and\nacoustic branches of antiferromagnetic resonance in the GHz frequency range and\na symmetry-protected crossing between them. By breaking rotational symmetry, we\nfurther show that strong magnon-magnon coupling with large tunable gaps can be\ninduced between the two resonant modes.'"
Luqiao Liu,Liu_Luqiao,arXiv:1806.01167,https://arxiv.org/abs/1806.01167,"b'Abstract:  Due to the difficulty in detecting and manipulating magnetic states of\nantiferromagnetic materials, studying their switching dynamics using electrical\nmethods remains a challenging task. In this work, by employing heavy metal/rare\nearth-transition metal alloy bilayers, we experimentally studied\ncurrent-induced domain wall dynamics in an antiferromagnetically coupled\nsystem. We show that the current-induced domain wall mobility reaches a maximum\nclose to the angular momentum compensation. With experiment and modelling, we\nfurther reveal the internal structures of domain walls and the underlying\nmechanisms for their fast motion. We show that the chirality of the\nferrimagnetic domain walls remains the same across the compensation points,\nsuggesting that spin orientations of specific sublattices rather than net\nmagnetization determine Dzyaloshinskii-Moriya interaction in heavy\nmetal/ferrimagnet bilayers. The high current-induced domain wall mobility and\nthe robust domain wall chirality in compensated ferrimagnetic material opens\nnew opportunities for high-speed spintronic devices.'"
Luqiao Liu,Liu_Luqiao,arXiv:1703.07470,https://arxiv.org/abs/1703.07470,"b'Abstract:  Recent studies on the magneto-transport properties of topological insulators\n(TI) have attracted great attention due to the rich spin-orbit physics and\npromising applications in spintronic devices. Particularly the strongly\nspin-moment coupled electronic states have been extensively pursued to realize\nefficient spin-orbit torque (SOT) switching. However, so far current-induced\nmagnetic switching with TI has only been observed at cryogenic temperatures. It\nremains a controversial issue whether the topologically protected electronic\nstates in TI could benefit spintronic applications at room temperature. In this\nwork, we report full SOT switching in a TI/ferromagnet bilayer heterostructure\nwith perpendicular magnetic anisotropy at room temperature. The low switching\ncurrent density provides a definitive proof on the high SOT efficiency from TI.\nThe effective spin Hall angle of TI is determined to be several times larger\nthan commonly used heavy metals. Our results demonstrate the robustness of TI\nas an SOT switching material and provide a direct avenue towards applicable\nTI-based spintronic devices.'"
Luqiao Liu,Liu_Luqiao,arXiv:1610.09200,https://arxiv.org/abs/1610.09200,"b'Abstract:  Despite the potential advantages of information storage in\nantiferromagnetically coupled materials, it remains unclear whether one can\ncontrol the magnetic moment orientation efficiently because of the cancelled\nmagnetic moment. Here, we report spin-orbit torque induced magnetization\nswitching of ferrimagnetic Co1-xTbx films with perpendicular magnetic\nanisotropy. Current induced switching is demonstrated in all of the studied\nfilm compositions, including those near the magnetization compensation point.\nThe spin-orbit torque induced effective field is further quantified in the\ndomain wall motion regime. A divergent behavior that scales with the inverse of\nmagnetic moment is confirmed close to the compensation point, which is\nconsistent with angular momentum conservation. Moreover, we also quantify the\nDzyaloshinskii-Moriya interaction energy in the Ta/Co1-xTbx system and we find\nthat the energy density increases as a function of the Tb concentration. The\ndemonstrated spin-orbit torque switching, in combination with the fast magnetic\ndynamics and minimal net magnetization of ferrimagnetic alloys, promises\nspintronic devices that are faster and with higher density than traditional\nferromagnetic systems.'"
Luqiao Liu,Liu_Luqiao,arXiv:1410.7494,https://arxiv.org/abs/1410.7494,"b'Abstract:  We demonstrate that the charge-spin conversion efficiency of topological\ninsulators (TI) can be experimentally determined by injecting spin-polarized\ntunneling electrons into a TI. Through a comparative study between bismuth\nselenide and bismuth antimony telluride, we verified the\ntopological-surface-state origin of the observed giant spin signals. By\ninjecting energetic electrons into bismuth selenide, we further studied the\nenergy dependence of the effective spin polarization at the TI surface. The\nexperimentally verified large spin polarization, as well as our calculations,\nprovides new insights into optimizing TI materials for near room-temperature\nspintronic applications.'"
Luqiao Liu,Liu_Luqiao,arXiv:1209.0962,https://arxiv.org/abs/1209.0962,"b'Abstract:  Two promising strategies for achieving efficient control of magnetization in\nfuture magnetic memory and non-volatile spin logic devices are spin transfer\ntorque from spin polarized currents and voltage-controlled magnetic anisotropy\n(VCMA). Spin transfer torque is in widespread development as the write\nmechanism for next-generation magnetic memory, while VCMA offers the potential\nof even better energy performance due to smaller Ohmic losses. Here we\nintroduce a 3-terminal magnetic tunnel junction (MTJ) device that combines both\nof these mechanisms to achieve new functionality: gate-voltage-modulated spin\ntorque switching. This gating makes possible both more energy-efficient\nswitching and also improved architectures for memory and logic applications,\nincluding a simple approach for making magnetic memories with a maximum-density\ncross-point geometry that does not require a control transistor for every MTJ.'"
Luqiao Liu,Liu_Luqiao,arXiv:1209.0655,https://arxiv.org/abs/1209.0655,"b'Abstract:  We show that direct current in a tantalum microstrip can induce steady-state\nmagnetic oscillations in an adjacent nanomagnet through spin torque from the\nspin Hall effect (SHE). The oscillations are detected electrically via a\nmagnetic tunnel junction (MTJ) contacting the nanomagnet. The oscillation\nfrequency can be controlled using the MTJ bias to tune the magnetic anisotropy.\nIn this 3-terminal device the SHE torque and the MTJ bias therefore provide\nindependent controls of the oscillation amplitude and frequency, enabling new\napproaches for developing tunable spin torque nano-oscillators.'"
Luqiao Liu,Liu_Luqiao,arXiv:1208.1711,https://arxiv.org/abs/1208.1711,"b'Abstract:  We report a giant spin Hall effect (SHE) in {\\beta}-W thin films. Using spin\ntorque induced ferromagnetic resonance with a {\\beta}-W/CoFeB bilayer\nmicrostrip we determine the spin Hall angle to be |\\theta|=0.30\\pm0.02, large\nenough for an in-plane current to efficiently reverse the orientation of an\nin-plane magnetized CoFeB free layer of a nanoscale magnetic tunnel junction\nadjacent to a thin {\\beta}-W layer. From switching data obtained with such\n3-terminal devices we independently determine |\\theta|=0.33\\pm0.06. We also\nreport variation of the spin Hall switching efficiency with W layers of\ndifferent resistivities and hence of variable ({\\alpha} and {\\beta}) phase\ncomposition.'"
Luqiao Liu,Liu_Luqiao,arXiv:1203.3266,https://arxiv.org/abs/1203.3266,"b'Abstract:  A pure spin current generated within a nonlocal spin valve can exert a spin\ntransfer torque on a nanomagnet. This nonlocal torque enables new design\nschemes for magnetic memory devices that do not require the application of\nlarge voltages across tunnel barriers that can suffer electrical breakdown.\nHere we report a quantitative measurement of this nonlocal spin torque using\nspin-torque-driven ferromagnetic resonance. Our measurement agrees well with\nthe prediction of an effective circuit model for spin transport. Based on this\nmodel, we suggest strategies for optimizing the strength of nonlocal torque.'"
Luqiao Liu,Liu_Luqiao,arXiv:1203.2875,https://arxiv.org/abs/1203.2875,"b'Abstract:  We report a giant spin Hall effect (SHE) in {\\beta}-Ta that generates spin\ncurrents intense enough to induce efficient spin-transfer-torque switching of\nferromagnets, thereby providing a new approach for controlling magnetic devices\nthat can be superior to existing technologies. We quantify this SHE by three\nindependent methods and demonstrate spin-torque (ST) switching of both\nout-of-plane and in-plane magnetized layers. We implement a three-terminal\ndevice that utilizes current passing through a low impedance Ta-ferromagnet\nbilayer to effect switching of a nanomagnet, with a higher-impedance magnetic\ntunnel junction for read-out. The efficiency and reliability of this device,\ntogether with its simplicity of fabrication, suggest that this three-terminal\nSHE-ST design can eliminate the main obstacles currently impeding the\ndevelopment of magnetic memory and non-volatile spin logic technologies.'"
Luqiao Liu,Liu_Luqiao,arXiv:1111.3702,https://arxiv.org/abs/1111.3702,"b'Abstract:  Several different experimental techniques have been used in efforts to\nmeasure the spin Hall conductivity and the spin Hall angle in Pt samples at\nroom temperature, with results that disagree by more than a factor of 20, with\nspin Hall conductivities from 2.4 x 10^4 to 5.1 x 10^5 [hbar/(2e)] (Ohm-m)^-1\nand spin Hall angles from 0.0037 to 0.08. We review this work, and analyze\npossible reasons for the discrepancies. We explain that the smallest values for\nthe spin Hall angle that have been reported, based on measurements of lateral\npermalloy/copper/platinum devices, are incorrect because the original analyses\ndid not properly take into account that copper layers in these devices will\nshunt charge current flowing through adjacent platinum wires, thereby greatly\nreducing the size of the spin-Hall-related signals. We suggest that differences\nbetween the results for the spin Hall angle found by other experimental\ntechniques are primarily a consequence of different assumptions about the value\nof the spin diffusion length in Pt. We present a new measurement of the spin\ndiffusion length in Pt within sputtered Pt/permalloy bilayer thin films at room\ntemperature, finding 1.4 \\pm 0.3 nm, a much smaller value than has generally\nbeen assumed previously. With this value for the spin diffusion length, the\npreviously-discordant results can be brought into much better agreement, with\nthe result that the spin Hall conductivities are (1.4 - 3.4) x 10^5 [hbar/(2e)]\n(Ohm-m)^-1 and the spin Hall angles are greater than 0.05. These values are\nsufficiently large that the spin Hall effect in Pt can be used to generate spin\ncurrents and spin transfer torques strong enough for efficient manipulation of\nmagnetic moments in adjacent ferromagnetic layers.'"
Luqiao Liu,Liu_Luqiao,arXiv:1110.6846,https://arxiv.org/abs/1110.6846,"b'Abstract:  The spin Hall effect (SHE) generates spin currents within nonmagnetic\nmaterials. Previously, studies of the SHE have been motivated primarily to\nunderstand its fundamental origin and magnitude. Here we demonstrate, using\nmeasurement and modeling, that in a Pt/Co bilayer with perpendicular magnetic\nanisotropy the SHE can produce a spin transfer torque that is strong enough to\nefficiently rotate and reversibly switch the Co magnetization, thereby\nproviding a new strategy both to understand the SHE and to manipulate magnets.\nWe suggest that the SHE torque can have a similarly strong influence on\ncurrent-driven magnetic domain wall motion in Pt/ferromagnet multilayers. We\nestimate that in optimized devices the SHE torque can switch magnetic moments\nusing currents comparable to those in magnetic tunnel junctions operated by\nconventional spin-torque switching, meaning that the SHE can enable magnetic\nmemory and logic devices with similar performance but simpler architecture than\nthe current state of the art.'"
Luqiao Liu,Liu_Luqiao,arXiv:1011.2788,https://arxiv.org/abs/1011.2788,"b'Abstract:  We demonstrate that the spin Hall effect in a thin film with strong\nspin-orbit scattering can excite magnetic precession in an adjacent\nferromagnetic film. The flow of alternating current through a Pt/NiFe bilayer\ngenerates an oscillating transverse spin current in the Pt, and the resultant\ntransfer of spin angular momentum to the NiFe induces ferromagnetic resonance\n(FMR) dynamics. The Oersted field from the current also generates an FMR signal\nbut with a different symmetry. The ratio of these two signals allows a\nquantitative determination of the spin current and the spin Hall angle.'"
Nancy Lynch,Lynch_Nancy,arXiv:1903.01217,https://arxiv.org/abs/1903.01217,"b'Abstract:  In this paper, we consider a network of spiking neurons with a deterministic\nsynchronous firing rule at discrete time. We propose three problems -- ""first\nconsecutive spikes counting"", ""total spikes counting"" and ""$k$-spikes temporal\nto spatial encoding"" -- to model how brains extract temporal information into\nspatial information from different neural codings. For a max input length $T$,\nwe design three networks that solve these three problems with matching lower\nbounds in both time $O(T)$ and number of neurons $O(\\log T)$ in all three\nquestions.'"
Nancy Lynch,Lynch_Nancy,arXiv:1811.10577,https://arxiv.org/abs/1811.10577,"b'Abstract:  In the paper titled ""The SNOW Theorem"" the authors proposed four desirable\nproperties in transaction processing systems for achieving low-latency of READ\ntransactions, with asynchronous and reliable communications, and referred to\nthem collectively as the SNOW properties: The underlying properties, in the\ncontext of an execution, are (i) strict serializability (S) property where READ\nand WRITE transactions seem to occur atomically, (ii) non-blocking (N) property\nimplies for read operations, (iii) one version and one round (O) property,\nwhere reads operations completes in one-round of client-server communication\nand only one version of the object value is sent, and (iv) concurrent WRITE\ntransactions (W) property, which means WRITE transactions can occur. Then they\nargued that it is impossible to implement all the four properties, in the same\nsystem, even with at least three clients. They referred to their result as the\nSNOW theorem, and they posed the two-client setting as an open question. Here\nwe revisit the results of the work and present several new results. In our\nfirst result, we resolve the two-client scenario: We prove that even with two\nclients, without client-to-client messaging, it is impossible to design an\ntransaction processing system which satisfies the SNOW properties. Second, we\nprovide a rigorous proof of the SNOW theorem for systems with at least three\nclients, i.e., we show that it is impossible to implement a transaction\nprocessing system, consisting of at least three clients, even with\nclient-to-client messaging, that satisfies the SNOW properties. Next we derive\na useful property for executions of algorithms that implement objects of data\ntypes considered in our work that helps us show the S property of algorithms\npresented in the paper. Then we present two new algorithms that satisfies S, N,\nW and wreaked versions O property.'"
Nancy Lynch,Lynch_Nancy,arXiv:1811.03968,https://arxiv.org/abs/1811.03968,"b'Abstract:  We consider multi-armed bandit problems in social groups wherein each\nindividual has bounded memory and shares the common goal of learning the best\narm/option. We say an individual learns the best option if eventually (as $t\\to\n\\infty$) it pulls only the arm with the highest expected reward. While this\ngoal is provably impossible for an isolated individual due to bounded memory,\nwe show that, in social groups, this goal can be achieved easily with the aid\nof social persuasion (i.e., communication) as long as the communication\nnetworks/graphs satisfy some mild conditions. To deal with the interplay\nbetween the randomness in the rewards and in the social interaction, we employ\nthe {\\em mean-field approximation} method. Considering the possibility that the\nindividuals in the networks may not be exchangeable when the communication\nnetworks are not cliques, we go beyond the classic mean-field techniques and\napply a refined version of mean-field approximation:\n(1) Using coupling we show that, if the communication graph is connected and\nis either regular or has doubly-stochastic degree-weighted adjacency matrix,\nwith probability $\\to 1$ as the social group size $N \\to \\infty$, every\nindividual in the social group learns the best option.\n(2) If the minimum degree of the graph diverges as $N \\to \\infty$, over an\narbitrary but given finite time horizon, the sample paths describing the\nopinion evolutions of the individuals are asymptotically independent. In\naddition, the proportions of the population with different opinions converge to\nthe unique solution of a system of ODEs. In the solution of the obtained ODEs,\nthe proportion of the population holding the correct opinion converges to $1$\nexponentially fast in time.\nNotably, our results hold even if the communication graphs are highly sparse.'"
Nancy Lynch,Lynch_Nancy,arXiv:1808.03884,https://arxiv.org/abs/1808.03884,"b'Abstract:  This paper is part of a project on developing an algorithmic theory of brain\nnetworks, based on stochastic Spiking Neural Network (SNN) models. Inspired by\ntasks that seem to be solved in actual brains, we are defining abstract\nproblems to be solved by these networks. In our work so far, we have developed\nmodels and algorithms for the Winner-Take-All problem from computational\nneuroscience [LMP17a,Mus18], and problems of similarity detection and neural\ncoding [LMP17b]. We plan to consider many other problems and networks,\nincluding both static networks and networks that learn.\nThis paper is about basic theory for the stochastic SNN model. In particular,\nwe define a simple version of the model. This version assumes that the neurons\'\nonly state is a Boolean, indicating whether the neuron is firing or not. In\nlater work, we plan to develop variants of the model with more elaborate state.\nWe also define an external behavior notion for SNNs, which can be used for\nstating requirements to be satisfied by the networks.\nWe then define a composition operator for SNNs. We prove that our external\nbehavior notion is ""compositional"", in the sense that the external behavior of\na composed network depends only on the external behaviors of the component\nnetworks. We also define a hiding operator that reclassifies some output\nbehavior of an SNN as internal. We give basic results for hiding.\nFinally, we give a formal definition of a problem to be solved by an SNN, and\ngive basic results showing how composition and hiding of networks affect the\nproblems that they solve. We illustrate our definitions with three examples:\nbuilding a circuit out of gates, building an ""Attention"" network out of a\n""Winner-Take-All"" network and a ""Filter"" network, and a toy example involving\ncombining two networks in a cyclic fashion.'"
Nancy Lynch,Lynch_Nancy,arXiv:1805.03727,https://arxiv.org/abs/1805.03727,"b'Abstract:  Atomicity or strong consistency is one of the fundamental, most intuitive,\nand hardest to provide primitives in distributed shared memory emulations. To\nensure survivability, scalability, and availability of a storage service in the\npresence of failures, traditional approaches for atomic memory emulation, in\nmessage passing environments, replicate the objects across multiple servers.\nCompared to replication based algorithms, erasure code-based atomic memory\nalgorithms has much lower storage and communication costs, but usually, they\nare harder to design. The difficulty of designing atomic memory algorithms\nfurther grows, when the set of servers may be changed to ensure survivability\nof the service over software and hardware upgrades, while avoiding service\ninterruptions. Atomic memory algorithms for performing server reconfiguration,\nin the replicated systems, are very few, complex, and are still part of an\nactive area of research; reconfigurations of erasure-code based algorithms are\nnon-existent.\nIn this work, we present ARES, an algorithmic framework that allows\nreconfiguration of the underlying servers, and is particularly suitable for\nerasure-code based algorithms emulating atomic objects. ARES introduces new\nconfigurations while keeping the service available. To use with ARES we also\npropose a new, and to our knowledge, the first two-round erasure code based\nalgorithm TREAS, for emulating multi-writer, multi-reader (MWMR) atomic objects\nin asynchronous, message-passing environments, with near-optimal communication\nand storage costs. Our algorithms can tolerate crash failures of any client and\nsome fraction of servers, and yet, guarantee safety and liveness property.\nMoreover, by bringing together the advantages of ARES and TREAS, we propose an\noptimized algorithm where new configurations can be installed without the\nobjects values passing through the reconfiguration clients.'"
Nancy Lynch,Lynch_Nancy,arXiv:1805.03691,https://arxiv.org/abs/1805.03691,"b'Abstract:  We study the problem of distributed task allocation inspired by the behavior\nof social insects, which perform task allocation in a setting of limited\ncapabilities and noisy environment feedback. We assume that each task has a\ndemand that should be satisfied but not exceeded, i.e., there is an optimal\nnumber of ants that should be working on this task at a given time. The goal is\nto assign a near-optimal number of workers to each task in a distributed manner\nand without explicit access to the values of the demands nor the number of ants\nworking on the task.\nWe seek to answer the question of how the quality of task allocation depends\non the accuracy of assessing whether too many (overload) or not enough (lack)\nants are currently working on a given task. Concretely, we address the open\nquestion of solving task allocation in the model where each ant receives\nfeedback that depends on the deficit defined as the (possibly negative)\ndifference between the optimal demand and the current number of workers in the\ntask. The feedback is modeled as a random variable that takes value lack or\noverload with probability given by a sigmoid of the deficit. Each ants receives\nthe feedback independently, but the higher the overload or lack of workers for\na task, the more likely it is that all the ants will receive the same, correct\nfeedback from this task; the closer the deficit is to zero, the less reliable\nthe feedback becomes. We measure the performance of task allocation algorithms\nusing the notion of regret, defined as the absolute value of the deficit summed\nover all tasks and summed over time.\nWe propose a simple, constant-memory, self-stabilizing, distributed algorithm\nthat quickly converges from any initial distribution to a near-optimal\nassignment. We also show that our algorithm works not only under stochastic\nnoise but also in an adversarial noise setting.'"
Nancy Lynch,Lynch_Nancy,arXiv:1803.02216,https://arxiv.org/abs/1803.02216,"b'Abstract:  In this paper, we study local and global broadcast in the dual graph model,\nwhich describes communication in a radio network with both reliable and\nunreliable links. Existing work proved that efficient solutions to these\nproblems are impossible in the dual graph model under standard assumptions. In\nreal networks, however, simple back-off strategies tend to perform well for\nsolving these basic communication tasks. We address this apparent paradox by\nintroducing a new set of constraints to the dual graph model that better\ngeneralize the slow/fast fading behavior common in real networks. We prove that\nin the context of these new constraints, simple back-off strategies now provide\nefficient solutions to local and global broadcast in the dual graph model. We\nalso precisely characterize how this efficiency degrades as the new constraints\nare reduced down to non-existent, and prove new lower bounds that establish\nthis degradation as near optimal for a large class of natural algorithms. We\nconclude with a preliminary investigation of the performance of these\nstrategies when we include additional generality to the model. These results\nprovide theoretical foundations for the practical observation that simple\nback-off algorithms tend to work well even amid the complicated link dynamics\nof real radio networks.'"
Nancy Lynch,Lynch_Nancy,arXiv:1802.08159,https://arxiv.org/abs/1802.08159,"b'Abstract:  We consider multi-armed bandit problems in social groups wherein each\nindividual has bounded memory and shares the common goal of learning the best\narm/option. We say an individual learns the best option if eventually (as $t\n\\to \\infty$) it pulls only the arm with the highest average reward. While this\ngoal is provably impossible for an isolated individual, we show that, in social\ngroups, this goal can be achieved easily with the aid of social persuasion,\ni.e., communication. Specifically, we study the learning dynamics wherein an\nindividual sequentially decides on which arm to pull next based on not only its\nprivate reward feedback but also the suggestions provided by randomly chosen\npeers. Our learning dynamics are hard to analyze via explicit probabilistic\ncalculations due to the stochastic dependency induced by social interaction.\nInstead, we employ the mean-field approximation method from statistical physics\nand we show:\n(1) With probability $\\to 1$ as the social group size $N \\to \\infty $, every\nindividual in the social group learns the best option.\n(2) Over an arbitrary finite time horizon $[0, T]$, with high probability (in\n$N$), the fraction of individuals that prefer the best option grows to 1\nexponentially fast as $t$ increases ($t\\in [0, T]$).\nA major innovation of our mean-filed analysis is a simple yet powerful\ntechnique to deal with absorbing states in the interchange of limits $N \\to\n\\infty$ and $t \\to \\infty $. The mean-field approximation method allows us to\napproximate the probabilistic sample paths of our learning dynamics by a\ndeterministic and smooth trajectory that corresponds to the unique solution of\na well-behaved system of ordinary differential equations (ODEs). Such an\napproximation is desired because the analysis of a system of ODEs is relatively\neasier than that of the original stochastic system.'"
Nancy Lynch,Lynch_Nancy,arXiv:1706.01382,https://arxiv.org/abs/1706.01382,"b""Abstract:  We study distributed algorithms implemented in a simplified biologically\ninspired model for stochastic spiking neural networks. We focus on tradeoffs\nbetween computation time and network complexity, along with the role of\nrandomness in efficient neural computation.\nIt is widely accepted that neural computation is inherently stochastic. In\nrecent work, we explored how this stochasticity could be leveraged to solve the\n`winner-take-all' leader election task. Here, we focus on using randomness in\nneural algorithms for similarity testing and compression. In the most basic\nsetting, given two $n$-length patterns of firing neurons, we wish to\ndistinguish if the patterns are equal or $\\epsilon$-far from equal.\nRandomization allows us to solve this task with a very compact network, using\n$O \\left (\\frac{\\sqrt{n}\\log n}{\\epsilon}\\right)$ auxiliary neurons, which is\nsublinear in the input size. At the heart of our solution is the design of a\n$t$-round neural random access memory, or indexing network, which we call a\nneuro-RAM. This module can be implemented with $O(n/t)$ auxiliary neurons and\nis useful in many applications beyond similarity testing.\nUsing a VC dimension-based argument, we show that the tradeoff between\nruntime and network size in our neuro-RAM is nearly optimal. Our result has\nseveral implications -- since our neuro-RAM can be implemented with\ndeterministic threshold gates, it shows that, in contrast to similarity\ntesting, randomness does not provide significant computational advantages for\nthis problem. It also establishes a separation between feedforward networks\nwhose gates spike with sigmoidal probability functions, and well-studied\ndeterministic sigmoidal networks, whose gates output real number sigmoidal\nvalues, and which can implement a neuro-RAM much more efficiently."""
Nancy Lynch,Lynch_Nancy,arXiv:1704.07133,https://arxiv.org/abs/1704.07133,"b""Abstract:  We adapt a recent algorithm by Ghaffari [SODA'16] for computing a Maximal\nIndependent Set in the LOCAL model, so that it works in the significantly\nweaker BEEP model. For networks with maximum degree $\\Delta$, our algorithm\nterminates locally within time $O((\\log \\Delta + \\log (1/\\epsilon)) \\cdot\n\\log(1/\\epsilon))$, with probability at least $1 - \\epsilon$. The key idea of\nthe modification is to replace explicit messages about transmission\nprobabilities with estimates based on the number of received messages.\nAfter the successful introduction (and implicit use) of local analysis, e.g.,\nby Barenboim et al. [JACM'16], Chung et al. [PODC'14], Ghaffari [SODA'16], and\nHalldorsson et al. [PODC'15], we study this concept in the BEEP model for the\nfirst time.\nBy doing so, we improve over local bounds that are implicitly derived from\nprevious work (that uses traditional global analysis) on computing a Maximal\nIndependent Set in the \\beep model for a large range of values of the parameter\n$\\Delta$. At the same time, we show that our algorithm in the \\beep model only\nneeds to pay a $\\log(1/\\epsilon)$ factor in the runtime compared to the best\nknown MIS algorithm in the much more powerful \\local model. We demonstrate that\nthis overhead is negligible, as communication via beeps can be implemented\nusing significantly less resources than communication in the LOCAL model. In\nparticular, when looking at implementing these models, one round of the \\local\nmodel needs at least $O(\\Delta)$ time units, while one round in the BEEP model\nneeds $O(\\log\\Delta)$ time units, an improvement that diminishes the loss of a\n$\\log(1/\\epsilon)$ factor in most settings."""
Nancy Lynch,Lynch_Nancy,arXiv:1703.01286,https://arxiv.org/abs/1703.01286,"b'Abstract:  Motivated by emerging applications to the edge computing paradigm, we\nintroduce a two-layer erasure-coded fault-tolerant distributed storage system\noffering atomic access for read and write operations. In edge computing,\nclients interact with an edge-layer of servers that is geographically near; the\nedge-layer in turn interacts with a back-end layer of servers. The edge-layer\nprovides low latency access and temporary storage for client operations, and\nuses the back-end layer for persistent storage. Our algorithm, termed Layered\nData Storage (LDS) algorithm, offers several features suitable for\nedge-computing systems, works under asynchronous message-passing environments,\nsupports multiple readers and writers, and can tolerate $f_1 < n_1/2$ and $f_2\n< n_2/3$ crash failures in the two layers having $n_1$ and $n_2$ servers,\nrespectively. We use a class of erasure codes known as regenerating codes for\nstorage of data in the back-end layer. The choice of regenerating codes,\ninstead of popular choices like Reed-Solomon codes, not only optimizes the cost\nof back-end storage, but also helps in optimizing communication cost of read\noperations, when the value needs to be recreated all the way from the back-end.\nThe two-layer architecture permits a modular implementation of atomicity and\nerasure-code protocols; the implementation of erasure-codes is mostly limited\nto interaction between the two layers. We prove liveness and atomicity of LDS,\nand also compute performance costs associated with read and write operations.\nFurther, in a multi-object system running $N$ independent instances of LDS,\nwhere only a small fraction of the objects undergo concurrent accesses at any\npoint during the execution, the overall storage cost is dominated by that of\npersistent storage in the back-end layer, and is given by $\\Theta(N)$.'"
Nancy Lynch,Lynch_Nancy,arXiv:1610.02084,https://arxiv.org/abs/1610.02084,"b""Abstract:  We initiate a line of investigation into biological neural networks from an\nalgorithmic perspective. We develop a simplified but biologically plausible\nmodel for distributed computation in stochastic spiking neural networks and\nstudy tradeoffs between computation time and network complexity in this model.\nOur aim is to abstract real neural networks in a way that, while not capturing\nall interesting features, preserves high-level behavior and allows us to make\nbiologically relevant conclusions.\nIn this paper, we focus on the important `winner-take-all' (WTA) problem,\nwhich is analogous to a neural leader election unit: a network consisting of\n$n$ input neurons and $n$ corresponding output neurons must converge to a state\nin which a single output corresponding to a firing input (the `winner') fires,\nwhile all other outputs remain silent. Neural circuits for WTA rely on\ninhibitory neurons, which suppress the activity of competing outputs and drive\nthe network towards a converged state with a single firing winner. We attempt\nto understand how the number of inhibitors used affects network convergence\ntime.\nWe show that it is possible to significantly outperform naive WTA\nconstructions through a more refined use of inhibition, solving the problem in\n$O(\\theta)$ rounds in expectation with just $O(\\log^{1/\\theta} n)$ inhibitors\nfor any $\\theta$. An alternative construction gives convergence in\n$O(\\log^{1/\\theta} n)$ rounds with $O(\\theta)$ inhibitors. We compliment these\nupper bounds with our main technical contribution, a nearly matching lower\nbound for networks using $\\ge \\log\\log n$ inhibitors. Our lower bound uses\nfamiliar indistinguishability and locality arguments from distributed computing\ntheory. It lets us derive a number of interesting conclusions about the\nstructure of any network solving WTA with good probability, and the use of\nrandomness and inhibition within such a network."""
Nancy Lynch,Lynch_Nancy,arXiv:1605.06844,https://arxiv.org/abs/1605.06844,"b'Abstract:  The focus of this paper is to understand storage costs of emulating an atomic\nshared memory over an asynchronous, distributed message passing system.\nPrevious literature has developed several shared memory emulation algorithms\nbased on replication and erasure coding techniques. In this paper, we present\ninformation-theoretic lower bounds on the storage costs incurred by shared\nmemory emulation algorithms. Our storage cost lower bounds are universally\napplicable, that is, we make no assumption on the structure of the algorithm or\nthe method of encoding the data.\nWe consider an arbitrary algorithm $A$ that implements an atomic multi-writer\nsingle-reader (MWSR) shared memory variable whose values come from a finite set\n$\\mathcal{V}$ over a system of $N$ servers connected by point-to-point\nasynchronous links. We require that in every fair execution of algorithm $A$\nwhere the number of server failures is smaller than a parameter $f$, every\noperation invoked at a non-failing client terminates. We define the storage\ncost of a server in algorithm $A$ as the logarithm (to base 2) of number of\nstates it can take on; the total-storage cost of algorithm $A$ is the sum of\nthe storage cost of all servers.\nOur results are as follows. (i) We show that if algorithm $A$ does not use\nserver gossip, then the total storage cost is lower bounded by $2\n\\frac{N}{N-f+1}\\log_2|\\mathcal{V}|-o(\\log_2|\\mathcal{V}|)$. (ii) The total\nstorage cost is at least $2 \\frac{N}{N-f+2}\n\\log_{2}|\\mathcal{V}|-o(\\log_{2}|\\mathcal{V}|)$ even if the algorithm uses\nserver gossip. (iii) We consider algorithms where the write protocol sends\ninformation about the value in at most one phase. We show that the total\nstorage cost is at least $\\nu^* \\frac{N}{N-f+\\nu^*-1} \\log_2( |\\mathcal{V}|)-\no(\\log_2(|\\mathcal{V}|),$ where $\\nu^*$ is the minimum of $f+1$ and the number\nof active write operations of an execution.'"
Nancy Lynch,Lynch_Nancy,arXiv:1605.05717,https://arxiv.org/abs/1605.05717,"b'Abstract:  Erasure codes offer an efficient way to decrease storage and communication\ncosts while implementing atomic memory service in asynchronous distributed\nstorage systems. In this paper, we provide erasure-code-based algorithms having\nthe additional ability to perform background repair of crashed nodes. A repair\noperation of a node in the crashed state is triggered externally, and is\ncarried out by the concerned node via message exchanges with other active nodes\nin the system. Upon completion of repair, the node re-enters active state, and\nresumes participation in ongoing and future read, write, and repair operations.\nTo guarantee liveness and atomicity simultaneously, existing works assume\neither the presence of nodes with stable storage, or presence of nodes that\nnever crash during the execution. We demand neither of these; instead we\nconsider a natural, yet practical network stability condition $N1$ that only\nrestricts the number of nodes in the crashed/repair state during broadcast of\nany message.\nWe present an erasure-code based algorithm $RADON_C$ that is always live, and\nguarantees atomicity as long as condition $N1$ holds. In situations when the\nnumber of concurrent writes is limited, $RADON_C$ has significantly improved\nstorage and communication cost over a replication-based algorithm $RADON_R$,\nwhich also works under $N1$. We further show how a slightly stronger network\nstability condition $N2$ can be used to construct algorithms that never violate\natomicity. The guarantee of atomicity comes at the expense of having an\nadditional phase during the read and write operations.'"
Nancy Lynch,Lynch_Nancy,arXiv:1605.01748,https://arxiv.org/abs/1605.01748,"b'Abstract:  Erasure codes are increasingly being studied in the context of implementing\natomic memory objects in large scale asynchronous distributed storage systems.\nWhen compared with the traditional replication based schemes, erasure codes\nhave the potential of significantly lowering storage and communication costs\nwhile simultaneously guaranteeing the desired resiliency levels. In this work,\nwe propose the Storage-Optimized Data-Atomic (SODA) algorithm for implementing\natomic memory objects in the multi-writer multi-reader setting. SODA uses\nMaximum Distance Separable (MDS) codes, and is specifically designed to\noptimize the total storage cost for a given fault-tolerance requirement. For\ntolerating $f$ server crashes in an $n$-server system, SODA uses an $[n, k]$\nMDS code with $k=n-f$, and incurs a total storage cost of $\\frac{n}{n-f}$. SODA\nis designed under the assumption of reliable point-to-point communication\nchannels. The communication cost of a write and a read operation are\nrespectively given by $O(f^2)$ and $\\frac{n}{n-f}(\\delta_w+1)$, where\n$\\delta_w$ denotes the number of writes that are concurrent with the particular\nread. In comparison with the recent CASGC algorithm, which also uses MDS codes,\nSODA offers lower storage cost while pays more on the communication cost.\nWe also present a modification of SODA, called SODA$_{\\text{err}}$, to handle\nthe case where some of the servers can return erroneous coded elements during a\nread operation. Specifically, in order to tolerate $f$ server failures and $e$\nerror-prone coded elements, the SODA$_{\\text{err}}$ algorithm uses an $[n, k]$\nMDS code such that $k=n-2e-f$. SODA$_{\\text{err}}$ also guarantees liveness and\natomicity, while maintaining an optimized total storage cost of\n$\\frac{n}{n-f-2e}$.'"
Nancy Lynch,Lynch_Nancy,arXiv:1604.06030,https://arxiv.org/abs/1604.06030,"b'Abstract:  We present dynamic I/O automata (DIOA), a compositional model of dynamic\nsystems. In DIOA, automata can be created and destroyed dynamically, as\ncomputation proceeds, and an automaton can dynamically change its signature,\ni.e., the set of actions in which it can participate.\nDIOA features operators for parallel composition, action hiding, action\nrenaming, a notion of automaton creation, and a notion of behavioral subtyping\nby means of trace inclusion. DIOA can model mobility, using signature\nmodification, and is hierarchical: a dynamically changing system of interacting\nautomata is itself modeled as a single automaton.\nWe also show that parallel composition, action hiding, action renaming, and\n(subject to some technical conditions) automaton creation are all monotonic\nwith respect to trace inclusion: if one component is replaced by another whose\ntraces are a subset of the former, then the set of traces of the system as a\nwhole can only be reduced.'"
Nancy Lynch,Lynch_Nancy,arXiv:1603.02981,https://arxiv.org/abs/1603.02981,"b'Abstract:  Many ant species employ distributed population density estimation in\napplications ranging from quorum sensing [Pra05], to task allocation [Gor99],\nto appraisal of enemy colony strength [Ada90]. It has been shown that ants\nestimate density by tracking encounter rates -- the higher the population\ndensity, the more often the ants bump into each other [Pra05,GPT93].\nWe study distributed density estimation from a theoretical perspective. We\nprove that a group of anonymous agents randomly walking on a grid are able to\nestimate their density within a small multiplicative error in few steps by\nmeasuring their rates of encounter with other agents. Despite dependencies\ninherent in the fact that nearby agents may collide repeatedly (and, worse,\ncannot recognize when this happens), our bound nearly matches what would be\nrequired to estimate density by independently sampling grid locations.\nFrom a biological perspective, our work helps shed light on how ants and\nother social insects can obtain relatively accurate density estimates via\nencounter rates. From a technical perspective, our analysis provides new tools\nfor understanding complex dependencies in the collision probabilities of\nmultiple random walks. We bound the strength of these dependencies using\n$local\\ mixing\\ properties$ of the underlying graph. Our results extend beyond\nthe grid to more general graphs and we discuss applications to size estimation\nfor social networks and density estimation for robot swarms.'"
Nancy Lynch,Lynch_Nancy,arXiv:1508.03660,https://arxiv.org/abs/1508.03660,"b'Abstract:  This paper studies the theory of the additive wireless network model, in\nwhich the received signal is abstracted as an addition of the transmitted\nsignals. Our central observation is that the crucial challenge for computing in\nthis model is not high contention, as assumed previously, but rather\nguaranteeing a bounded amount of \\emph{information} in each neighborhood per\nround, a property that we show is achievable using a new random coding\ntechnique.\nTechnically, we provide efficient algorithms for fundamental distributed\ntasks in additive networks, such as solving various symmetry breaking problems,\napproximating network parameters, and solving an \\emph{asymmetry revealing}\nproblem such as computing a maximal input.\nThe key method used is a novel random coding technique that allows a node to\nsuccessfully decode the received information, as long as it does not contain\ntoo many distinct values. We then design our algorithms to produce a limited\namount of information in each neighborhood in order to leverage our enriched\ntoolbox for computing in additive networks.'"
Nancy Lynch,Lynch_Nancy,arXiv:1505.04514,https://arxiv.org/abs/1505.04514,"b'Abstract:  We present the first algorithm that implements an abstract MAC (absMAC) layer\nin the Signal-to-Interference-plus-Noise-Ratio (SINR) wireless network model.\nWe first prove that efficient SINR implementations are not possible for the\nstandard absMAC specification. We modify that specification to an ""approximate""\nversion that better suits the SINR model. We give an efficient algorithm to\nimplement the modified specification, and use it to derive efficient algorithms\nfor higher-level problems of global broadcast and consensus.\nIn particular, we show that the absMAC progress property has no efficient\nimplementation in terms of the SINR strong connectivity graph $G_{1-\\epsilon}$,\nwhich contains edges between nodes of distance at most $(1-\\epsilon)$ times the\ntransmission range, where $\\epsilon>0$ is a small constant that can be chosen\nby the user. This progress property bounds the time until a node is guaranteed\nto receive some message when at least one of its neighbors is transmitting.\nTo overcome this limitation, we introduce the slightly weaker notion of\napproximate progress into the absMAC specification. We provide a fast\nimplementation of the modified specification, based on decomposing a known\nalgorithm into local and global parts. We analyze our algorithm in terms of\nlocal parameters such as node degrees, rather than global parameters such as\nthe overall number of nodes. A key contribution is our demonstration that such\na local analysis is possible even in the presence of global interference.\nOur absMAC algorithm leads to several new, efficient algorithms for solving\nhigher-level problems in the SINR model. Namely, by combining our algorithm\nwith known high-level algorithms, we obtain an improved algorithm for global\nsingle-message broadcast in the SINR model, and the first efficient algorithm\nfor multi-message broadcast in that model.'"
Nancy Lynch,Lynch_Nancy,arXiv:1505.03799,https://arxiv.org/abs/1505.03799,"b""Abstract:  We introduce the study of the ant colony house-hunting problem from a\ndistributed computing perspective. When an ant colony's nest becomes unsuitable\ndue to size constraints or damage, the colony must relocate to a new nest. The\ntask of identifying and evaluating the quality of potential new nests is\ndistributed among all ants. The ants must additionally reach consensus on a\nfinal nest choice and the full colony must be transported to this single new\nnest. Our goal is to use tools and techniques from distributed computing theory\nin order to gain insight into the house-hunting process.\nWe develop a formal model for the house-hunting problem inspired by the\nbehavior of the Temnothorax genus of ants. We then show a \\Omega(log n) lower\nbound on the time for all n ants to agree on one of k candidate nests. We also\npresent two algorithms that solve the house-hunting problem in our model. The\nfirst algorithm solves the problem in optimal O(log n) time but exhibits some\nfeatures not characteristic of natural ant behavior. The second algorithm runs\nin O(k log n) time and uses an extremely simple and natural rule for each ant\nto decide on the new nest."""
Nancy Lynch,Lynch_Nancy,arXiv:1502.02538,https://arxiv.org/abs/1502.02538,"b'Abstract:  The FLP result shows that crash-tolerant consensus is impossible to solve in\nasynchronous systems, and several solutions have been proposed for\ncrash-tolerant consensus under alternative (stronger) models. One popular\napproach is to augment the asynchronous system with appropriate failure\ndetectors, which provide (potentially unreliable) information about process\ncrashes in the system, to circumvent the FLP impossibility.\nIn this paper, we demonstrate the exact mechanism by which (sufficiently\npowerful) asynchronous failure detectors enable solving crash-tolerant\nconsensus. Our approach, which borrows arguments from the FLP impossibility\nproof and the famous result from CHT, which shows that $\\Omega$ is a weakest\nfailure detector to solve consensus, also yields a natural proof to $\\Omega$ as\na weakest asynchronous failure detector to solve consensus. The use of I/O\nautomata theory in our approach enables us to model execution in a more\ndetailed fashion than CHT and also addresses the latent assumptions and\nassertions in the original result in CHT.'"
Nancy Lynch,Lynch_Nancy,arXiv:1407.4167,https://arxiv.org/abs/1407.4167,"b'Abstract:  This paper considers the communication and storage costs of emulating atomic\n(linearizable) multi-writer multi-reader shared memory in distributed\nmessage-passing systems. The paper contains three main contributions: (1) We\npresent a atomic shared-memory emulation algorithm that we call Coded Atomic\nStorage (CAS). This algorithm uses erasure coding methods. In a storage system\nwith $N$ servers that is resilient to $f$ server failures, we show that the\ncommunication cost of CAS is $\\frac{N}{N-2f}$. The storage cost of CAS is\nunbounded. (2) We present a modification of the CAS algorithm known as CAS with\nGarbage Collection (CASGC). The CASGC algorithm is parametrized by an integer\n$\\delta$ and has a bounded storage cost. We show that in every execution where\nthe number of write operations that are concurrent with a read operation is no\nbigger than $\\delta$, the CASGC algorithm with parameter $\\delta$ satisfies\natomicity and liveness. We explicitly characterize the storage cost of CASGC,\nand show that it has the same communication cost as CAS. (3) We describe an\nalgorithm known as the Communication Cost Optimal Atomic Storage (CCOAS)\nalgorithm that achieves a smaller communication cost than CAS and CASGC. In\nparticular, CCOAS incurs read and write communication costs of $\\frac{N}{N-f}$\nmeasured in terms of number of object values. We also discuss drawbacks of\nCCOAS as compared with CAS and CASGC.'"
Nancy Lynch,Lynch_Nancy,arXiv:1405.1688,https://arxiv.org/abs/1405.1688,"b'Abstract:  We consider the ANTS problem [Feinerman et al.] in which a group of agents\ncollaboratively search for a target in a two-dimensional plane. Because this\nproblem is inspired by the behavior of biological species, we argue that in\naddition to studying the {\\em time complexity} of solutions it is also\nimportant to study the {\\em selection complexity}, a measure of how likely a\ngiven algorithmic strategy is to arise in nature due to selective pressures. In\nmore detail, we propose a new selection complexity metric $\\chi$, defined for\nalgorithm ${\\cal A}$ such that $\\chi({\\cal A}) = b + \\log \\ell$, where $b$ is\nthe number of memory bits used by each agent and $\\ell$ bounds the fineness of\navailable probabilities (agents use probabilities of at least $1/2^\\ell$). In\nthis paper, we study the trade-off between the standard performance metric of\nspeed-up, which measures how the expected time to find the target improves with\n$n$, and our new selection metric.\nIn particular, consider $n$ agents searching for a treasure located at\n(unknown) distance $D$ from the origin (where $n$ is sub-exponential in $D$).\nFor this problem, we identify $\\log \\log D$ as a crucial threshold for our\nselection complexity metric. We first prove a new upper bound that achieves a\nnear-optimal speed-up of $(D^2/n +D) \\cdot 2^{O(\\ell)}$ for $\\chi({\\cal A})\n\\leq 3 \\log \\log D + O(1)$. In particular, for $\\ell \\in O(1)$, the speed-up is\nasymptotically optimal. By comparison, the existing results for this problem\n[Feinerman et al.] that achieve similar speed-up require $\\chi({\\cal A}) =\n\\Omega(\\log D)$. We then show that this threshold is tight by describing a\nlower bound showing that if $\\chi({\\cal A}) < \\log \\log D - \\omega(1)$, then\nwith high probability the target is not found within $D^{2-o(1)}$ moves per\nagent. Hence, there is a sizable gap to the straightforward $\\Omega(D^2/n + D)$\nlower bound in this setting.'"
Nancy Lynch,Lynch_Nancy,arXiv:1405.1671,https://arxiv.org/abs/1405.1671,"b'Abstract:  We study the multi-message broadcast problem using abstract MAC layer models\nof wireless networks. These models capture the key guarantees of existing MAC\nlayers while abstracting away low-level details such as signal propagation and\ncontention. We begin by studying upper and lower bounds for this problem in a\n{\\em standard abstract MAC layer model}---identifying an interesting dependence\nbetween the structure of unreliable links and achievable time complexity. In\nmore detail, given a restriction that devices connected directly by an\nunreliable link are not too far from each other in the reliable link topology,\nwe can (almost) match the efficiency of the reliable case. For the related\nrestriction, however, that two devices connected by an unreliable link are not\ntoo far from each other in geographic distance, we prove a new lower bound that\nshows that this efficiency is impossible. We then investigate how much extra\npower must be added to the model to enable a new order of magnitude of\nefficiency. In more detail, we consider an {\\em enhanced abstract MAC layer\nmodel} and present a new multi-message broadcast algorithm that (under certain\nnatural assumptions) solves the problem in this model faster than any known\nsolutions in an abstract MAC layer setting.'"
Nancy Lynch,Lynch_Nancy,arXiv:1208.6125,https://arxiv.org/abs/1208.6125,"b'Abstract:  Efficient communication in wireless networks is typically challenged by the\npossibility of interference among several transmitting nodes. Much important\nresearch has been invested in decreasing the number of collisions in order to\nobtain faster algorithms for communication in such networks.\nThis paper proposes a novel approach for wireless communication, which\nembraces collisions rather than avoiding them, over an additive channel. It\nintroduces a coding technique called Bounded-Contention Coding (BCC) that\nallows collisions to be successfully decoded by the receiving nodes into the\noriginal transmissions and whose complexity depends on a bound on the\ncontention among the transmitters.\nBCC enables deterministic local broadcast in a network with n nodes and at\nmost a transmitters with information of l bits each within O(a log n + al) bits\nof communication with full-duplex radios, and O((a log n + al)(log n)) bits,\nwith high probability, with half-duplex radios. When combined with random\nlinear network coding, BCC gives global broadcast within O((D + a + log n)(a\nlog n + l)) bits, with high probability. This also holds in dynamic networks\nthat can change arbitrarily over time by a worst-case adversary. When no bound\non the contention is given, it is shown how to probabilistically estimate it\nand obtain global broadcast that is adaptive to the true contention in the\nnetwork.'"
Nancy Lynch,Lynch_Nancy,arXiv:1206.0154,https://arxiv.org/abs/1206.0154,"b'Abstract:  The local broadcast problem assumes that processes in a wireless network are\nprovided messages, one by one, that must be delivered to their neighbors. In\nthis paper, we prove tight bounds for this problem in two well-studied wireless\nnetwork models: the classical model, in which links are reliable and collisions\nconsistent, and the more recent dual graph model, which introduces unreliable\nedges. Our results prove that the Decay strategy, commonly used for local\nbroadcast in the classical setting, is optimal. They also establish a\nseparation between the two models, proving that the dual graph setting is\nstrictly harder than the classical setting, with respect to this primitive.'"
Nancy Lynch,Lynch_Nancy,arXiv:math/9409221,https://arxiv.org/abs/math/9409221,"b""Abstract:  A method of analyzing time bounds for randomized distributed algorithms is\npresented, in the context of a new and general framework for describing and\nreasoning about randomized algorithms. The method consists of proving auxiliary\nstatements of the form U (t)->(p) U', which means that whenever the algorithm\nbegins in a state in set U, with probability p, it will reach a state in set U'\nwithin time t. The power of the method is illustrated by its use in proving a\nconstant upper bound on the expected time for some process to reach its\ncritical region, in Lehmann and Rabin's Dining Philosophers algorithm."""
Nancy Lynch,Lynch_Nancy,arXiv:math/9409220,https://arxiv.org/abs/math/9409220,"b'Abstract:  We consider the following scheduling problem. A system is composed of $n$\nprocessors drawn from a pool of $N$. The processors can become faulty while in\noperation and faulty processors never recover. A report is issued whenever a\nfault occurs. This report states only the existence of a fault, but does not\nindicate its location. Based on this report, the scheduler can reconfigure the\nsystem and choose another set of $n$ processors. The system operates\nsatisfactorily as long as at most $f$ of the $n$ selected processors are\nfaulty. We exhibit a scheduling strategy allowing the system to operate\nsatisfactorily until approximately $(N/n)f$ faults are reported in the worst\ncase. Our precise bound is tight.'"
Samuel Madden,Madden_Samuel,arXiv:1903.05008,https://arxiv.org/abs/1903.05008,"b'Abstract:  Data-driven analysis is important in virtually every modern organization.\nYet, most data is underutilized because it remains locked in silos inside of\norganizations; large organizations have thousands of databases, and billions of\nfiles that are not integrated together in a single, queryable repository.\nDespite 40+ years of continuous effort by the database community, data\nintegration still remains an open challenge. In this paper, we advocate a\ndifferent approach: rather than trying to infer a common schema, we aim to find\nanother common representation for diverse, heterogeneous data. Specifically, we\nargue for an embedding (i.e., a vector space) in which all entities, rows,\ncolumns, and paragraphs are represented as points. In the embedding, the\ndistance between points indicates their degree of relatedness. We present\nTermite, a prototype we have built to learn the best embedding from the data.\nBecause the best representation is learned, this allows Termite to avoid much\nof the human effort associated with traditional data integration tasks. On top\nof Termite, we have implemented a Termite-Join operator, which allows people to\nidentify related concepts, even when these are stored in databases with\ndifferent schemas and in unstructured data such as text files, webpages, etc.\nFinally, we show preliminary evaluation results of our prototype via a user\nstudy, and describe a list of future directions we have identified.'"
Samuel Madden,Madden_Samuel,arXiv:1903.03229,https://arxiv.org/abs/1903.03229,"b'Abstract:  Optimizing the physical data storage and retrieval of data are two key\ndatabase management problems. In this paper, we propose a language that can\nexpress a wide range of physical database layouts, going well beyond the row-\nand column- based methods that are widely used in database management systems.\nWe also build a compiler for this language, which is specialized for a dataset\nand a query workload. We conduct experiments using a popular database\nbenchmark, which shows that the performance of these specialized queries is\ncompetitive with a state-of-the-art in memory compiled database system.'"
Samuel Madden,Madden_Samuel,arXiv:1903.00424,https://arxiv.org/abs/1903.00424,"b""Abstract:  Data replication is crucial in modern distributed systems as a means to\nprovide high availability. Many techniques have been proposed to utilize\nreplicas to improve a system's performance, often requiring expensive\ncoordination or sacrificing consistency. In this paper, we present SCAR, a new\ndistributed and replicated in-memory database that allows serializable\ntransactions to read from backup replicas with minimal coordination. SCAR works\nby assigning logical timestamps to database records so that a transaction can\nsafely read from a backup replica without coordinating with the primary\nreplica, because the records cannot be changed up to a certain logical time. In\naddition, we propose two optimization techniques, timestamp synchronization and\nparallel locking and validation, to further reduce coordination. We show that\nSCAR outperforms systems with conventional concurrency control algorithms and\nreplication strategies by up to a factor of 2 on three popular benchmarks. We\nalso demonstrate that SCAR achieves higher throughput by running under reduced\nisolation levels and detects concurrency anomalies in real time."""
Samuel Madden,Madden_Samuel,arXiv:1811.02059,https://arxiv.org/abs/1811.02059,"b'Abstract:  In this paper, we present STAR, a new distributed in-memory database with\nasymmetric replication. By employing a single-node non-partitioned architecture\nfor some replicas and a partitioned architecture for other replicas, STAR is\nable to efficiently run both highly partitionable workloads and workloads that\ninvolve cross-partition transactions. The key idea is a new phase-switching\nalgorithm where the execution of single-partition and cross-partition\ntransactions are separated. In the partitioned phase, single-partition\ntransactions are run on multiple machines in parallel to exploit more\nconcurrency. In the single-master phase, mastership for the entire database is\nswitched to a designated coordinator node, which can execute these transactions\nwithout the use of expensive coordination protocols like two-phase commit.\nBecause the coordinator node has a full copy of the database, this\nphase-switching can be done at negligible cost. Our experiments on two popular\nbenchmarks (YCSB and TPC-C) show that high availability via replication can\ncoexist with fast serializable transaction execution in distributed in-memory\ndatabases, with STAR outperforming systems that employ conventional concurrency\ncontrol and replication algorithms by up to one order of magnitude.'"
Samuel Madden,Madden_Samuel,arXiv:1808.02515,https://arxiv.org/abs/1808.02515,"b""Abstract:  Thanks to the rapid proliferation of connected devices, sensor-generated time\nseries constitute a large and growing portion of the world's data. Often, this\ndata is collected from distributed, resource-constrained devices and\ncentralized at one or more servers. A key challenge in this setup is reducing\nthe size of the transmitted data without sacrificing its quality. Lower quality\nreduces the data's utility, but smaller size enables both reduced network and\nstorage costs at the servers and reduced power consumption in sensing devices.\nA natural solution is to compress the data at the sensing devices.\nUnfortunately, existing compression algorithms either violate the memory and\nlatency constraints common for these devices or, as we show experimentally,\nperform poorly on sensor-generated time series.\nWe introduce a time series compression algorithm that achieves\nstate-of-the-art compression ratios while requiring less than 1KB of memory and\nadding virtually no latency. This method is suitable not only for low-power\ndevices collecting data, but also for servers storing and querying data; in the\nlatter context, it can decompress at over 3GB/s in a single thread, even faster\nthan many algorithms with much lower compression ratios. A key component of our\nmethod is a high-speed forecasting algorithm that can be trained online and\nsignificantly outperforms alternatives such as delta coding.\nExtensive experiments on datasets from many domains show that these results\nhold not only for sensor data but also across a wide array of other time\nseries."""
Samuel Madden,Madden_Samuel,arXiv:1806.03723,https://arxiv.org/abs/1806.03723,"b'Abstract:  As neural networks become widely deployed in different applications and on\ndifferent hardware, it has become increasingly important to optimize inference\ntime and model size along with model accuracy. Most current techniques optimize\nmodel size, model accuracy and inference time in different stages, resulting in\nsuboptimal results and computational inefficiency. In this work, we propose a\nnew technique called Smallify that optimizes all three of these metrics at the\nsame time. Specifically we present a new method to simultaneously optimize\nnetwork size and model performance by neuron-level pruning during training.\nNeuron-level pruning not only produces much smaller networks but also produces\ndense weight matrices that are amenable to efficient inference. By applying our\ntechnique to convolutional as well as fully connected models, we show that\nSmallify can reduce network size by 35X with a 6X improvement in inference time\nwith similar accuracy as models found by traditional training techniques.'"
Samuel Madden,Madden_Samuel,arXiv:1710.11528,https://arxiv.org/abs/1710.11528,"b'Abstract:  Many database columns contain string or numerical data that conforms to a\npattern, such as phone numbers, dates, addresses, product identifiers, and\nemployee ids. These patterns are useful in a number of data processing\napplications, including understanding what a specific field represents when\nfield names are ambiguous, identifying outlier values, and finding similar\nfields across data sets. One way to express such patterns would be to learn\nregular expressions for each field in the database. Unfortunately, exist- ing\ntechniques on regular expression learning are slow, taking hundreds of seconds\nfor columns of just a few thousand values. In contrast, we develop XSystem, an\nefficient method to learn patterns over database columns in significantly less\ntime. We show that these patterns can not only be built quickly, but are\nexpressive enough to capture a number of key applications, including detecting\noutliers, measuring column similarity, and assigning semantic labels to columns\n(based on a library of regular expressions). We evaluate these applications\nwith datasets that range from chemical databases (based on a collaboration with\na pharmaceutical company), our university data warehouse, and open data from\nMassData.gov.'"
Samuel Madden,Madden_Samuel,arXiv:1709.10436,https://arxiv.org/abs/1709.10436,"b'Abstract:  Data integration has been a long-standing challenge in data management with\nmany applications. A key step in data integration is entity consolidation. It\ntakes a collection of clusters of duplicate records as input and produces a\nsingle ""golden record"" for each cluster, which contains the canonical value for\neach attribute. Truth discovery and data fusion methods, as well as Master Data\nManagement (MDM) systems, can be used for entity consolidation. However, to\nachieve better results, the variant values (i.e., values that are logically the\nsame with different formats) in the clusters need to be consolidated before\napplying these methods.\nFor this purpose, we propose a data-driven method to standardize the variant\nvalues based on two observations: (1) the variant values usually can be\ntransformed to the same representation (e.g., ""Mary Lee"" and ""Lee, Mary"") and\n(2) the same transformation often appears repeatedly across different clusters\n(e.g., transpose the first and last name). Our approach first uses an\nunsupervised method to generate groups of value pairs that can be transformed\nin the same way (i.e., they share a transformation). Then the groups are\npresented to a human for verification and the approved ones are used to\nstandardize the data. In a real-world dataset with 17,497 records, our method\nachieved 75% recall and 99.5% precision in standardizing variant values by\nasking a human 100 yes/no questions, which completely outperformed a state of\nthe art data wrangling tool.'"
Samuel Madden,Madden_Samuel,arXiv:1709.06416,https://arxiv.org/abs/1709.06416,"b'Abstract:  Data analytics applications combine multiple functions from different\nlibraries and frameworks. Even when each function is optimized in isolation,\nthe performance of the combined application can be an order of magnitude below\nhardware limits due to extensive data movement across these functions. To\naddress this problem, we propose Weld, a new interface between data-intensive\nlibraries that can optimize across disjoint libraries and functions. Weld\nexposes a lazily-evaluated API where diverse functions can submit their\ncomputations in a simple but general intermediate representation that captures\ntheir data-parallel structure. It then optimizes data movement across these\nfunctions and emits efficient code for diverse hardware. Weld can be integrated\ninto existing frameworks such as Spark, TensorFlow, Pandas and NumPy without\nchanging their user-facing APIs. We demonstrate that Weld can speed up\napplications using these frameworks by up to 29x.'"
Samuel Madden,Madden_Samuel,arXiv:1707.00721,https://arxiv.org/abs/1707.00721,"b'Abstract:  A polystore system is a database management system (DBMS) composed of\nintegrated heterogeneous database engines and multiple programming languages.\nBy matching data to the storage engine best suited to its needs, complex\nanalytics run faster and flexible storage choices helps improve data\norganization. BigDAWG (Big Data Working Group) is our reference implementation\nof a polystore system. In this paper, we describe the current BigDAWG software\nrelease which supports PostgreSQL, Accumulo and SciDB. We describe the overall\narchitecture, API and initial results of applying BigDAWG to the MIMIC II\nmedical dataset.'"
Samuel Madden,Madden_Samuel,arXiv:1704.04738,https://arxiv.org/abs/1704.04738,"b'Abstract:  Determining if two sets are related - that is, if they have similar values or\nif one set contains the other - is an important problem with many applications\nin data cleaning, data integration, and information retrieval. A particularly\npopular metric that has been proposed is to measure the relatedness of two sets\nby treating the elements as vertices of a bipartite graph and calculating the\nscore of the maximum matching pairing between elements. Compared to other\nmetrics which require exact matchings between elements, this metric uses a\nsimilarity function to compare elements between the two sets, making it robust\nto small dissimilarities in elements and more useful for real-world, dirty\ndata. Unfortunately, the metric suffers from expensive computational cost,\ntaking O(n^3) time, where n is the number of elements in sets, for each\nset-to-set comparison. Thus for applications which try to search for all\npairings of related sets in a brute-force manner, the runtime becomes\nunacceptably large.\nTo address this challenge, we developed SilkMoth, a system capable of rapidly\ndiscovering related set pairs in collections of sets. Internally, SilkMoth\ncreates a signature for each set, with the property that any other set which is\nrelated must match the signature. SilkMoth then uses these signatures to prune\nthe search space, so only sets which match the signatures are left as\ncandidates. Finally, SilkMoth applies the maximum matching metric on remaining\ncandidates to verify which of these candidates are truly related sets. Thus, a\ncontribution of this paper is the characterization of the space of signatures\nwhich enable this property. We show that selecting the optimal signature in\nthis space is NP-complete, and based on insights from the characterization of\nthe space, we propose two novel filters which help to prune the candidates\nfurther before verification.'"
Samuel Madden,Madden_Samuel,arXiv:1701.06093,https://arxiv.org/abs/1701.06093,"b'Abstract:  Big data applications have fast arriving data that must be quickly ingested.\nAt the same time, they have specific needs to preprocess and transform the data\nbefore it could be put to use. The current practice is to do these preparatory\ntransformations once the data is already ingested, however, this is expensive\nto run and cumbersome to manage. As a result, there is a need to push data\npreprocessing down to the ingestion itself. In this paper, we present a\ndeclarative data ingestion system, called INGESTBASE, to allow application\ndevelopers to plan and specify their data ingestion logic in a more systematic\nmanner. We introduce the notion of ingestions plans, analogous to query plans,\nand present a declarative ingestion language to help developers easily build\nsophisticated ingestion plans. INGESTBASE provides an extensible ingestion\noptimizer to rewrite and optimize ingestion plans by applying rules such as\noperator reordering and pipelining. Finally, the INGESTBASE runtime engine runs\nthe optimized ingestion plan in a distributed and fault-tolerant manner. Later,\nat query processing time, INGESTBASE supports ingestion-aware data access and\ninterfaces with upstream query processors, such as Hadoop MapReduce and Spark,\nto post- process the ingested data. We demonstrate through a number of\nexperiments that INGESTBASE: (i) is flexible enough to express a variety of\ningestion techniques, (ii) incurs a low ingestion overhead, (iii) provides\nefficient access to the ingested data, and (iv) has much better performance, up\nto 6 times, than preparing data as an afterthought, via a query processor.'"
Samuel Madden,Madden_Samuel,arXiv:1701.05799,https://arxiv.org/abs/1701.05799,"b'Abstract:  The Intel Science and Technology Center for Big Data is developing a\nreference implementation of a Polystore database. The BigDAWG (Big Data Working\nGroup) system supports ""many sizes"" of database engines, multiple programming\nlanguages and complex analytics for a variety of workloads. Our recent efforts\ninclude application of BigDAWG to an ocean metagenomics problem and\ncontainerization of BigDAWG. We intend to release an open source BigDAWG v1.0\nin the Spring of 2017. In this article, we will demonstrate a number of\npolystore applications developed with oceanographic researchers at MIT and\ndescribe our forthcoming open source release of the BigDAWG system.'"
Samuel Madden,Madden_Samuel,arXiv:1611.04705,https://arxiv.org/abs/1611.04705,"b'Abstract:  Existing database systems are not optimized for queries with a LIMIT\nclause---operating instead in an all-or-nothing manner. In this paper, we\npropose a fast LIMIT query evaluation engine, called NeedleTail, aimed at\nletting analysts browse a small sample of the query results on large datasets\nas quickly as possible, independent of the overall size of the result set.\nNeedleTail introduces density maps, a lightweight in-memory indexing structure,\nand a set of efficient algorithms (with desirable theoretical guarantees) to\nquickly locate promising blocks, trading off locality and density. In settings\nwhere the samples are used to compute aggregates, we extend techniques from\nsurvey sampling to mitigate the bias in our samples. Our experimental results\ndemonstrate that NeedleTail returns results 4x faster on HDDs and 9x faster on\nSSDs on average, while occupying up to 23x less memory than existing\ntechniques.'"
Samuel Madden,Madden_Samuel,arXiv:1609.07548,https://arxiv.org/abs/1609.07548,"b'Abstract:  Organizations are often faced with the challenge of providing data management\nsolutions for large, heterogenous datasets that may have different underlying\ndata and programming models. For example, a medical dataset may have\nunstructured text, relational data, time series waveforms and imagery. Trying\nto fit such datasets in a single data management system can have adverse\nperformance and efficiency effects. As a part of the Intel Science and\nTechnology Center on Big Data, we are developing a polystore system designed\nfor such problems. BigDAWG (short for the Big Data Analytics Working Group) is\na polystore system designed to work on complex problems that naturally span\nacross different processing or storage engines. BigDAWG provides an\narchitecture that supports diverse database systems working with different data\nmodels, support for the competing notions of location transparency and semantic\ncompleteness via islands and a middleware that provides a uniform multi--island\ninterface. Initial results from a prototype of the BigDAWG system applied to a\nmedical dataset validate polystore concepts. In this article, we will describe\npolystore databases, the current BigDAWG architecture and its application on\nthe MIMIC II medical dataset, initial performance results and our future\ndevelopment plans.'"
Samuel Madden,Madden_Samuel,arXiv:1603.00567,https://arxiv.org/abs/1603.00567,"b'Abstract:  As data volumes continue to rise, manual inspection is becoming increasingly\nuntenable. In response, we present MacroBase, a data analytics engine that\nprioritizes end-user attention in high-volume fast data streams. MacroBase\nenables efficient, accurate, and modular analyses that highlight and aggregate\nimportant and unusual behavior, acting as a search engine for fast data.\nMacroBase is able to deliver order-of-magnitude speedups over alternatives by\noptimizing the combination of explanation and classification tasks and by\nleveraging a new reservoir sampler and heavy-hitters sketch specialized for\nfast data streams. As a result, MacroBase delivers accurate results at speeds\nof up to 2M events per second per query on a single core. The system has\ndelivered meaningful results in production, including at a telematics company\nmonitoring hundreds of thousands of vehicles.'"
Samuel Madden,Madden_Samuel,arXiv:1602.08791,https://arxiv.org/abs/1602.08791,"b'Abstract:  BigDAWG is a polystore system designed to work on complex problems that\nnaturally span across different processing or storage engines. BigDAWG provides\nan architecture that supports diverse database systems working with different\ndata models, support for the competing notions of location transparency and\nsemantic completeness via islands of information and a middleware that provides\na uniform multi-island interface. In this article, we describe the current\narchitecture of BigDAWG, its application on the MIMIC II medical dataset, and\nour plans for the mechanics of cross-system queries. During the presentation,\nwe will also deliver a brief demonstration of the current version of BigDAWG.'"
Samuel Madden,Madden_Samuel,arXiv:1506.04815,https://arxiv.org/abs/1506.04815,"b'Abstract:  Organizations and teams collect and acquire data from various sources, such\nas social interactions, financial transactions, sensor data, and genome\nsequencers. Different teams in an organization as well as different data\nscientists within a team are interested in extracting a variety of insights\nwhich require combining and collaboratively analyzing datasets in diverse ways.\nDataHub is a system that aims to provide robust version control and provenance\nmanagement for such a scenario. To be truly useful for collaborative data\nscience, one also needs the ability to specify queries and analysis tasks over\nthe versioning and the provenance information in a unified manner. In this\npaper, we present an initial design of our query language, called VQuel, that\naims to support such unified querying over both types of information, as well\nas the intermediate and final results of analyses. We also discuss some of the\nkey language design and implementation challenges moving forward.'"
Samuel Madden,Madden_Samuel,arXiv:1503.01143,https://arxiv.org/abs/1503.01143,"b'Abstract:  Stream processing addresses the needs of real-time applications. Transaction\nprocessing addresses the coordination and safety of short atomic computations.\nHeretofore, these two modes of operation existed in separate, stove-piped\nsystems. In this work, we attempt to fuse the two computational paradigms in a\nsingle system called S-Store. In this way, S-Store can simultaneously\naccommodate OLTP and streaming applications. We present a simple transaction\nmodel for streams that integrates seamlessly with a traditional OLTP system. We\nchose to build S-Store as an extension of H-Store, an open-source, in-memory,\ndistributed OLTP database system. By implementing S-Store in this way, we can\nmake use of the transaction processing facilities that H-Store already\nsupports, and we can concentrate on the additional implementation features that\nare needed to support streaming. Similar implementations could be done using\nother main-memory OLTP platforms. We show that we can actually achieve higher\nthroughput for streaming workloads in S-Store than an equivalent deployment in\nH-Store alone. We also show how this can be achieved within H-Store with the\naddition of a modest amount of new functionality. Furthermore, we compare\nS-Store to two state-of-the-art streaming systems, Spark Streaming and Storm,\nand show how S-Store matches and sometimes exceeds their performance while\nproviding stronger transactional guarantees.'"
Samuel Madden,Madden_Samuel,arXiv:1412.5263,https://arxiv.org/abs/1412.5263,"b'Abstract:  Graph analytics is becoming increasingly popular, with a deluge of new\nsystems for graph analytics having been proposed in the past few years. These\nsystems often start from the assumption that a new storage or query processing\nsystem is needed, in spite of graph data being often collected and stored in a\nrelational database in the first place. In this paper, we study Vertica\nrelational database as a platform for graph analytics. We show that\nvertex-centric graph analysis can be translated to SQL queries, typically\ninvolving table scans and joins, and that modern column-oriented databases are\nvery well suited to running such queries. Specifically, we present an\nexperimental evaluation of the Vertica relational database system on a variety\nof graph analytics, including iterative analysis, a combination of graph and\nrelational analyses, and more complex 1- hop neighborhood graph analytics,\nshowing that it is competitive to two popular vertex-centric graph analytics\nsystems, namely Giraph and GraphLab.'"
Samuel Madden,Madden_Samuel,arXiv:1409.0798,https://arxiv.org/abs/1409.0798,"b'Abstract:  Relational databases have limited support for data collaboration, where teams\ncollaboratively curate and analyze large datasets. Inspired by software version\ncontrol systems like git, we propose (a) a dataset version control system,\ngiving users the ability to create, branch, merge, difference and search large,\ndivergent collections of datasets, and (b) a platform, DataHub, that gives\nusers the ability to perform collaborative data analysis building on this\nversion control system. We outline the challenges in providing dataset version\ncontrol at scale.'"
Samuel Madden,Madden_Samuel,arXiv:1209.3686,https://arxiv.org/abs/1209.3686,"b""Abstract:  Crowd-sourcing has become a popular means of acquiring labeled data for a\nwide variety of tasks where humans are more accurate than computers, e.g.,\nlabeling images, matching objects, or analyzing sentiment. However, relying\nsolely on the crowd is often impractical even for data sets with thousands of\nitems, due to time and cost constraints of acquiring human input (which cost\npennies and minutes per label). In this paper, we propose algorithms for\nintegrating machine learning into crowd-sourced databases, with the goal of\nallowing crowd-sourcing applications to scale, i.e., to handle larger datasets\nat lower costs. The key observation is that, in many of the above tasks, humans\nand machine learning algorithms can be complementary, as humans are often more\naccurate but slow and expensive, while algorithms are usually less accurate,\nbut faster and cheaper.\nBased on this observation, we present two new active learning algorithms to\ncombine humans and algorithms together in a crowd-sourced database. Our\nalgorithms are based on the theory of non-parametric bootstrap, which makes our\nresults applicable to a broad class of machine learning models. Our results, on\nthree real-life datasets collected with Amazon's Mechanical Turk, and on 15\nwell-known UCI data sets, show that our methods on average ask humans to label\none to two orders of magnitude fewer items to achieve the same accuracy as a\nbaseline that labels random images, and two to eight times fewer questions than\nprevious active learning schemes."""
Samuel Madden,Madden_Samuel,arXiv:1208.2925,https://arxiv.org/abs/1208.2925,"b'Abstract:  This paper presents a new approach to select events of interest to a user in\na social media setting where events are generated by the activities of the\nuser\'s friends through their mobile devices. We argue that given the unique\nrequirements of the social media setting, the problem is best viewed as an\ninductive learning problem, where the goal is to first generalize from the\nusers\' expressed ""likes"" and ""dislikes"" of specific events, then to produce a\nprogram that can be manipulated by the system and distributed to the collection\ndevices to collect only data of interest. The key contribution of this paper is\na new algorithm that combines existing machine learning techniques with new\nprogram synthesis technology to learn users\' preferences. We show that when\ncompared with the more standard approaches, our new algorithm provides up to\norder-of-magnitude reductions in model training time, and significantly higher\nprediction accuracies for our target application. The approach also improves on\nstandard machine learning techniques in that it produces clear programs that\ncan be manipulated to optimize data collection and filtering.'"
Samuel Madden,Madden_Samuel,arXiv:1208.2013,https://arxiv.org/abs/1208.2013,"b'Abstract:  Developing high-performance applications that interact with databases is a\ndifficult task, as developers need to understand both the details of the\nlanguage in which their applications are written in, and also the intricacies\nof the relational model. One popular solution to this problem is the use of\nobject-relational mapping (ORM) libraries that provide transparent access to\nthe database using the same language that the application is written in.\nUnfortunately, using such frameworks can easily lead to applications with poor\nperformance because developers often end up implementing relational operations\nin application code, and doing so usually does not take advantage of the\noptimized implementations of relational operations, efficient query plans, or\npush down of predicates that database systems provide. In this paper we present\nQBS, an algorithm that automatically identifies fragments of application logic\nthat can be pushed into SQL queries. The QBS algorithm works by automatically\nsynthesizing invariants and postconditions for the original code fragment. The\npostconditions and invariants are expressed using a theory of ordered relations\nthat allows us to reason precisely about the contents and order of the records\nproduced even by complex code fragments that compute joins and aggregates. The\ntheory is close in expressiveness to SQL, so the synthesized postconditions can\nbe readily translated to SQL queries. Using 40 code fragments extracted from\nover 120k lines of open-source code written using the Java Hibernate ORM, we\ndemonstrate that our approach can convert a variety of imperative constructs\ninto relational specifications.'"
Samuel Madden,Madden_Samuel,arXiv:1208.0271,https://arxiv.org/abs/1208.0271,"b""Abstract:  Database-backed applications are nearly ubiquitous in our daily lives.\nApplications that make many small accesses to the database create two\nchallenges for developers: increased latency and wasted resources from numerous\nnetwork round trips. A well-known technique to improve transactional database\napplication performance is to convert part of the application into stored\nprocedures that are executed on the database server. Unfortunately, this\nconversion is often difficult. In this paper we describe Pyxis, a system that\ntakes database-backed applications and automatically partitions their code into\ntwo pieces, one of which is executed on the application server and the other on\nthe database server. Pyxis profiles the application and server loads,\nstatically analyzes the code's dependencies, and produces a partitioning that\nminimizes the number of control transfers as well as the amount of data sent\nduring each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is\nable to generate partitions with up to 3x reduction in latency and 1.7x\nimprovement in throughput when compared to a traditional non-partitioned\nimplementation and has comparable performance to that of a custom stored\nprocedure implementation."""
Samuel Madden,Madden_Samuel,arXiv:1203.6049,https://arxiv.org/abs/1203.6049,"b'Abstract:  Replicating data across multiple data centers not only allows moving the data\ncloser to the user and, thus, reduces latency for applications, but also\nincreases the availability in the event of a data center failure. Therefore, it\nis not surprising that companies like Google, Yahoo, and Netflix already\nreplicate user data across geographically different regions.\nHowever, replication across data centers is expensive. Inter-data center\nnetwork delays are in the hundreds of milliseconds and vary significantly.\nSynchronous wide-area replication is therefore considered to be unfeasible with\nstrong consistency and current solutions either settle for asynchronous\nreplication which implies the risk of losing data in the event of failures,\nrestrict consistency to small partitions, or give up consistency entirely. With\nMDCC (Multi-Data Center Consistency), we describe the first optimistic commit\nprotocol, that does not require a master or partitioning, and is strongly\nconsistent at a cost similar to eventually consistent protocols. MDCC can\ncommit transactions in a single round-trip across data centers in the normal\noperational case. We further propose a new programming model which empowers the\napplication developer to handle longer and unpredictable latencies caused by\ninter-data center communication. Our evaluation using the TPC-W benchmark with\nMDCC deployed across 5 geographically diverse data centers shows that MDCC is\nable to achieve throughput and latency similar to eventually consistent quorum\nprotocols and that MDCC is able to sustain a data center outage without a\nsignificant impact on response times while guaranteeing strong consistency.'"
Samuel Madden,Madden_Samuel,arXiv:1203.5485,https://arxiv.org/abs/1203.5485,"b""Abstract:  In this paper, we present BlinkDB, a massively parallel, sampling-based\napproximate query engine for running ad-hoc, interactive SQL queries on large\nvolumes of data. The key insight that BlinkDB builds on is that one can often\nmake reasonable decisions in the absence of perfect answers. For example,\nreliably detecting a malfunctioning server using a distributed collection of\nsystem logs does not require analyzing every request processed by the system.\nBased on this insight, BlinkDB allows one to trade-off query accuracy for\nresponse time, enabling interactive queries over massive data by running\nqueries on data samples and presenting results annotated with meaningful error\nbars. To achieve this, BlinkDB uses two key ideas that differentiate it from\nprevious work in this area: (1) an adaptive optimization framework that builds\nand maintains a set of multi-dimensional, multi-resolution samples from\noriginal data over time, and (2) a dynamic sample selection strategy that\nselects an appropriately sized sample based on a query's accuracy and/or\nresponse time requirements. We have built an open-source version of BlinkDB and\nvalidated its effectiveness using the well-known TPC-H benchmark as well as a\nreal-world analytic workload derived from Conviva Inc. Our experiments on a 100\nnode cluster show that BlinkDB can answer a wide range of queries from a\nreal-world query trace on up to 17 TBs of data in less than 2 seconds (over\n100\\times faster than Hive), within an error of 2 - 10%."""
Samuel Madden,Madden_Samuel,arXiv:1109.6881,https://arxiv.org/abs/1109.6881,"b""Abstract:  Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible\nto task people with small jobs, such as labeling images or looking up phone\nnumbers, via a programmatic interface. MTurk tasks for processing datasets with\nhumans are currently designed with significant reimplementation of common\nworkflows and ad-hoc selection of parameters such as price to pay per task. We\ndescribe how we have integrated crowds into a declarative workflow engine\ncalled Qurk to reduce the burden on workflow designers. In this paper, we focus\non how to use humans to compare items for sorting and joining data, two of the\nmost common operations in DBMSs. We describe our basic query interface and the\nuser interface of the tasks we post to MTurk. We also propose a number of\noptimizations, including task batching, replacing pairwise comparisons with\nnumerical ratings, and pre-filtering tables before joining them, which\ndramatically reduce the overall cost of running sorts and joins on the crowd.\nIn an experiment joining two sets of images, we reduce the overall cost from\n$67 in a naive implementation to about $3, without substantially affecting\naccuracy or latency. In an end-to-end experiment, we reduced cost by a factor\nof 14.5."""
